{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u5b58\u76d8\u5f3a\u8feb\u75c7","text":"<p>\u4e2d\u5e74\u5371\u673a\u6280\u672f\u5b85\u7684\u4e2a\u4eba\u7f51\u7ad9\uff0c\u6b22\u8fce\u968f\u4fbf\u770b\u770b\uff0c\u4e1c\u897f\u6bd4\u8f83\u6742\uff0c\u4e5f\u8bb8\u80fd\u7ffb\u5230\u4f60\u559c\u6b22\u7684</p>"},{"location":"about/","title":"About Me","text":"<p>\u751f\u7269\u5343\u8001\u7684\u5fc3\uff0c\u6e38\u620f\u5f00\u53d1\u7684\u68a6\uff0c\u7801\u519c\u725b\u9a6c\u7684\u547d</p> <ul> <li>github</li> </ul> <p>\u4ee5\u4e0b\u6e38\u620f/\u7cfb\u5217\u6b7b\u5fe0</p> <ul> <li>into the breach</li> <li>\u9ad8\u7ea7\u6218\u4e89</li> <li>\u706b\u7130\u4e4b\u7eb9\u7ae0</li> <li>\u585e\u5c14\u8fbe\u4f20\u8bf4</li> <li>\u6076\u9b54\u57ce</li> <li>\u7a7a\u6d1e\u9a91\u58eb</li> <li>\u751f\u5316\u5371\u673a</li> <li>\u535a\u5fb7\u4e4b\u95e8</li> </ul>"},{"location":"mangowc/","title":"\u914d\u7f6e\u5e73\u94fa\u5f0f\u7a97\u53e3\u7ba1\u7406\u5668Mangowc","text":"<p>\u5bf9\u5e73\u94fa\u5f0f\u7a97\u53e3\u7ba1\u7406\u5668\u6211\u4e00\u76f4\u8dc3\u8dc3\u6b32\u8bd5\uff0c\u4f46\u662f\u6ca1\u6709\u5f88\u5408\u9002\u7684\u5207\u5165\u70b9\uff0c\u4e0d\u60f3\u5b8c\u5168\u4ece\u96f6 \u914d\u8d77\u3002\u4f46\u662fmangowc\u5438\u5f15\u4e86\u6211\u7684\u6ce8\u610f\uff0c\u7b80\u6d01\uff0c\u5bb9\u6613\u914d\u7f6e\uff0c\u5f00\u76d2\u7248\u672c\u5c31\u633a\u597d\u770b\u4e86\u3002 \u6700\u96be\u7684\u662f\uff0c\u4e2d\u6587\u652f\u6301\u987a\u6ed1\uff0c\u5168\u5c40\u8dd1\u4e00\u4e2afcitx5\uff0c\u5728\u6240\u6709\u5e94\u7528\u91cc\u90fd\u53ef\u4ee5\u8f93\u5165\u4e2d\u6587\uff0c \u5305\u62ec\u7ec8\u7aef\u3002\u8ba9\u6211\u8ba4\u771f\u4ecekde plasma6\u5207\u6362\u5230mangowc\u7684\u662f\uff0c\u5728plasma6\u4e0b\u6211\u628abrave \u7684\u4e2d\u6587\u8f93\u5165\u6eda\u6302\u4e86\uff0c\u7f51\u4e0a\u627e\u4e0d\u5230\u89e3\u51b3\u65b9\u6848\uff0c\u6bd5\u7adf\u672c\u6765brave\u5c31\u662f\u5c0f\u4f17\u3002</p> <p>mangowc\u7684\u7f3a\u70b9\uff1a\u6587\u6863\u975e\u5e38\u5c11\uff0c\u6709\u95ee\u9898\u4e00\u641c\uff0c\u51fa\u6765\u7684\u7ed3\u679c\u90fd\u662f\u8bb2\u6c34\u679c\u7684\u3002</p> <p>\u952e\u76d8\u7ed1\u5b9a</p> <p>\u5e94\u7528</p> <p>\u72b6\u6001\u680f</p>"},{"location":"pictures/","title":"\u968f\u4fbf\u753b\u753b","text":""},{"location":"pictures/#_2","title":"\u83dc\u9e1f\u6280\u672f","text":"<ul> <li>Inkscape\u5236\u4f5c\u57fa\u672clogo</li> </ul>"},{"location":"pictures/#_3","title":"\u968f\u4fbf","text":"<ul> <li>\u519c\u6c11\u65e5\u8bb0:\u65bd\u5de5\u4e2d</li> <li>\u77ed\u7bc7\uff1a\u672a\u5b8c\u6210\u7684\u66f2\u5b50</li> <li>\u540e\u534a\u8f88\u5b50\u6d82\u9e26</li> <li>\u535a\u58eb\u6d82\u9e26</li> <li>\u9ad8\u4e2d\u5927\u5b66\u6d82\u9e26</li> </ul>"},{"location":"pictures/#_4","title":"\u4e34\u6479","text":"<ul> <li>\u592a\u7a7a\u5821\u5792\u7247\u5934\u4e34\u6479</li> <li>\u6e38\u620f\u76f8\u5173</li> </ul>"},{"location":"pictures/#_5","title":"\u7ec3\u4e60","text":"<ul> <li>\u6c34\u5f69\u8bfe\uff0c2012</li> <li>Drawing on the right side of the brain</li> </ul>"},{"location":"projects/","title":"\u9879\u76ee","text":"<p>\u6709\u5c0f\u7a0b\u5e8f\u7684\u60f3\u6cd5\u90fd\u6254\u5728\u8fd9\uff0c\u4f46\u4e0d\u4e00\u5b9a\u80fd\u5199\u51fa\u6765\uff0c\u5199\u51fa\u6765\u4e5f\u4e0d\u4e00\u5b9a\u80fd\u7528</p> <ul> <li>\u6bcf\u65e5\u5f53\u5b63\u9e21\u6c64\uff1a\u6bcf\u65e5\u968f\u673a\u751f\u6210\u65e0\u5398\u5934\u77ed\u53e5\u5b50\u3002github</li> <li>\u5907\u4efd\u6587\u4ef6\u4fa0\uff1a\u65b9\u4fbf\u7684\u5168\u5c40\u5907\u4efd\u5927\u91cf\u914d\u7f6e\u6587    \u4ef6\u3002github</li> <li>\u7ec8\u7aef\u968f\u673a\u673a\u68b0\u56fe(\u672a\u5f00\u59cb)</li> <li>\u4eea\u8868\u76d8\u98ce\u683ceww bar(\u672a\u5f00\u59cb)</li> <li>\u4ece\u96f6\u5f00\u59cb\u505a\u4e2a\u4eba\u7f51\u7ad9\uff0c\u4e00\u4e2a\u5931\u8d25\u7684\u9879\u76ee\u56de\u987e</li> </ul>"},{"location":"tech/","title":"Tech notes \u6280\u672f\u7b14\u8bb0","text":""},{"location":"tech/#machine-learning-data-science-from-scratch","title":"Machine Learning / Data Science From Scratch","text":"<p>This is a series of n00b friendly notebooks for implementing ML techniques from strach. The intent is help folks like myself who don't have sufficient academic training in machine learning.</p> <p>Most notebooks are available in this github repo.</p> <ul> <li>Genetic Algorithm - an optimization method inspired by evolution</li> <li>Linear Regression</li> <li>Random Forest</li> <li>Perceptron Algorithm</li> <li>Basic Neural Network</li> <li>Recurrent Neural Network</li> <li>Clustering</li> <li>XGBoost</li> </ul>"},{"location":"tech/#_1","title":"\u6363\u817e\u914d\u7f6e\uff0c\u8f6f\u4ef6","text":"<ul> <li>\u7528ImageMagick\u6279\u5904\u7406\u56fe\u7247</li> <li>Update root partition</li> <li>Arch Linux + windows 11 \u53cc\u542f\u52a8</li> <li>systemd\u8bbe\u7f6e\u5b9a\u65f6\u4f5c\u4e1a</li> <li>\u914d\u7f6e\u5e73\u94fa\u5f0f\u7a97\u53e3\u7ba1\u7406\u5668mangowc</li> <li>\u7528mkdocs\u505a\u4e2a\u4eba\u7f51\u7ad9</li> <li>\u52a8\u6001\u767b\u9646\u80cc\u666f\uff0c\u9501\u5c4f\u80cc\u666f\u56fe</li> <li>\u5c0f\u9738\u738b\u673a\u5668\u5b66\u4e60\u673a\u914d\u7f6e</li> <li>emacs\u914d\u7f6e</li> <li>neovim\u914d\u7f6e</li> <li>\u5410\u69fdlinux+\u5c0f\u95ee\u9898\u89e3\u51b3\u65b9\u6848</li> <li>\u8f6f\u4ef6\u5fc3\u5f97</li> </ul>"},{"location":"tech/#_2","title":"\u5c0f\u9547\u505a\u9898\u867e","text":"<p>\u505a\u4e86\u6709\u610f\u601d\u7684\u9898\uff0c\u5c31\u987a\u624b\u5199\u4e00\u4e2a\u601d\u8def</p> <ul> <li>pythagorea\u51e0\u4f55\u624b\u6e38</li> </ul>"},{"location":"writing/","title":"\u968f\u4fbf\u804a\u804a","text":""},{"location":"writing/#_2","title":"\u8ddf\u81ea\u5df1\u5570\u55e6","text":"<ul> <li>\u9886\u4e3b\u7b14\u8bb0</li> <li>2019</li> <li>2012</li> <li>2011</li> <li>2010</li> <li>\u6df1\u5733\u56de\u5fc6</li> </ul>"},{"location":"writing/#_3","title":"\u97f3\u4e50\u76f8\u5173","text":"<ul> <li>\u6f14\u51fa\u6d41\u6c34\u8d26</li> <li>\u5373\u5174Canon in D\uff0c\u725b\u4eba\u5373\u5174\u5409\u4ed6\uff0c\u6211\u5373\u5174\u914d\u9f13</li> <li>28\uff0c\u4e50\u624b\u7248\u5408\u4f5c\u7684\u539f\u521b\u3002\u6211\u8d1f\u8d23\u9f13</li> <li>\u706b\u8f66\u97f3\u4e50\u5199\u5230\u54ea\u7b97\u54ea</li> </ul>"},{"location":"writing/#_4","title":"\u6e38\u620f\u76f8\u5173","text":"<ul> <li>\u5199\u70b9\u5173\u4e8eLena Raine\u5927\u795e</li> <li>\u72ec\u7acb\u6e38\u620f\u6d41\u6c34\u8d26</li> <li>\u6e38\u620fOST\u6d41\u6c34\u8d26</li> <li>\u6d77\u62c9\u9c81\u5927\u53a8\u6797\u514b\u7684\u65c5\u884c</li> <li>\u7a7a\u6d1e\u9a91\u58eb\u94c1\u8def\u5927\u4ea8</li> <li>Celeste\u7684n\u79cd\u6b7b\u6cd5</li> <li>\u4ee5\u6211\u4e3a\u4e2d\u5fc3\u7684\u6e38\u620f\u53f2</li> </ul>"},{"location":"pictures/college/","title":"\u9ad8\u4e2d\u5927\u5b66\u6d82\u9e26","text":""},{"location":"pictures/games/","title":"\u6e38\u620f\u76f8\u5173","text":"<p>\u9ad8\u4e2d\u65f6\u5019\u505a\u7684\u5c0f\u7eb8\u7247\u201c\u4e66\u7b7e\u201d</p> <p></p> <p></p> <p></p>"},{"location":"pictures/inkscape_logo/","title":"\u7528Inkscape\u5236\u4f5c\u57fa\u672clogo","text":"<p>Inkscape\u867d\u7136\u7528\u8d77\u6765\u4e0d\u5982Illustrator\u987a\u624b\uff0c\u4f46\u90a3\u4e48\u591a\u5e74\u4e0b\u6765\uff0c\u53ef\u7528\u6027\u63d0\u5347\u5f88\u591a \u4e86\uff0c\u5bf9\u6211\u6765\u8bf4\u591f\u7528\u3002</p> <p>\u8d77\u59cb\u70b9\uff1a\u62cd\u4e86\u4e00\u5f20\u6c34\u6c6a\u6c6a\u7684\u7f8a\u9a7c\u7167\u7247\uff0c\u5b69\u5b50\u4eec\u5f88\u559c\u6b22\u3002\u6b63\u597d\u6211\u8981\u5956\u52b1\u5979\u4eec\u5b8c\u6210\u8fd9 \u6b21\u65c5\u884c\uff0c\u5c31\u5e72\u8106\u4ee5\u8fd9\u5f20\u7167\u7247\u4e3a\u84dd\u672c\u505a\u5956\u724c\u3002</p> <p></p> <p>\u7b2c0\u6b65\u662f\u7eb8\u4e0a\u753b\u4e2a\u521d\u7a3f\uff0c\u62cd\u4e2a\u7167\uff0c\u5bfc\u5165Inkscape\u3002\u7167\u7247\u8d28\u91cf\u8981\u6c42\u4e0d\u9ad8\uff0c\u63cf\u7ebf\u7684\u65f6 \u5019\u53ef\u4ee5\u6539\u3002</p> <p></p> <p>Path -&gt; Trace bitmap\u3002\u8c03\u6574\u5408\u9002\u7684\u9608\u503c\uff0c\u5927\u81f4\u8f6e\u5ed3\u5c31\u51fa\u6765\u4e86\u3002</p> <p></p> <p>\u628a\u539f\u56fe\u9690\u85cf/\u5220\u9664\u6389\u540e\u53ef\u4ee5\u770b\u89c1\u521a\u63cf\u5b8c\u7684\u6837\u5b50\u3002\u4ee5\u8fd9\u4e2a\u4e3a\u57fa\u7840\uff0c\u5f62\u72b6\u6bd4\u8f83\u89c4\u5f8b\u7684 \u90e8\u5206\u6211\u91cd\u65b0\u753b\uff0c\u6bd4\u5982\u5706\u5f62\uff0c\u7b80\u5355\u7684\u66f2\u7ebf\u3002</p> <p>\u5706\u5f62\uff1a\u5706\u5f62\u5de5\u5177\uff0c\u5feb\u6377\u952eE\uff0c\u6309\u4f4fctrl\u53ef\u4ee5\u753b\u6574\u5706\u800c\u4e0d\u662f\u692d\u5706\uff0c\u5b9e\u9645\u64cd\u4f5c\u6211\u53d1\u73b0 \u4f1a\u65f6\u4e0d\u65f6\u5d29\u6210\u692d\u5706\uff0c\u5f97\u6162\u6162\u62d6\u62fd\u3002</p> <p>\u66f2\u7ebf\uff1a\u7528\u94a2\u7b14\u5de5\u5177B\u753b\u51e0\u4e2a\u70b9\uff0c\u753b\u5b8c\u4e4b\u540e\u6309\u56de\u8f66\uff0c\u628a\u586b\u5145fill\u6539\u4e3a\u900f\u660e\uff0c\u7ebf\u6761 stroke\u4e0a\u8272\u5e76\u8c03\u6574\u7c97\u7ec6\u3002\u518d\u5404\u81ea\u9009\u62e9\u8fd9\u4e9b\u8282\u70b9\u8c03\u6574\u5f27\u5ea6\u3002\u94a2\u7b14\u5de5\u5177\u4e0d\u662f\u7279\u522b\u7b26\u5408 \u6211\u7684\u76f4\u89c9\uff0c\u6709\u4e9b\u5730\u65b9\u8fd8\u6ca1\u641e\u6e05\u695a\u600e\u4e48\u7528\uff0c\u6240\u4ee5\u590d\u6742\u7684\u90e8\u5206\u5c3d\u91cf\u91c7\u7528trace bitmap\u7ed9\u51fa\u6765\u7684\u7ebf\u6761\u3002\u5220\u6389\u591a\u4f59\u7684\u8282\u70b9\uff0c\u8c03\u6574\u4f59\u4e0b\u7684\u8282\u70b9\u3002</p> <p>\u5feb\u6377\u952eS\uff1a\u9009\u62e9\u5bf9\u8c61\u3002  * \u6309shift\u518d\u9009\u53ef\u4ee5\u540c\u65f6\u9009\u591a\u4e2a\u3002  * \u9009\u62e9\u540e\u53ef\u4ee5\u8fdb\u884c\u7f29\u653e\uff0c\u62d6\u62fd  * \u9009\u4e86\u4e00\u6b21\u518d\u70b9\u51fb\u540c\u4e00\u4e2a\u7269\u4f53\uff0c\u5c31\u53ef\u4ee5\u65cb\u8f6c\u4e86</p> <p>\u4e2d\u95f4\u72b6\u6001\u5982\u4e0b\uff1a</p> <p></p> <p>\u8033\u6735\u91cd\u65b0\u753b\uff0c\u5377\u6bdb\u8f6e\u5ed3\u90e8\u5206\u590d\u5236\u7c98\u8d34\u5927\u6cd5\u3002\u6700\u540e\u52a0\u5165\u6587\u5b57\u3002\u5feb\u6377\u952eT\uff0cFill\u7684\u90e8 \u5206\u8981\u9009\u989c\u8272\uff0c\u5426\u5219\u5c31\u662f\u900f\u660e\uff0c\u5565\u4e5f\u770b\u4e0d\u89c1\u3002\u5b57\u4f53\u8981\u8c03\u6574\u3002\u9ed8\u8ba4\u5b57\u4f53\u53ef\u80fd\u662f\u663e\u5fae\u955c \u5927\u5c0f\u5751\u4eba\u3002\u6709\u4e86\u6587\u5b57\u4e4b\u540e\u753b\u4e00\u4e2a\u5706\uff0c\u540c\u65f6\u9009\u62e9\u6587\u5b57\u548c\u5706\uff0cText -&gt; Put on Path\uff0c \u628a\u6587\u5b57\u53d8\u6210\u5e26\u5f27\u5ea6\u7684\u3002\u901a\u8fc7\u8c03\u6574\u5706\u7684\u7f29\u653e\u6765\u8c03\u6574\u6587\u5b57\u7684\u5f27\u5ea6\u3002\u6700\u7ec8\u6548\u679c\u5982\u4e0b</p> <p></p> <p>\u6253\u5370\uff0c\u4e0a\u8272\uff0c\u8d34\u5230\u786c\u7eb8\u677f\u4e0a\u505a\u6210\u5956\u724c\u3002</p> <p></p> <p>\u6536\u5de5\uff01</p>"},{"location":"pictures/pencil/","title":"\u94c5\u7b14\u7ec3\u4e60","text":""},{"location":"pictures/phd/","title":"\u535a\u58eb\u6d82\u9e26","text":""},{"location":"pictures/right_brain/","title":"Drawing on the right side of the brain","text":"<p>\u5f53\u65f6Drawing on the Right Side of the Brain\u8fd9\u672c\u4e66\u5f88\u706b\uff0c\u5c31\u7167\u7740\u5b66\u4e86\u4e00\u70b9</p>"},{"location":"pictures/right_brain/#_1","title":"\u7ec3\u4e60\u4e00\uff1a\u7ebf\u6761","text":"<p>\u76ee\u7684\uff1a\u5728\u65e0\u6cd5\u8fa8\u8bc6\u7269\u4f53\u7684\u60c5\u51b5\u4e0b\u753b\uff0c\u53ea\u5173\u6ce8\u7ebf\u6761\u672c\u8eab\uff0c\u4e5f\u80fd\u8fbe\u5230\u4e0d\u5dee\u7684\u51c6\u786e\u5ea6</p> <ul> <li>\u5de6\u8111\u6a21\u5f0f\uff0c\u6b63\u7740\u753b\u7ebf\u6761\uff0c\u77e5\u9053\u753b\u7684\u7269\u4f53\u662f\u4ec0\u4e48</li> </ul> <p></p> <ul> <li>\u53f3\u8111\u6a21\u5f0f\uff0c\u628a\u539f\u753b\u5012\u8fc7\u6765\u4e34\u6479\uff08\u5f53\u7136\u753b\u4e5f\u662f\u5012\u8fc7\u6765\u7684\uff0c\u53ea\u662f\u4e3a\u4e86\u67e5\u770b\uff0c\u626b\u63cf\u7684 \u65f6\u5019\u6b63\u7740\u626b\uff09\u3002\u56e0\u4e3a\u65e0\u6cd5\u8fa8\u8bc6\u5012\u7740\u7684\u7269\u4f53\uff0c\u6240\u4ee5\u5de6\u8111\u4e0d\u53c2\u4e0e\u3002</li> </ul>"},{"location":"pictures/right_brain/#_2","title":"\u7ec3\u4e60\u4e8c\uff0c\u4e09\uff1a\u5c3d\u91cf\u5feb\uff0c\u4e0d\u8fc7\u8111","text":""},{"location":"pictures/right_brain/#_3","title":"\u7ec3\u4e60\u56db\uff0c\u4e94\uff1a\u4eba\u7269\uff0c\u5c3d\u91cf\u5feb\uff0c\u4e0d\u8fc7\u8111","text":""},{"location":"pictures/robotech/","title":"\u592a\u7a7a\u5821\u5792\u7247\u5934\u4e34\u6479","text":""},{"location":"pictures/unfinished_song/","title":"\u672a\u5b8c\u6210\u7684\u66f2\u5b50","text":"<p>\u9605\u8bfb\u987a\u5e8f\uff1a\u4ece\u53f3\u5230\u5de6</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"pictures/watercolor/","title":"\u6c34\u5f69\u8bfe\uff0c2012","text":"<p>\u5230hydepark art center\u4e0a\u4e86\u4e00\u5b66\u671f\u6c34\u5f69\uff0c\u6bcf\u5468\u4e00\u8282\u8bfe\uff0c\u6bcf\u8bfe\u4e24\u4e2a\u534a\u5c0f\u65f6\u3002\u4e00\u5b66 \u671f\u8bf4\u77ed\u4e5f\u77ed\uff0c\u522b\u7684\u6b63\u4e8b\u6b6a\u4e8b\u90fd\u78e8\u6d0b\u5de5\uff0c\u552f\u72ec\u6c34\u5f69\u7ec3\u4e60\u7a3f\u79ef\u7d2f\u4e86\u4e00\u53e0\u3002</p>"},{"location":"pictures/watercolor/#417","title":"4.17","text":"<p>\u7b80\u5355\u7269\u4f53\uff0c\u5c3d\u91cf\u7b80\u5316\u7740\u753b\u3002\u7528\u52a0\u4e92\u8865\u8272\u6765\u8868\u8fbe\u9634\u5f71\uff0c\u800c\u4e0d\u662f\u8c03\u6574\u540c\u4e00\u8272\u7684\u6df1\u6d45\u3002</p> <p></p>"},{"location":"pictures/watercolor/#424","title":"4.24","text":"<p>\u5b66\u8c03\u7070\u8272\uff0c\u5c1d\u8bd5\u7528\u94c5\u7b14\u6253\u7a3f\u548c\u4e0d\u6253\u7a3f\u7684\u533a\u522b\uff0c\u8fd9\u8282\u8bfe\u4e4b\u540e\u5c31\u4e0d\u4e8b\u5148\u7528\u94c5\u7b14\u6253\u7a3f\u4e86\u3002</p> <p></p>"},{"location":"pictures/watercolor/#51","title":"5.1","text":"<p>\u9759\u7269</p> <p></p>"},{"location":"pictures/watercolor/#58","title":"5.8","text":"<p>\u571f\u8c46\u571f\u8c46\u571f\u8c46</p> <p> </p>"},{"location":"pictures/watercolor/#515","title":"5.15","text":"<p>\u6a21\u7279\u3002\u4e0d\u7a7f\u8863\u670d\u8fd8\u597d\u753b\u4e9b\uff0c\u4e00\u7a7f\uff0c\u5c31\u753b\u5f97\u4e0d\u7537\u4e0d\u5973\u4e86\u3002</p> <p></p> <p></p>"},{"location":"pictures/watercolor/#521","title":"5.21","text":"<p>\u8138\u3002\u5148\u6c34\u5f69\uff0c\u518d\u7528\u7ebf\u6761\u7b14\u52fe\u753b\u548c\u4fee\u6b63\u3002</p> <p></p> <p></p> <p></p>"},{"location":"pictures/watercolor/#65","title":"6.5","text":"<p>\u5404\u79cd\u767d\u83dc</p> <p></p> <p></p>"},{"location":"pictures/watercolor/#612","title":"6.12","text":"<p>\u518d\u6b21\u611f\u53f9\u6211\u600e\u4e48\u753b\u5973\u6a21\u7279\u90fd\u50cf\u7537\u7684</p> <p></p> <p></p>"},{"location":"projects/dotfile_syncer/","title":"\u5907\u4efd\u6587\u4ef6\u4fa0","text":"<p>\u9879\u76ee\u72b6\u6001\uff1a\u65bd\u5de5\u4e2d</p>"},{"location":"projects/dotfile_syncer/#_2","title":"\u7528\u6237\u9700\u6c42","text":"<p>\u5f85\u89e3\u51b3\u7684\u95ee\u9898\uff1a\u6bcf\u6b21linux\u88c5\u673a\u90fd\u8981\u624b\u52a8\u4fee\u6539\u4e0d\u5c11\u6587\u4ef6\uff0c\u6563\u843d\u5728\u5404\u4e2a\u89d2\u843d\uff0c\u6709\u4e9b \u662f/home\u4e0b\u9762\u7684\uff0c\u6709\u4e9b\u662f/etc\u8fd9\u6837\u5168\u5c40\u7684\u3002\u56de\u5934\u5f88\u96be\u627e\u56de\u6765\u4fee\u6539\u4e86\u5565\uff0c\u4e0b\u6b21\u88c5 \u7cfb\u7edf\u53c8\u8981\u91cd\u5934\u5f00\u59cb\u3002</p>"},{"location":"projects/dotfile_syncer/#_3","title":"\u8bbe\u8ba1\u601d\u8def","text":"<p>\u601d\u8def\uff1a\u5907\u4efd\u672c\u8eab\u4f7f\u7528rsync</p> <pre><code>rsync -avrP &lt;src&gt; &lt;dest&gt;\n</code></pre> <p>\u9700\u8981\u8bb0\u5f55\u6240\u6709\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u5e76\u8ba9rsync\u77e5\u9053\u3002</p> <ul> <li>\u8bbe\u5b9a\u4e00\u4e2a\u9ed8\u8ba4\u76ee\u6807\u6587\u4ef6\u5939\u3002 </li> <li>\u6dfb\u52a0\u5907\u4efd\u6587\u4ef6\u5217\u8868</li> <li>\u91c7\u7528\u4e00\u4e2acsv\uff0c\u5982\u679c\u5206\u522b\u77e5\u9053src, dest\uff0c\u5c31\u5199\u5165\u4e00\u884c<code>src,dest</code></li> <li>\u5982\u679c\u5bf9dest\u6ca1\u6709\u7279\u5b9a\u7684\u8981\u6c42\uff0c\u5c31\u5199\u5165<code>src,</code>\uff0c\u7559\u7a7a\u7b2c\u4e8c\u5217</li> </ul> <p>\u5907\u4efd</p> <ul> <li>\u904d\u5386\u5907\u4efd\u5217\u8868\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u884c\uff0c\u63d0\u53d6src, dest\u3002\u5982\u679cdest\u4e3a\u7a7a\uff0c\u5219\u91c7\u7528\u8ddfsrc    \u4e00\u6837\u7684\u7ed3\u6784\uff0c<code>&lt;default_dest&gt;/&lt;src&gt;</code>\uff0c\u8fd9\u6837\u7684\u3002\u6bd4\u5982\u6211\u60f3\u968f\u65f6\u5907\u4efd    <code>/etc/share/sddm/theme.conf</code>\uff0c\u6211\u5c31\u628a\u8fd9\u4e2a\u6587\u4ef6\u8def\u5f84\u52a0\u5230\u5907\u4efd\u5217\u8868\u91cc\u3002    dest\u4f1a\u81ea\u52a8\u751f\u6210\u4e3a<code>~/dropbox/etc/share/sddm/theme.conf</code>\u3002</li> <li>\u6267\u884c<code>rsync -avrP &lt;src&gt; &lt;dest&gt;</code></li> <li>\u4ee5\u4e0a\u5728python\u91cc\u7528subprocess\u5b9e\u73b0</li> </ul>"},{"location":"projects/dotfile_syncer/#_4","title":"\u4f7f\u7528","text":"<p>\u9047\u5230\u9700\u8981\u5907\u4efd\u7684\u6587\u4ef6\uff0c\u5c31\u8dd1\u4e00\u4e2a<code>echo \"&lt;src_path&gt;,\" &gt;&gt; ~/syncall_db.csv</code></p> <p>\u5728<code>.zshrc</code>\u6216\u8005<code>.bashrc</code>\u91cc\u52a0\u5165</p> <pre><code>alias synkall='synkall_py ~/syncall_db.csv'\n</code></pre> <p>\u5c31\u53ef\u4ee5\u5728\u4efb\u610f\u5730\u65b9\u8dd1synkall\u4e86</p>"},{"location":"projects/eww_panel/","title":"\u4eea\u8868\u76d8\u98ce\u683ceww panel","text":""},{"location":"projects/eww_panel/#_1","title":"\u7528\u6237\u9700\u6c42","text":"<p>\u76ee\u6807\uff1a\u4eea\u8868\u76d8\u98ce\u683c\u7684eww bar\uff0c\u5143\u7d20\u6709</p> <ul> <li>\u8f6c\u901f\u8868\uff1acpu\u98ce\u6247</li> <li>\u91cc\u7a0b\u8868\uff1a\u5f00\u673a\u65f6\u95f4</li> <li>\u901f\u5ea6\uff1acpu</li> <li>\u51b7\u5374\u6db2\u6e29\u5ea6\uff1a\u786c\u76d8\u6e29\u5ea6</li> <li>\u6536\u97f3\u673a\uff1a\u97f3\u4e50\u53f0\u3002\u6574\u6d3b\uff1a\u753b\u5f88\u591a\u5e27\u7684\u673a\u5668\u4eba\uff0c\u6839\u636e\u97f3\u9891\u4fe1\u606f\u8ba9\u673a\u5668\u4eba\u8df3\u821e\u3002</li> </ul> <p>\u8fd8\u6ca1\u60f3\u597d\u600e\u4e48\u5f52\u7c7b\u7684</p>"},{"location":"projects/eww_panel/#_2","title":"\u8bbe\u8ba1\u601d\u8def","text":"<p>\u8dd1\u4e00\u4e2a\u76d1\u63a7\u811a\u672c\uff0c\u5b9a\u65f6\u8bfb\u7cfb\u7edf\uff0c\u6839\u636e\u8bfb\u6570\u6765\u51b3\u5b9a\u663e\u793a\u3002</p>"},{"location":"projects/eww_panel/#_3","title":"\u65b9\u6848\u4e00\uff1a\u786c\u51f9\u56fe\u7247","text":"<p>\u6bd4\u598250\u4e2a\u6863\u4f4d\u7684\u5206\u8fa8\u7387\uff0c\u5c31\u505a50\u5f20\u6307\u9488\u4f4d\u7f6e\u6e10\u8fdb\u7684\u56fe\u7247\uff0c\u6839\u636e\u4e0d\u540c\u7684\u7cfb\u7edf\u6307\u6807\u8bfb \u6570\u5207\u6362\u76f8\u5e94\u7684\u56fe\u7247</p>"},{"location":"projects/eww_panel/#_4","title":"\u65b9\u6848\u4e8c\uff1a\u5b57\u7b26\u5b9e\u73b0","text":"<p>\u65e0\u6cd5\u60f3\u8c61\u5982\u4f55\u7528\u5b57\u7b26\u5b9e\u73b0\u4eea\u8868\u56fe</p>"},{"location":"projects/eww_panel/#_5","title":"\u4f7f\u7528","text":""},{"location":"projects/mech_colorscript/","title":"\u7ec8\u7aef\u673a\u68b0\u968f\u673a\u56fe","text":"<p>\u521b\u610f\u6284\u88ad\u81ea\uff1a pokemon-colorscripts</p>"},{"location":"projects/mech_colorscript/#_2","title":"\u7528\u6237\u9700\u6c42","text":"<p>\u4ece\u56fe\u7247\u5230\u4e0a\u9762\u94fe\u63a5\u91cc\u7684\u6548\u679c\u4e00\u6761\u9f99\u3002\u7b80\u5355\u6765\u8bf4\u5c31\u662f\u6211\u6709\u4e00\u5f20\u559c\u6b22\u7684\u56fe\u7247\uff0c\u6211\u60f3\u8ba9 \u5b83\u4ee5unicode art\u7684\u5f62\u5f0f\u51fa\u73b0\uff0c\u5e76\u5728\u6253\u5f00\u7ec8\u7aef\u65f6\u968f\u673a\u663e\u793a\u3002\u521d\u8877\u662f \u673a\u5668\u4eba\u56fe\u7247\uff0c\u4f46\u5b9e\u9645\u5e76\u6ca1\u6709\u9650\u5236\u3002</p>"},{"location":"projects/mech_colorscript/#_3","title":"\u8bbe\u8ba1\u601d\u8def","text":"<p>\u4ecepokemon colorscript\u8d77\u6b65\uff0c\u9700\u8981\u7684\u529f\u80fd\u6709</p> <ul> <li>\u4ece\u56fe\u7247\u751f\u6210unicode art\u3002</li> <li>\u4ece\u4e00\u4e2a\u6587\u4ef6\u5939\u7684\u6240\u6709\u56fe\u7247\u6279\u91cf\u5904\u7406\u3002</li> <li>\u8bfb\u53d6unicode art\u3002</li> <li>\u7b80\u5355\u7684\u547d\u4ee4\u884c\u6a21\u5f0f\u5f15\u5bfc\u914d\u7f6eunicode art\u7684\u53c2\u6570\u3002</li> <li>\u7ed9\u4e0d\u540c\u7684\u56fe\u7247\u52a0\u4e0atag\uff0c\u53ef\u4ee5\u5207\u6362\u5728\u54ea\u4e2a\u7c7b\u522b\u968f\u673a\u9009\u3002\u6bd4\u5982\u53ef\u4ee5\u53ea\u9009pokemon,    \u6216\u8005\u53ea\u9009\u673a\u5668\u4eba\u3002</li> <li>\u4e3a\u4e86\u907f\u514d\u6bcf\u6b21\u6539\u53d8\u7c7b\u522b\u90fd\u8981\u624b\u52a8\u4fee\u6539zshrc\uff0c\u4f7f\u7528\u73af\u5883\u53d8\u91cf\uff0c\u8fd9\u4e2a\u73af\u5883\u53d8\u91cf\u53ef    \u4ee5\u5728mech script\u91cc\u8bbe\u7f6e\u3002</li> </ul>"},{"location":"projects/mech_colorscript/#_4","title":"\u4f7f\u7528","text":"<p>\u6211\u7528\u7684\u662fzsh\uff0c\u901a\u8fc7oh my zsh\u914d\u7684\u3002\u4f46\u6211\u4e0d\u8bb0\u5f97\u6765\u6e90\u662f\u5b98\u7f51\u8fd8\u662f\u88ab\u989d\u5916\u914d\u7f6e\u8fc7\u7684\u3002 \u76f8\u5173\u7684\u90e8\u5206\u5e94\u8be5\u662f\u8fd9\u4e9b\uff0c\u540c\u65f6\u663e\u793apokemon\u548cfastfetch</p> <pre><code># Display Pokemon-colorscripts\n# Project page: https://gitlab.com/phoneybadger/pokemon-colorscripts#on-\nother-distros-and-macos\n#pokemon-colorscripts --no-title -s -r #without fastfetch\npokemon-colorscripts --no-title -s -r | fastfetch -c $HOME/.config/fastf\netch/config-pokemon.jsonc --logo-type file-raw --logo-height 10 --logo-w\nidth 5 --logo -\n</code></pre> <p>TODO\uff1a\u4fee\u6539\u4e00\u4e0b\u547d\u4ee4\uff0c\u6539\u4e3amech colorscripts</p>"},{"location":"tech/apps/","title":"\u8f6f\u4ef6\u5fc3\u5f97\u548c\u63a8\u8350","text":""},{"location":"tech/apps/#_2","title":"\u7cfb\u7edf\u5de5\u5177","text":"<p>\u5927\u90e8\u5206\u662f\u547d\u4ee4\u884c\u5de5\u5177\u3002</p> <ul> <li>tree: \u6811\u72b6\u6587\u4ef6\u6d4f\u89c8\uff0c\u8f85\u52a9ls</li> <li>less: \u9884\u89c8\u6587\u4ef6\u3002arch linux\u8fde\u8fd9\u4e2a\u90fd\u4e0d\u9884\u88c5\u662f\u6211\u6ca1\u60f3\u5230\u7684\u3002\u3002</li> <li>auto-cpufreq: \u636e\u8bf4\u662f\u7701\u7535\u5c0f\u52a9\u624b</li> </ul> <pre><code>sudo pacman -S auto-cpufreq\nsystemctl enable auto-cpufreq\nsystemctl start auto-cpufreq\n</code></pre> <ul> <li>rsync \uff1a\u540c\u6b65\u6587\u4ef6\u5939\u3002\u9002\u5408\u5199\u811a\u672c\u3002</li> <li>yazi\uff1a\u9e2d\u5b50\u3002\u7ec8\u7aef\u6a21\u5f0f\u7684\u6587\u4ef6\u7ba1\u7406\u5668\uff0c\u9700\u8981\u4e00\u70b9\u4e0a\u624b\uff0c\u4f46\u6211\u5f88\u559c\u6b22\u5b83\u7684\u6807\u8bb0    \u518d\u64cd\u4f5c\u529f\u80fd\uff0c\u9632\u6b62\u624b\u6ed1\u3002</li> <li>\u7a7a\u683c\uff1a\u9009\u53d6\u6587\u4ef6</li> <li>shift+enter\uff1a\u9009\u62e9\u6253\u5f00\u65b9\u5f0f</li> <li>y\uff1a\u590d\u5236\uff0cp\uff1a\u7c98\u8d34</li> <li>bt\uff1a\u589e\u52a0\u65b0tab</li> <li><code>[</code>,<code>]</code>\uff0c\u5207\u6362tab\u3002</li> <li>a\uff1a\u6dfb\u52a0\u6587\u4ef6\u3002\u5982\u679c\u8f93\u5165\u540d\u5b57\u540e\u52a0\u4e00\u4e2a<code>/</code>\uff0c\u5c31\u6539\u4e3a\u521b\u5efa\u6587\u4ef6\u5939\u3002</li> <li>r\uff1a\u6539\u540d</li> <li>d\uff1a\u5220\u9664</li> <li>ctrl+c\uff1a\u5173\u95edtab\uff0c\u5982\u679c\u53ea\u5269\u4e0b\u4e00\u4e2atab\uff0c\u5c31\u9000\u51fa\u7a0b\u5e8f</li> <li>kwin\uff1akde\u81ea\u5e26\u7684\u7a97\u53e3\u5e73\u94fa\u7ba1\u7406\u5668\uff0c\u9002\u5408\u60f3\u7528\u4e00\u4e9b\u5e73\u94fa\uff0c\u53c8\u4e0d\u613f\u610f\u653e\u5f03kde\u5168    \u5bb6\u6876\u7684\u4eba\u3002</li> <li>findimagedupes\uff1a\u5bfb\u627e\u76f8\u4f3c\u7684\u56fe\u7247\u3002<code>findimagedupes -R -- . &gt; dups.log</code>\u3002    \u6ce8\u610f\u662f\u76f8\u4f3c\uff0c\u4e0d\u4e00\u5b9a\u76f8\u540c\uff0c\u4e0d\u8981\u4f9d\u8d56\u5b83\u6765\u5220\u9664\u540c\u6837\u7684\u56fe\u7247\u3002</li> <li>yt-dlp\uff1a\u4e0b\u8f7dyoutube\u89c6\u9891\u5230\u672c\u5730\u7684\u547d\u4ee4\u884c\u5de5\u5177\u3002</li> <li>cmus\uff1a\u6781\u7b80\u98ce\u683c\u7684\u97f3\u4e50\u64ad\u653e\u5668\u3002\u7531\u4e8eMOC\u88c5\u4e0d\u4e0a\uff0c\u5c31\u6362\u6210\u4e86cmus\uff0c\u4f53\u9a8c\u826f\u597d</li> <li>evince\uff1a\u770bpdf</li> </ul>"},{"location":"tech/apps/#_3","title":"\u6587\u6863\u7f16\u8f91\u5668","text":"<p>\u4f5c\u4e3a\u6587\u6863\u7f16\u8f91\u5668\u7231\u597d\u8005\uff0c\u6211\u9879\u76ee\u7ba1\u7406\u9760\u7528\u4e0d\u540c\u7684\u7f16\u8f91\u5668\uff0c\u4e00\u4e2a\u7f16\u8f91\u5668\u7ba1\u4e00\u5757\u4e1a\u52a1\u3002</p> <ul> <li>nano: \u5bf9\u4e8e\u4e0d\u559c\u6b22vim\u7684\u4eba\u6765\u8bf4\uff0c\u8fd9\u4e2a\u5728\u7ec8\u7aef\u7528\u5f88\u5408\u9002\uff0c\u6211\u53ea\u8981\u6709terminal\u5c31    \u4f1a\u914d\u7f6e\u4e00\u4e2a\u3002\u7528\u4e86n\u5e74\u540e\u6211\u624d\u53d1\u73b0\u652f\u6301\u8bed\u6cd5\u9ad8\u4eae\uff0c\u4e00\u76f4\u90fd\u9ed1\u767d\u914d\u88f8\u5954\u3002</li> <li>emacs: \u8fd9\u51e0\u5e74emacs\u70ed\u5ea6\u9aa4\u964d\uff0c\u5e02\u573a\u4efd\u989d\u6bd4vim\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u4f46org mode\u8fd8    \u662f\u9999\u3002\u4e0d\u60f3\u6298\u817e\u7684\u53ef\u4ee5\u53c2\u8003\u6211\u7684\u914d\u7f6e\u3002</li> <li>code: VS code\u7684linux\u7248</li> <li>neovim/lazyvim\u3002\u6211\u5f88\u9057\u61be\u5f53\u521d\u56e0\u4e3a\u5723\u6218\u800c\u6ca1\u6709\u5165\u5751\uff0c\u73b0\u5728\u5e74\u7eaa\u5927\u4e86\u518d\u5b66\u4e00    \u5957\u5feb\u6377\u952e\u4f53\u7cfb\u5f88\u96be\u3002\u76f8\u89c1\u6068\u665a\uff0c\u5b8c\u5168\u53ef\u4ee5\u53d6\u4ee3IDE\uff0c\u5bf9\u4e8e\u8001\u4eba\u7a0b\u5e8f\u733f\u6765\u8bf4\uff0c    vim\u6050\u6015\u662f\u6700\u53cb\u597d\u7684\uff0c\u4e0d\u9700\u8981\u9f20\u6807\uff0c\u800c\u4e14\u51e0\u4e4e\u6574\u4e2a\u5c4f\u5e55\u90fd\u662f\u7f16\u8f91\u754c\u9762\uff0c\u9002\u5408\u8001    \u4eba\u5927\u5b57\u4f53\u3002</li> <li>atom\uff1a\u5feb\u8981\u88abzed\u8fed\u4ee3\u4e86\uff08\u540c\u4e00\u516c\u53f8\uff09\uff0c\u4f46\u662f\u6211\u559c\u6b22\u5b83\u7684\u7b80\u6d01\u98ce\u683c\u3002</li> <li>zed(zeditor)\uff1a\u6709\u4e86\u5b83\u4e4b\u540e\u6211\u5c31\u518d\u4e5f\u4e0d\u7528vs code\u4e86\u3002\u5f00\u7bb1\u5373\u7528\u7684\u529f\u80fd\u6709\uff1a\u5f00    \u7ec8\u7aef\uff0cgit\u6574\u5408\uff0c\u76f4\u63a5\u770b\u5230\u4e0a\u6b21commit\uff0c\u8bed\u6cd5\u68c0\u67e5\uff0c\u663e\u793amethod\u7684\u5b9a\u4e49\u548c\u6ce8\u91ca\u3002    \u5b83\u4e0d\u591a\u4e0d\u5c11\u6b63\u597d\u6709\u6211\u9700\u8981\u7684\u4e1c\u897f\uff0c\u6ca1\u6709\u6211\u4e0d\u9700\u8981\u7684\u3002\u552f\u4e00\u9057\u61be\u7684\u662f\u76ee\u524d\u6ca1\u627e    \u5230\u900f\u660e\u80cc\u666f\u600e\u4e48\u641e\u3002</li> <li>mousepad\uff1axfce\u7684\u8bb0\u4e8b\u672c\uff0c\u591f\u7b80\u5355\uff0c\u6211\u5f53\u4fbf\u7b7e\u4f7f\u7528\u3002</li> </ul>"},{"location":"tech/apps/#_4","title":"\u7ec8\u7aef","text":"<ul> <li>tilix: \u53ef\u4ee5\u5e73\u94fa\u7a97\u53e3\u7684\u7ec8\u7aef\u3002\uff08\u67d0\u6b21\u88abarch\u6eda\u6302\u4e86\uff0c\u4e0d\u77e5\u9053\u8ddfwayland\u6709\u6ca1    \u6709\u5173\u7cfb\uff09</li> <li>kitty\uff1a\u5f53\u7ea2\u8f7b\u91cf\u7ea7\u3001\u53ef\u5b9a\u5236\u7ec8\u7aef\uff0c\u652f\u6301\u900f\u660e</li> <li>konsole\uff1akde\u9ed8\u8ba4\u7ec8\u7aef\uff0c\u6211\u7528\u5b83\u4f5c\u4e3a\u9ed8\u8ba4\u7ec8\u7aef\u6765\u8dd1\u7ec8\u7aef\u6a21\u5f0f\u7684\u7a0b\u5e8f\u3002\u5206\u5c4f\u5f88    \u65b9\u4fbf\u3002</li> </ul>"},{"location":"tech/apps/#_5","title":"\u5176\u5b83","text":"<ul> <li>darktable\uff1a\u7167\u7247\u7f16\u8f91\u548c\u7ba1\u7406\u3002\u636e\u8bf4\u7528\u8fc7\u4e4b\u540e\u5c31\u4e0d\u60f3\u7528lightroom\u4e86</li> <li>RawTherapee\uff1a\u4e5f\u662f\u7167\u7247\u7f16\u8f91\u548c\u7ba1\u7406\uff0c\u5404\u6709\u4f18\u70b9\u3002</li> <li>krita\uff1a\u66ff\u4ee3photoshop\u3002</li> <li>gimp\uff1a\u867d\u7136\u6211\u4e0d\u4e60\u60ef\u5b83\u7684\u5feb\u6377\u952e\uff0c\u8fd8\u662f\u8981\u8d5e\u4e00\u4e0b\u7ec8\u4e8e\u628a\u53cd\u4eba\u7c7b\u7684\u6bcf\u4e2a\u90e8\u4ef6\u5355    \u72ec\u4e00\u4e2a\u7a97\u53e3\u6539\u6389\u4e86\uff0c\u53d8\u6210\u6574\u5408\u7684\u4e86\u3002</li> <li>imagemagick\uff1b\u4e00\u7cfb\u5217\u5904\u7406\u56fe\u7247\u7684\u547d\u4ee4\u884c\u3002\u867d\u7136\u542c\u7740\u53cd\u76f4\u89c9\uff0c\u4f46\u662f\u6709\u4e9b\u5c0f\u4e1c\u897f    \u8fd8\u633a\u65b9\u4fbf\uff0c\u6bd4\u5982terminal\u76f4\u63a5\u770b\u56fe<code>display a.jpg</code>\u3002\u6279\u91cf\u7ed9\u56fe\u7247\u52a0\u8fb9\u6846\u4e5f\u5f88    \u65b9\u4fbf\u3002</li> <li>brave\uff1a\u6d4f\u89c8\u5668\u3002\u7528\u8d77\u6765\u5f88\u8212\u670d\uff0c\u5185\u5b58\u8017\u7684\u53ef\u4ee5\u63a5\u53d7\u3002</li> </ul>"},{"location":"tech/archlinux_config/","title":"\u5c0f\u9738\u738b\u673a\u5668\u5b66\u4e60\u673a\u914d\u7f6e","text":""},{"location":"tech/archlinux_config/#kdekwin","title":"KDE(\u81ea\u5e26kwin)","text":"<p>\u521d\u8877\u662f\u914d\u4e00\u4e2a\u5177\u5907\u4e00\u4e9b\u53ef\u5b9a\u5236\u529f\u80fd\uff0c\u53c8\u5bb9\u6613\u4e0a\u624b\u7684\u7cfb\u7edf\u7528\u6765\u5b66\u4e60\u673a\u5668\u5b66\u4e60\u3002\u8bd5\u8fc7 hyprland,bspwm\u90fd\u88ab\u76f4\u63a5\u83dc\u9e1f\u529d\u9000\u3002KDE plasma\u51fa\u5382\u53ef\u7528\uff0c\u56fe\u5f62\u754c\u9762\u70b9\u9f20\u6807\u5c31\u80fd \u5b9e\u73b0\u4e0d\u5c11\u82b1\u91cc\u80e1\u54e8\u7684\u914d\u7f6e\uff0c\u6240\u4ee5\u4f5c\u4e3a\u57fa\u672c\u7684\u50bb\u74dc\u578b\u7a97\u53e3\u7ba1\u7406\u5668\u5f88\u5408\u683c\u3002\u53e6\u4e00\u4e2a\u9009 \u62e9\u662fxfce\uff0c\u4e0d\u8fc7\u6211\u60f3\u8bd5\u70b9\u65b0\u73a9\u5177\uff0c\u800cxfce\u66fe\u7ecf\u662f\u6211\u7684\u4e3b\u529b\u3002</p> <p>\u6211\u7684\u4e0b\u4e00\u7ec4\u8981\u6c42\u662f\uff0c\u5728\u4e0d\u653e\u5f03\u9f20\u6807\u64cd\u4f5c\u7684\u6761\u4ef6\u4e0b\uff0c\u6709\u5e73\u94fa\u5f0f\u7a97\u53e3\u7ba1\u7406\u548c\u952e\u76d8\u64cd\u4f5c \u7684\u9009\u9879\u3002plasma\u81ea\u5e26\u7684kwin\u5c31\u80fd\u6ee1\u8db3\uff0c\u4f9d\u9760\u51e0\u4e2a\u5feb\u6377\u952e\u6765\u628a\u9009\u5b9a\u7a97\u53e3\u653e\u5230\u4e8b\u5148\u8bbe \u5b9a\u597d\u7684\u5e73\u94fa\u683c\u5b50\u91cc\u3002\u53e6\u5916emacs, tilix\u672c\u8eab\u5c31\u5e26\u5e73\u94fa\u529f\u80fd\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u7a97\u53e3\u7ba1\u7406\u5668\u3002</p>"},{"location":"tech/archlinux_config/#ewwpolybar","title":"eww+polybar","text":"<p>\u4e0b\u4e00\u4e2a\u8981\u6c42\u662f\uff0c\u770b\u89c1\u522b\u4eba\u6709\u6f02\u4eae\u7684\u5c0f\u7a97\u53e3\u90e8\u4ef6\uff0c\u4e5f\u60f3\u6284\u4e00\u4e2a\u3002conky\u914d\u7f6e\u5931\u8d25\uff0c \u53d1\u73b0eww\u662f\u5f53\u7ea2\u9009\u62e9\uff0c\u4e5f\u6709\u5f88\u591a\u4f5c\u4e1a\u6284\u3002\u6211\u770b\u4e0a\u4e86\u8fd9\u4e2agithub repo\u7684\u914d\u7f6e\u3002 \u8fd9\u4e2arepo\u662feww+bspwm\u7684\u7ec4\u5408\uff0c\u6548\u679c\u56fe\u975e\u5e38\u6f02\u4eae\uff0c\u4f46\u6211\u9700\u8981\u7684\u53ea\u6709eww\uff0c\u6240\u4ee5\u628a autostart\u91cc\u4e0d\u76f8\u5173\u7684\u90e8\u5206\u5220\u6389\uff0c\u5e76\u8ba9kde\u7684autostart\u8dd1\u8fd9\u4e2a\u811a\u672c\u3002\u53e6\u4e00\u4e2a\u9700\u8981 \u6539\u52a8\u7684\u662feww\u7684\u6837\u5f0f\uff0c\u56e0\u4e3a\u8ddf\u5f53\u524d\u663e\u793a\u5668\u76f8\u5173\uff0c\u76f4\u63a5\u6284\u7684\u4f5c\u4e1a\u4e0d\u4e00\u5b9a\u9002\u5408\u3002\u6211\u6539 \u4e86\u8fb9\u754c\u7559\u767d\u3002\u5176\u4e2d<code>:y \"0px\"</code>\u5220\u6389\u9876\u90e8\u7559\u767d\uff0c<code>:distance \"50px\"</code>\u4f7f\u5f97bar\u548c\u4e0b \u9762\u7a97\u53e3\u4e4b\u95f4\u6b63\u597d\u6709\u4e00\u70b9\u7a7a\u9699\u3002\u5728\u4ee5\u4e0b\u8fd9\u6837\u7684\u6bb5\u843d\u91cc\u6539\uff1a</p> <pre><code>;; Central bar \n(defwindow bar_center\n  :geometry \n    (geometry \n      :anchor \"top center\"\n      :x \"0px\"\n      :y \"0px\"\n      :width \"930px\"\n      :height \"50px\")\n  :stacking \"bg\"\n  :reserve \n    (struts \n      :distance \"50px\" \n      :side \"top\")\n...\n</code></pre> <p>\u6700\u7ec8\u6548\u679c\uff0c\u9999\u8549\u662f\u9f20\u6807\u6307\u9488</p> <p></p> <p>\u6700\u65b0\u914d\u7f6e\u5728github</p> <p>\u7cfb\u7edf\u8d44\u6e90\u5360\u7528</p> <p></p> <p>\u53ef\u4ee5\u770b\u51fa\uff0cplasma\u672c\u8eab\u6ca1\u6709\u592a\u5938\u5f20\uff0cX\uff0ceww\uff0ckwin\u5c31\u8d21\u732e\u4e86\u4e0d\u5c11\u3002brave\u6d4f\u89c8\u5668 \u4e5f\u5360\u4e86\u53ef\u89c2\u7684\u4e00\u90e8\u5206\uff0c\u4f46\u8fd8\u662f\u6bd4chrome\u597d\u591a\u3002\u5982\u679c\u4e0d\u505a\u4efb\u4f55\u7279\u6b8a\u4efb\u52a1\uff0c\u57fa\u672c\u5185\u5b58 \u6d88\u8017\u5927\u7ea63-4G\u3002\u5982\u679c\u8981\u4f7f\u7528IDE\u6216\u8005\u5927\u90e8\u5934\u5e94\u7528\uff0c\u5c31\u53e6\u8bf4\u4e86\u3002</p>"},{"location":"tech/archlinux_config/#zsh","title":"zsh","text":"<p>\u706b\u8d77\u6765\u7684bash\u66ff\u4ee3\u54c1\uff0c\u667a\u80fd\u5f88\u591a\uff0c\u63d2\u4ef6\u4e5f\u5f88\u591a\u3002\u6211\u5fd8\u8bb0\u4ece\u54ea\u91cc\u88c5\u7684\u4e86\uff0c\u4f46\u662f\u5b98\u7f51 \u662f\u8fd9\u91cc</p> <p>\u6bd4\u8f83\u6d41\u884c\u7684\u4e3b\u9898\u662fagnoster\uff0c\u6211\u7684\u5c0f\u4fee\u6539\u662f\u5220\u6389\u7528\u6237\u767b\u5f55\u4fe1\u606f\u3002\u5728.zshrc\u91cc\u672b\u5c3e \u52a0\u4e0a</p> <pre><code>prompt_context() {}\n</code></pre>"},{"location":"tech/archlinux_config/#kitty","title":"kitty","text":"<p>\u52a0\u4e86\u900f\u660e\u80cc\u666f\uff0c\u5e76\u4e14\u628ashell\u6539\u6210\u4e86zsh\u3002\u5728.config/kitty/kitty.conf\u5199</p> <pre><code>background_opacity 0.85\nshell zsh\n</code></pre>"},{"location":"tech/archlinux_config/#_2","title":"\u4e2d\u6587","text":"<p>\u7528fcitx\u3002\u81ea\u4ecekde plasma\u6362\u6210wayland\u540e\uff0c\u7a81\u7136\u53ea\u6709\u6d4f\u89c8\u5668\u91cc\u80fd\u5207\u6362\u4e86\uff0c\u89e3\u51b3\u529e \u6cd5\u662f</p> <pre><code># /etc/environment\nGTK_IM_MODULE=fcitx\nQT_IM_MODULE=fcitx\nXMODIFIERS=@im=fcitx\nSDL_IM_MODULE=fcitx\nGLFW_IM_MODULE=fcitx\n</code></pre> <p>\u6216\u8005</p> <pre><code># ~/.xprofile\nexport XIM=fcitx\nexport XIM_PROGRAM=fcitx\nexport GTK_IM_MODULE=fcitx\nexport QT_IM_MODULE=fcitx\nexport XMODIFIERS=\"@im=fcitx\"\neval `dbus-launch --sh-syntax --exit-with-session`\nexec fcitx &amp;\n</code></pre> <p>\u6211\u4e24\u4e2a\u90fd\u6539\u4e86\uff0c\u91cd\u542f\u7535\u8111\u5c31\u597d\u4e86\u3002\u53ef\u80fd\u53ea\u9700\u8981\u6539\u4e00\u4e2a\u3002</p>"},{"location":"tech/archlinux_config/#_3","title":"\u7ec8\u7aef\u4e0b\u7684\u6587\u4ef6\u5939\u80cc\u666f\u8272","text":"<p>\u9ed8\u8ba4\u914d\u7f6e\u4e0b\uff0cls\u5982\u679c\u9047\u5230\u6743\u9650\u8fc7\u5bbd\u7684\u6587\u4ef6\u5939\uff08\u6bd4\u5982777\uff09\uff0c\u80cc\u666f\u4f1a\u88ab\u9ad8\u4eae\u3002\u8fd9\u5bf9 \u4e8e\u5f3a\u5236\u5f00\u653ewindows\u5206\u533a\u5171\u4eab\u7684\u53cc\u7cfb\u7edf\u5c31\u5f88\u4e0d\u65b9\u4fbf\u3002\u53ef\u4ee5\u6309\u7167\u8fd9\u4e2a\u7b54 \u6848  \u89e3\u51b3\u3002</p> <p>\u540c\u65f6\u4e5f\u9700\u8981\u6539\u53d8zsh\u81ea\u52a8\u8865\u5168\u7528\u91c7\u7528\u7684\u914d\u8272\uff1a</p> <pre><code>zstyle ':completion:*' list-colors \"${(s.:.)LS_COLORS}\"\n</code></pre> <p>\u5c31\u6e05\u51c0\u4e86\uff0c\u6ca1\u6709\u70e6\u4eba\u7684\u9ad8\u4eae\u8272\u4e86</p>"},{"location":"tech/archlinux_config/#emacs","title":"emacs","text":"<p>\u900f\u660e\u80cc\u666f\u5982\u4e0b\u5b9e\u73b0\uff1a</p> <pre><code>(set-frame-parameter nil 'alpha-background 85)\n(add-to-list 'default-frame-alist '(alpha-background . 85))\n</code></pre>"},{"location":"tech/archlinux_config/#nano","title":"nano","text":"<p>\u662f\u7684\uff0cnano\u4e5f\u53ef\u4ee5\u914d\u7f6e\uff0c\u6bd4\u5982\u8bed\u6cd5\u9ad8\u4eae\uff0c\u81ea\u52a8\u6362\u884c\u7b49\u7b49\u3002\u914d\u7f6e\u6587\u4ef6\u662f <code>/etc/nanorc</code>\uff0c\u9ad8\u4eae\u53c2\u8003\u8fd9 \u91cc\u3002\u6211\u8fd8\u5b89 \u88c5\u4e86nano-syntax-highlighting\u5305\uff0c\u4e5f\u8bb8\u4e0d\u662f\u5fc5\u987b\u7684\u3002</p>"},{"location":"tech/archlinux_config/#_4","title":"\u5176\u5b83","text":"<p>\u6709\u4e9b\u7a0b\u5e8f\u5728\u56fe\u5f62\u754c\u9762\u53ef\u4ee5\u8bbe\u7f6e\u900f\u660e\u80cc\u666f\uff0c\u6bd4\u5982vscode\u901a\u8fc7\u63d2\u4ef6\uff0ckonsole,tilix\uff0c \u5c31\u4e0d\u8d58\u8ff0\u4e86\u3002</p> <p>\u4e00\u7cfb\u5217\u64cd\u4f5c\u5b8c\u6bd5\u540e\uff0c\u900f\u660e\u80cc\u666f\u8fbe\u6210\uff0c\u53ef\u4ee5\u968f\u65f6\u770b\u5230\u4e00\u70b9\u70b9\u597d\u770b\u7684\u684c\u9762\u80cc\u666f\u3002</p> <p>\u6700\u7ec8\u6548\u679c</p> <p></p>"},{"location":"tech/archlinux_config/#_5","title":"\u4e0d\u6298\u817e\u4e0d\u51fa\u95ee\u9898\uff0c\u4e00\u6298\u817e\u5c31\u51fa\u7684\u65b0\u95ee\u9898","text":"<p>\u5f85\u89e3\u51b3\uff1a\u4efb\u4f55tilinig wm\u90fd\u65e0\u6cd5\u4f7f\u7528\uff0c\u800c\u4e14\u88c5\u6765\u88c5\u53bb\u8fd8\u628aSDDM\u641e\u574f\u4e86</p> <p>\u73b0\u72b6\u662f\uff0cSDDM\u4e0b\u65e0\u6cd5\u9009\u62e9window manager/desktop environment\uff0c\u53ea\u5269\u4e2aplasma x11\u5149\u6746\u53f8\u4ee4\u3002\u51fa\u73b0\u8fc7\u7684\u53e6\u4e00\u4e2a\u95ee\u9898\u662f\uff0c\u767b\u9646\u8fdb\u53bb\u5565\u4e5f\u4e0d\u80fd\u5e72\uff0c\u5305\u62ec\u4f7f\u7528\u7f51\u4e0a\u641c \u5230\u7684\u70ed\u952e\u4e5f\u6ca1\u6709\u54cd\u5e94\u3002\u7814\u7a76\u4e00\u756a\u53d1\u73b0\uff0c\u8fd9\u4e2a\u662f\u4e24\u4e2a\u95ee\u9898\u7684\u7ec4\u5408\uff0c1.tiling WM\u65e0 \u6cd5\u4f7f\u7528\uff0c2. .xinitrc\u88ab\u4fee\u6539\u5e76\u9ed8\u8ba4\u8fdb\u53bb\u90a3\u4e2a\u4e0d\u80fd\u4f7f\u7528\u7684wm\u3002\u65e2\u7136\u5982\u6b64\uff0c\u5c31\u4fee\u6539~/.xinitrc</p> <pre><code>#!/bin/sh\nexec startplasma-x11\n</code></pre>"},{"location":"tech/archlinux_win/","title":"Dual boot arch linux + win 11","text":""},{"location":"tech/archlinux_win/#windowsarch-linux","title":"\u57fa\u672c\u7cfb\u7edf\u5b89\u88c5\uff1a\u53cc\u542f\u52a8windows\u548carch linux","text":"<p>\u6211\u53c2\u8003\u7684\u5e94\u8be5\u662f\u8fd9\u4e2a\u6559\u7a0b \u548c\u8fd9\u4e2a</p> <p>\u7b14\u8bb0\u672c\u51fa\u5382\u81ea\u5e26windows\uff0c\u4fdd\u7559windows\u7684\u60c5\u51b5\u4e0b\u52a0\u585earch linux\u3002\u5b98\u7f51\u4e0b\u8f7d\u5b89\u88c5 \u7528\u7684iso\uff0c\u4e00\u6b65\u6b65\u8ddf\u7740\u63d0\u793a\u8d70\uff0c\u6709\u95ee\u9898\u67e5arch wiki\u5c31\u5dee\u4e0d\u591a\u4e86\u3002\u51e0\u4e2a\u6ce8\u610f\u70b9\uff1a</p> <ul> <li>\u7ed9linux\u7559\u591a\u4e00\u70b9\u5206\u533a\u3002\u4ee5\u540e\u5acc\u591a\u4e86\u7f29\u5c0f\u5bb9\u6613\uff0c\u5acc\u5c0f\u4e86\u6269\u5c55\u96be\u3002\u6211\u5efa\u8bae\u81f3\u5c11\u7559    \u4e2a200G\u3002\u5982\u679c\u72af\u4e86\u8ddf\u6211\u4e00\u6837\u7684\u89c4\u5212\u9519\u8bef\uff0c\u53ef\u4ee5\u53c2\u8003\u8fd9\u7bc7\u6765\u4fee\u6b63\u3002</li> <li>\u7ed9/boot\u4e5f\u591a\u7559\u70b9\u3002/boot\u662ffat32\uff0c\u6211\u81f3\u4eca\u6ca1\u627e\u5230\u5b89\u5168resize\u7684\u529e\u6cd5\u3002\u7f51\u4e0a\u6709    \u4e9b\u5730\u65b9\u63a8\u8350\u7684512\u53ea\u80fd\u8bf4\u52c9\u5f3a\u591f\u7528\uff0c\u5efa\u8bae\u7559\u5e72\u8106\u7559\u51e0\u4e2aG\u3002</li> <li>\u88c5linux\u4e4b\u524d\u5148\u5728windows\u91cc\u767b\u9646\u8d26\u6237\u628a\u673a\u5668\u5173\u8054\u4e0a\uff0c\u786e\u8ba4\u4e00\u4e0b\u8d26\u53f7\u4e0a\u80fd\u627e\u5230    recovery key\uff0c\u7ffb\u8f66\u5927\u84dd\u5c4f\u7684\u65f6\u5019\u9700\u8981\u7528\u3002</li> <li>\u786c\u76d8\u5206\u533a\u540d\u79f0\u6bcf\u53f0\u7535\u8111\u4e0d\u4e00\u6837\uff0c\u6ce8\u610f\u4fee\u6539</li> <li>fdisk\u64cd\u4f5c\u7684\u65f6\u5019\u770b\u6e05\u695a\uff0c\u4e0d\u8981\u624b\u6ed1\u628awindows\u5206\u533a\u683c\u5f0f\u5316\u4e86</li> <li>\u7eb8\u4e0a\u8bb0\u4e0b\u6bcf\u4e2a\u5206\u533a\u53f7\uff0c\u82e5\u5e72\u91cd\u8981\u5730\u65b9\u9700\u8981\u7528</li> <li>\u542f\u52a8\u5217\u8868\u7528edk2-shell\u6765\u914d\u7f6e\uff0c\u5f80linux\u88c5\u7684\u542f\u52a8\u914d\u7f6e\u91cc\u52a0windows\u3002\u8ddf\u7740\u8fd9\u4e2a\u89c6    \u9891\u53ef\u4ee5\u65e0\u8111\u6284\u7b54\u6848\u3002</li> <li>bitlocker\u5f88\u5bb9\u6613\u88ab\u89e6\u53d1\uff0c\u9700\u8981\u7528recovery key\u6765\u89e3\u9501\u8fdb\u5165windows\u3002\u5728win\u4e0b\u9762\u628aencryption\u6574\u4e2a    disable\uff0c\u53ef\u4ee5\u89e3\u51b3\u4e00\u90e8\u5206\u95ee\u9898\uff0c\u4f46\u6709\u65f6\u5019\u8fd8\u662f\u83ab\u540d\u5176\u5999windows\u7684\u542f\u52a8\u5206\u533a\u53d8\u4e3a\u9996\u9009\uff0c\u76f4\u63a5\u8df3\u8fc7linux\uff0c    \u9700\u8981\u91cd\u542f\u8fdbBIOS\u6539\u3002</li> <li>\u7741\u773c\u7761\u89c9\u7684\u95ee\u9898\uff1a\u9ed8\u8ba4\u884c\u4e3a\u7684\u4f11\u7720\u662f\u5047\u4f11\u7720\uff0c\u7535\u6c60\u6389\u7684\u98de\u5feb\uff0c\u9192\u6765\u673a\u5668\u4e5f\u662f    \u70ed\u7684\u3002\u9700\u8981\u5982\u4e0b\u4fee\u6539\uff1a<ul> <li><code>cat /sys/power/mem_sleep</code>\u3002\u51fa\u6765\u7684\u7ed3\u679c\u5e94\u8be5\u662f<code>[s2idle] deep</code>\uff0c\u8bf4\u660e  \u76ee\u524d\u662f\u6d45\u5c42\u7761\u7720s2ide\u6a21\u5f0f\uff0c\u4f46\u662f\u652f\u6301\u4f11\u7720deep\u6a21\u5f0f\u3002</li> <li><code>echo deep | sudo tee /sys/power/mem_sleep</code>\u3002\u518d\u6b21\u67e5\u770bmem_sleep\uff0c  \u51fa\u6765\u7684\u7ed3\u679c\u5c31\u5e94\u8be5\u662f<code>s2idle [deep]</code>\u4e86\u3002</li> <li>\u6211\u6ce8\u610f\u5230\u4ee5\u4e0a\u6587\u4ef6\u4f1a\u81ea\u5df1\u5207\u56de\u53bb\uff0c\u4e0d\u77e5\u9053\u4e3a\u5565\u3002\u5982\u679c\u51fa\u73b0\u8fd9\u79cd\u4e8b\uff0c\u5c31\u624b\u52a8   \u4fee\u6539<code>/etc/systmd/sleep.conf</code>\uff0c\u52a0\u4e0a<code>MemorySleepMode=deep</code></li> </ul> </li> <li>\u6211\u7684\u672c\u5b50\u662fasus rog strix\uff0c\u86cb\u75bc\u7684\u6807\u914d\u662fsleep/hibernate\u6a21\u5f0f\u4f9d\u7136\u4f1a\u4eae\u706f\u3002    \u6309\u7406\u8bf4\u8fdbwindows armory crate\u80fd\u5173\u6389\u591c\u706f\uff0c\u4f46\u662f\u6211\u53ea\u627e\u5230\u4e86\u7535\u6c60\u6a21\u5f0f\u7684\u4e0b    \u7684\u5173\u95ed\u3002\u4e8e\u662f\u56de\u5230arch\uff0c\u5b89\u88c5asusctrl\u3002\u5b89\u88c5\u672b\u5c3e\u4f1a\u62a5\u9519\uff0c\u4f46\u5176\u5b9e\u8fd8\u662f\u88c5\u4e0a    \u4e86\uff0c\u5229\u7528<code>asusctl aura-power lightbar</code>\uff0c\u6309\u7167-h\u7684\u6307\u793a\u5c31\u53ef\u4ee5\u5173\u6389\u4e86\u3002\u53ef    \u4ee5\u9009\u62e9\u7761\u89c9\u65f6\u5173\u706f\uff0c\u9192\u6765\u5f00\u706f\u3002</li> </ul> <p>\u529d\u9000\u70b9</p> <ul> <li>\u53cc\u7cfb\u7edf\u4e0b\u9762\u65e0\u6cd5\u8ba9dropbox\u7ba1\u7406\u5171\u4eab\u5206\u533a\uff0c\u56e0\u4e3adropbox\u5728win\u4e0a\u4e0d\u652f\u6301ext4\uff0c\u5728linux\u4e0a\u53ea\u652f\u6301ext4\u3002    \u8fd9\u4e2a\u672c\u8d28\u95ee\u9898\u662fdropbox\u53ea\u80fd\u7528native filesystem\u8ddf\u8e2a\u6587\u4ef6\u6709\u6ca1\u6709\u66f4\u65b0\uff0c\u6ca1\u6cd5hack\u3002</li> <li>\u7cfb\u7edf\u542f\u52a8\u4f9d\u7136\u6ca1\u6709\u5f88\u7a33\u5b9a\u5730\u6bcf\u6b21\u90fd\u8fdb\u5165linux boot menu\uff0c\u6709\u65f6\u76f4\u63a5\u8df3win\uff0c\u6709\u65f6\u5019\u84dd\u5c4f\uff08bitlocker\uff09\u3002</li> </ul>"},{"location":"tech/archlinux_win/#_1","title":"\u57fa\u672c\u5de5\u5177","text":"<p>\u5728\u8fd9\u7bc7\u91cc\u53ef\u4ee5\u627e\u5230</p>"},{"location":"tech/archlinux_win/#_2","title":"\u5171\u4eab\u786c\u76d8","text":"<p>\u5171\u4eab\u786c\u76d8\uff0c\u5305\u62ec\u52a0\u88c5\u7684\u7b2c\u4e8c\u5757\u786c\u76d8\uff0c\u90fd\u6ca1\u6709\u51fa\u573a\u5373\u7528\u7684\u8bfb\u5199\u652f\u6301\u3002\u539f\u7406\u662f windows\u5206\u533antfs\u65e0\u6cd5\u7406\u89e3linux\u4e0b\u7684\u6743\u9650\uff0c\u6240\u4ee5\u5e72\u8106\u5c31\u4ee5\u53ea\u8bfb\u6a21\u5f0f\u6302\u8f7d\u3002\u89e3\u6cd5\u662f \u5b89\u88c5ntfs-3g\u5e76\u4e14\u4fee\u6539/etc/fstab\u3002</p> <p>\u8001\u89c4\u77e9\uff0c\u5148\u627eUUID</p> <pre><code>sudo fdisk -l # \u65b9\u4fbf\u67e5\u770b\u54ea\u4e2a\u662f\u5171\u4eab\u5206\u533a/\u76d8\nsudo blkid  # \u5bfb\u627e\u5bf9\u5e94\u7684UUID\n</code></pre> <p>\u518d\u627e\u7528\u6237\u7684UID, GID</p> <pre><code>id\n</code></pre> <p>\u4fee\u6539/etc/fstab\u3002\u9009\u4e00\u4e2a\u559c\u6b22\u7684\u6302\u8f7d\u70b9\u3002\u6211\u4e00\u822c\u5c31\u7528<code>/home/$USER/&lt;windows\u4e0b\u7684 \u5206\u533a\u540d\u5b57&gt;</code>\u3002\u5176\u4e2dfmask\u53ef\u66ff\u6362\u4e3a\u60f3\u8981\u7684\u9ed8\u8ba4\u6587\u4ef6\u6743\u9650\u3002111\u8868\u793a\u5168\u5c40\u8bfb\u5199\u4f46\u4e0d\u53ef \u4ee5\u6267\u884c\uff0c\u5fae\u91cf\u517c\u987e\u65b9\u4fbf\u548c\u5b89\u5168\u3002</p> <pre><code>UUID=&lt;uuid&gt;   &lt;mount path&gt;     ntfs-3g defaults,user,uid=1000,gid=1000,fmask=111,dmask=000,nofail,rw,big_write 0       0\n</code></pre> <p>\u6700\u9ebb\u70e6\u7684\u90e8\u5206\u6765\u4e86\uff1a\u6bcf\u6b21windows\u5173\u673a\u8981\u5b8c\u5168\u5173\u673a\uff0c\u6309\u7740shift\u4ece\u7cfb\u7edf\u9009\u9879\u91cc\u5173\u673a\u3002 arch btw\u4f60\u8d62\u4e86\uff0c\u6211\u518d\u4e5f\u4e0d\u5f00windows\u884c\u4e86\u53e3\u5df4\u3002</p> <p>\u91cd\u8981\uff1a\u5982\u6b64\u6253\u8865\u4e01\u7684\u5206\u533asteam\u548cdropbox\u90fd\u4e0d\u8ba4\u3002\u6211\u540e\u6765\u91cd\u65b0\u683c\u6210etx4\u4e86\uff0c\u683c\u4e86 \u4e4b\u540eUUID\u4f1a\u53d8\uff0c\u8981\u4fee\u6539fstab\u91cd\u65b0\u6302\u8f7d\u3002</p>"},{"location":"tech/archlinux_win/#git","title":"\u914d\u7f6egit","text":"<ul> <li>\u9996\u5148\u9700\u8981\u62ff\u5230access token\u3002\u8fdb\u5165Settings -&gt; Developer Settings -&gt;    Personal access tokens\u3002\u6211\u9009\u62e9\u7684\u662fPersonal access tokens (classic)</li> <li><code>git config --global credential.helper store</code>\u3002</li> </ul>"},{"location":"tech/archlinux_win/#steam","title":"steam","text":"<p>\u9ed8\u8ba4\u7684\u5b89\u88c5\u6ca1\u6709\u95ee\u9898\uff0c\u4f46\u662f\u663e\u5361\u8981\u5904\u7406\u4e00\u4e0b\uff0cnvidia\u7684\u3002\u4e00\u8fdb\u53bb\u5361\u6210\u72d7\uff0c\u7ed3\u679c\u662f \u72ec\u663e\u6ca1\u8ba4\u51fa\u6765\u3002\u67e5\u770b\uff1a</p> <pre><code>nvidia-smi\n</code></pre> <p>\u6b63\u5e38\u60c5\u51b5\u5e94\u8be5\u4f1a\u5410\u51fa\u4e00\u5806\u6570\u636e\uff0c\u4f46\u6211\u7684\u62a5\u9519\uff0c\u8bf4driver library version mismatch\uff0c\u4e8e\u662f\u91cd\u88c5\uff1a</p> <pre><code>sudo pacman -S nvidia-dkms nvidia-utils\nsudo reboot\n</code></pre> <p>\u91cd\u542f\u4e4b\u540e\u5c31\u597d\u4e86\uff0c\u535a\u5fb7\u4e4b\u95e83\u8dd1\u8d77\u6765\u4e86\u3002\u82b1\u4e86\u534a\u4e2a\u591a\u5c0f\u65f6\u634f\u4e86\u4e2a\u5973\u88c5\u5927\u4f6c\u5fb7\u9c81\u4f0a \u5f00\u5c40\uff0c\u4e94\u5206\u949f\u521d\u89c1\u6740\uff0c\u8d70\u4e24\u6b65\u5c31\u6302\u4e86\u3002</p> <p>\u53e6\u4e00\u4e2a\u95ee\u9898\u662f\uff0c\u56fe\u50cf\u663e\u793a\u50cf\u7d20\u5316\u4e25\u91cd\u5e76\u4e14\u989c\u8272\u4e0d\u5bf9\uff0claunch option\u52a0\u4e0a\uff1a</p> <pre><code>--graphics Vulkan\n</code></pre> <p>\u91cd\u542f\u6e38\u620f</p>"},{"location":"tech/archlinux_win/#_3","title":"\u5176\u5b83\u95ee\u9898\u89e3\u51b3\u65b9\u6848","text":"<p>/boot\u65e0\u6cd5\u6269\u5bb9\u5bfc\u81f4\u7684\u66f4\u65b0\u5931\u8d25\uff1a512mb\u7684\u5206\u533a\u4e0d\u591f\u7528\u600e\u4e48\u529e\uff1f\u7a7a\u95f4\u5360\u7528\u6700\u5927\u7684\u662f fallback image\uff0c\u8fd9\u4e2a\u4e0d\u63a8\u8350\u5220\u4f46\u662f\u53ef\u4ee5\u5220\u3002\u4e0d\u63a8\u8350\u662f\u56e0\u4e3a\u5982\u679c\u7cfb\u7edf\u574f\u6389\u53ef\u4ee5\u6709 \u4e2a\u5907\u7528\u7684\u7528\u6765\u4fee\uff0c\u4f46\u662f\u5982\u679c\u574f\u6389\u6211\u4e0d\u5982\u5e72\u8106\u91cd\u88c5\u987a\u4fbf\u628a/boot\u683c\u4e86\u91cd\u65b0\u641e\u4e2a\u5927\u7684\u3002 \u5220\u6389\u4e4b\u524d\u9700\u8981\u6539\u4e2a\u914d\u7f6e\uff1a</p> <p><code>etc/mkinitcpio.d/linux.preset</code></p> <p>\u9ed8\u8ba4\u8bbe\u5b9a\u662f<code>PRESETS=('default' 'fallback')</code>\uff0c\u6539\u6210<code>PRESETS=('default')</code>\uff0c \u628afallback\u76f8\u5173\u7684\u8bbe\u5b9a\u90fd\u6ce8\u91ca\u6389\uff0c\u5c31\u53ef\u4ee5\u5220\u6389initramfs-linux-fallback.img \u4e86</p>"},{"location":"tech/dynamic_wallpaper/","title":"\u52a8\u6001\u767b\u9646\u80cc\u666f\uff0c\u9501\u5c4f\u80cc\u666f\u56fe","text":"<p>\u7528\u7684\u662fSDDM\u663e\u793a\u7ba1\u7406\u5668\uff0c\u4f46\u662f\u57fa\u672c\u539f\u7406\u548c\u64cd\u4f5c\u5e94\u8be5\u662f\u5171\u901a\u7684\u3002</p> <p>\u9996\u5148\uff0c\u91cd\u8981\u8def\u5f84\u662f<code>/usr/share/sddm/themes/breeze</code>\uff0c\u5176\u4e2d\u7684breeze\u6309\u9700\u66ff\u6362 \u4e3a\u4e3b\u9898\u540d\u5b57\u3002\u8fd9\u4e2a\u6587\u4ef6\u5939\u4e0b\u6709\u4e24\u79cd\u914d\u7f6e\u6587\u4ef6\uff0c<code>theme.conf</code>\u548c <code>theme.conf.user</code>\u3002\u80cc\u666f\u56fe\u7247\u7531<code>background=&lt;path&gt;</code>\u6765\u5236\u5b9a\u3002\u4e0d\u786e\u5b9a\u7684\u8bdd\u5c31\u90fd \u6539\u3002\u8fd9\u4e2apath\u8c8c\u4f3c\u8fd8\u4e0d\u80fd\u662f\u7279\u5b9a\u7528\u6237\u4e0b\u7684<code>/home/&lt;username&gt;</code>\u91cc\uff0c\u6240\u4ee5\u6211\u9009\u62e9\u4fdd \u7559\u539f\u672c\u7684\u56fe\u7247\u8def\u5f84\u6765\u8986\u76d6\u3002</p> <p>lock screen\u7684\u9ed8\u8ba4\u80cc\u666f\u8def\u5f84\u5728\u54ea\u4e2aconfig\u91cc\u5199\u7684\uff0c\u6211\u6123\u662f\u6ca1\u627e\u5230\uff0c\u4e0d\u8fc7\u53ef\u4ee5\u5728 \u7cfb\u7edf\u8bbe\u7f6eScreen Locking\u91cc\u628a\u56fe\u7247\u8def\u5f84\u6539\u6210\u81ea\u5df1\u7684\u3002</p> <p>\u867d\u7136\u4f9d\u9760\u7cfb\u7edf\u8bbe\u7f6e\u53ef\u4ee5\u6539\u80cc\u666f\uff0c\u4e0d\u8fc7\u4e0d\u80fd\u5b9e\u73b0\u8fd9\u4e2a\u86cb\u75bc\u9700\u6c42\uff1a\u968f\u673a\u5207\u6362\u80cc\u666f\uff0c\u8fbe \u6210\u6bcf\u6b21\u4f11\u7720/\u5f00\u673a/\u767b\u51fa\u540e\u90fd\u6709\u60ca\u559c\u3002\u6211\u7684\u601d\u8def\u662f\uff0c\u6bcf\u6b21\u767b\u51fa\u7684\u65f6\u5019\u8dd1\u4e00\u4e2ascript\uff0c \u968f\u673a\u590d\u5236\u4e00\u4e2a\u56fe\u7247\u66ff\u6362\u6389conf\u91cc\u5199\u6b7b\u7684\u56fe\u7247\u3002\u4e3a\u4e86\u4fdd\u8bc1\u6743\u9650\uff0c\u6211\u628a\u8fd9\u4e2a\u56fe\u7247\u6539\u6210 \u4e86\u5f00\u653e\u6743\u9650\uff1a<code>sudo chmod a+w wallpaper.jpg</code>\u8fd9\u6837\u7684\u3002</p> <p>\u603b\u7ed3\u4e0b\u6765\uff0c\u6211\u9700\u8981\u8ba9\u4e00\u4e2ascript\u968f\u673a\u4fee\u6539\u4e24\u4e2a\u56fe\u7247\uff1a</p> <pre><code>/usr/share/sddm/themes/sequoia_2/sddm.jpg\n/home/$USER/.config/wallpaper/lock.jpg\n</code></pre> <p>\u4ee5\u4e0b\u8fd9\u6837\u5c31\u53ef\u4ee5\u4e86\uff0c\u5b58\u4e3alogout.sh\uff0c\u6539\u4e3a\u53ef\u6267\u884c<code>chmod +x logout.sh</code>\uff0c\u5728\u7cfb \u7edf\u8bbe\u7f6e\uff0cautostart\u91cc\uff0c\u52a0\u5165\u767b\u51fa\u65f6\u8fd0\u884c\u811a\u672c\u5373\u53ef\u3002</p> <pre><code>#!/bin/bash\n\npath=/home/yli/.config/wallpaper/candidates\n\nlock=$(ls $path | shuf -n 1)\n\nsddm=$(ls $path | shuf -n 1)\n\ncp \"$path/$sddm\" /usr/share/sddm/themes/sequoia_2/sddm.jpg\ncp \"$path/$lock\" /home/yli/.config/wallpaper/lock.jpg\n</code></pre> <p>\u540e\u6765\u53c8\u52a0\u4e86\u4e00\u4e2asystemd job\uff0c\u6bcf\u5c0f\u65f6\u8dd1\u4e00\u6b21\uff0c\u8fd9\u6837\u4e0d\u9700\u8981\u767b\u51fa\u4e5f\u4f1a\u81ea\u52a8\u66f4\u65b0\u56fe\u7247\u3002 KDE\u53ef\u4ee5\u7528\u81ea\u5e26\u7684\u8bbe\u7f6e\u5de5\u5177\u968f\u673a\u5207\u6362\u684c\u9762\uff0c\u5176\u5b83\u7684\u53ef\u80fd\u9700\u8981\u624b\u52a8\uff0c\u7528crontab\u6216\u8005 systemd\u5b9a\u65f6\u66f4\u65b0\u3002</p> <p>\u63d2\u64ad\uff1a\u5feb\u901f\u6539\u56fe\u7247\u683c\u5f0f\u53ef\u4ee5\u7528imageMagick\uff0c\u4f8b\u5b50\uff1a</p> <pre><code>magick sample.jpg sample.png\ndisplay sample.png # \u786e\u8ba4\u6210\u529f\n</code></pre>"},{"location":"tech/emacs/","title":"emacs\u914d\u7f6e","text":"<p>\u76f4\u63a5\u4e0a\u914d\u7f6e\u6587\u4ef6<code>~/.emacs.d/init.el</code>\uff1a</p> <p>\u5b9e\u73b0\u7684\u4e3b\u8981\u6709</p> <ul> <li>\u542f\u52a8\u65f6\u7a97\u53e3\u5927\u5c0f\u548c\u4f4d\u7f6e</li> <li>\u542f\u52a8\u65f6\u81ea\u52a8\u5f00\u4e0a\u4e0b\u5206\u5c4f\uff0c\u4e00\u4e2a\u663e\u793aTODO\uff0c\u4e00\u4e2a\u663e\u793a\u5f53\u5929\u65e5\u7a0b</li> <li>\u7528org-mode\u5b9e\u73b0\u9879\u76ee\u7ba1\u7406\uff0c\u76f4\u63a5\u6284\u4e86\u4e00\u4e2a\u5e26\u70b9\u81ea\u52a8\u5316\u7684\u3002\u611f\u89c9\u53ef\u4ee5\u6709\u5f88\u591a\u4f5c    \u4e1a\u53ef\u4ee5\u6284\uff0c\u8fd8\u5728\u7814\u7a76\u4e2d\u3002org-mode\u5176\u5b9e\u5728\u5de5\u4f5c\u4e0a\u7528\u7684\u66f4\u591a\uff0c\u6bd5\u7adf\u9700\u8981\u591a\u7ebf\u7a0b\uff0c    \u8bb0\u5f55deadline\u3002\u4e2a\u4eba\u7535\u8111\u4e0a\u6ca1\u6709\u5565\u662f\u5fc5\u987b\u505a\u7684\uff0c\u5c31\u662f\u628a\u9879\u76ee\u8981\u505a\u7684\u5217\u51fa\u6765\u6bd4    \u8f83\u65b9\u4fbf\u770b\u8fdb\u5ea6\u800c\u5df2\u3002\u529f\u80fd\u5305\u62ec\uff1a<ul> <li>\u5229\u7528\u5feb\u6377\u952e\u548c\u81ea\u52a8\u66f4\u65b0\u6765\u65b9\u4fbf\u7684\u7ba1\u7406TODO-&gt;DONE\u7684\u5207\u6362\u3002\u5b50\u9879\u76ee\u6709\u8fdb\u5ea6\u6539\u53d8      \u7684\u65f6\u5019\uff0c\u603b\u9879\u76ee\u7684\u8fdb\u5ea6\u4f1a\u81ea\u52a8\u66f4\u65b0</li> <li>\u505a\u5b8c\u7684\u9879\u76ee\u53ef\u4ee5\u7528\u5feb\u6377\u952e\u6254\u5230\u4e00\u4e2aarchive\u6587\u4ef6\u91cc\u3002\u65e5\u540e\u6d4f\u89c8\u8fd9\u4e2a\u6587\u4ef6\u53ef\u4ee5\u8ffd      \u6eaf\u4e4b\u524d\u90fd\u5e72\u4e86\u5565\u3002</li> </ul> </li> <li>\u5b57\u4f53\u662fHack\uff0c\u4e3b\u9898\u662fdracula</li> </ul> <p>\u9700\u8981\u989d\u5916\u624b\u52a8\u64cd\u4f5c\u7684\u90e8\u5206\uff1a\u7f3a\u5931\u7684\u5305\u88f9\u7528M-x package-install\u88c5\uff0c\u8f93\u5165\u5305\u88f9\u540d \u5b57\u53ef\u4ee5\u81ea\u52a8\u8865\u5168\u3002\u5982\u679c\u65e0\u6cd5\u8bfb\u53d6init.el\uff0c\u770b\u770b\u6839\u6587\u4ef6\u76ee\u5f55\u6709\u6ca1\u6709\u51b2\u7a81\u7684\u65e7 \u7248.eamcs\u914d\u7f6e\u6587\u4ef6\u3002</p> <pre><code>;; Disable GUI\n(tool-bar-mode -1)\n(menu-bar-mode -1)\n(setq use-dialog-box t)\n(setq use-file-dialog nil)\n(setq-default frame-title-format '(\"%b  -  GNU Emacs\"))\n\n;; Hide the startup screen\n(setq inhibit-startup-screen t)\n\n(setq completion-ignore-case t)\n(setq case-fold-search t)\n\n(set-frame-font \"Hack 14\")\n\n(add-hook 'text-mode-hook 'turn-on-auto-fill)\n\n;; case insensitive search\n(setq read-file-name-completion-ignore-case t\n      read-buffer-completion-ignore-case t\n      completion-ignore-case t\n      completion-category-defaults nil\n      completion-category-overrides nil\n      completion-styles '(basic))\n\n(load-theme 'solarized-dark t)\n\n;;dark mode\n(set-variable 'frame-background-mode 'dark)\n\n;;(custom-set-variables\n ;; custom-set-variables was added by Custom.\n ;; If you edit it by hand, you could mess it up, so be careful.\n ;; Your init file should contain only one such instance.\n ;; If there is more than one, they won't work right.\n ;;'(package-selected-packages '(emacs-color-theme-solarized standard-themes)))\n;;(custom-set-faces\n ;; custom-set-faces was added by Custom.\n ;; If you edit it by hand, you could mess it up, so be careful.\n ;; Your init file should contain only one such instance.\n ;; If there is more than one, they won't work right.\n ;;)\n\n(add-to-list 'load-path \"~/.emacs.d/auto-dark/\")\n(require 'auto-dark)\n(auto-dark-mode)\n\n(require 'package)\n(add-to-list 'package-archives\n             '(\"melpa-stable\" . \"https://stable.melpa.org/packages/\"))\n(package-initialize)\n(custom-set-variables\n ;; custom-set-variables was added by Custom.\n ;; If you edit it by hand, you could mess it up, so be careful.\n ;; Your init file should contain only one such instance.\n ;; If there is more than one, they won't work right.\n '(package-selected-packages\n   '(dracula-theme emacs-color-theme-solarized markdown-mode org-modern\n           standard-themes tangotango-theme)))\n(custom-set-faces\n ;; custom-set-faces was added by Custom.\n ;; If you edit it by hand, you could mess it up, so be careful.\n ;; Your init file should contain only one such instance.\n ;; If there is more than one, they won't work right.\n )\n\n(set-frame-parameter nil 'alpha-background 90)\n(add-to-list 'default-frame-alist '(alpha-background . 90))\n\n;;from https://christiantietze.de/posts/2021/02/emacs-org-todo-doing-done-checkbox-cycling/\n(setq org-agenda-files '(\"~/org\"))\n(setq org-todo-keywords\n      (quote ((sequence \"TODO(t)\" \"PROG(p)\" \"|\" \"DONE(d)\"))))\n\n(defun org-todo-if-needed (state)\n  \"Change header state to STATE unless the current item is in STATE already.\"\n  (unless (string-equal (org-get-todo-state) state)\n    (org-todo state)))\n\n(defun ct/org-summary-todo-cookie (n-done n-not-done)\n  \"Switch header state to DONE when all subentries are DONE, to TODO when none are DONE, and to DOING otherwise\"\n  (let (org-log-done org-log-states)   ; turn off logging\n    (org-todo-if-needed (cond ((= n-done 0)\n                               \"TODO\")\n                              ((= n-not-done 0)\n                               \"DONE\")\n                              (t\n                               \"PROG\")))))\n(add-hook 'org-after-todo-statistics-hook #'ct/org-summary-todo-cookie)\n\n(defun ct/org-summary-checkbox-cookie ()\n  \"Switch header state to DONE when all checkboxes are ticked, to TODO when none are ticked, and to DOING otherwise\"\n  (let (beg end)\n    (unless (not (org-get-todo-state))\n      (save-excursion\n        (org-back-to-heading t)\n        (setq beg (point))\n        (end-of-line)\n        (setq end (point))\n        (goto-char beg)\n        ;; Regex group 1: %-based cookie\n        ;; Regex group 2 and 3: x/y cookie\n        (if (re-search-forward \"\\\\[\\\\([0-9]*%\\\\)\\\\]\\\\|\\\\[\\\\([0-9]*\\\\)/\\\\([0-9]*\\\\)\\\\]\"\n                               end t)\n            (if (match-end 1)\n                ;; [xx%] cookie support\n                (cond ((equal (match-string 1) \"100%\")\n                       (org-todo-if-needed \"DONE\"))\n                      ((equal (match-string 1) \"0%\")\n                       (org-todo-if-needed \"TODO\"))\n                      (t\n                       (org-todo-if-needed \"PROG\")))\n              ;; [x/y] cookie support\n              (if (&gt; (match-end 2) (match-beginning 2)) ; = if not empty\n                  (cond ((equal (match-string 2) (match-string 3))\n                         (org-todo-if-needed \"DONE\"))\n                        ((or (equal (string-trim (match-string 2)) \"\")\n                             (equal (match-string 2) \"0\"))\n                         (org-todo-if-needed \"TODO\"))\n                        (t\n                         (org-todo-if-needed \"PROG\")))\n                (org-todo-if-needed \"PROG\"))))))))\n(add-hook 'org-checkbox-statistics-hook #'ct/org-summary-checkbox-cookie)\n\n;;show agenda at startup\n(add-hook 'serve-after-make-frame-hook 'org-agenda-list)\n\n(defun emacs-startup-screen ()\n  \"display agenda and all todos\"\n  (org-agenda nil \"n\"))\n(add-hook 'emacs-startup-hook #'emacs-startup-screen)\n\n;;open todo file at startup\n(find-file \"~/org/todo.org\")\n\n;; change initial height\n;; update for different monitor set up\n(setq initial-frame-alist\n      (append initial-frame-alist\n          '(\n        (height . 90)\n        (width . 120)\n        (left . 900)\n        )))\n\n;;https://brunoarine.com/blog/refresh-agenda-on-file-change/\n(defun my/redo-all-agenda-buffers ()\n  (interactive)\n  (dolist (buffer (buffer-list))\n    (when (derived-mode-p 'org-agenda-mode)\n      (org-agenda-maybe-redo))))\n\n(add-hook 'org-mode-hook\n          (lambda ()\n            (add-hook 'after-save-hook 'my/redo-all-agenda-buffers nil 'make-it-local)))\n</code></pre> <p>\u7565\u5fae\u6ede\u540e\u7684\u6700\u7ec8\u6548\u679c\uff0c\u6ca1\u6709\u66f4\u65b0\u622a\u56fe</p> <p></p>"},{"location":"tech/image_magick/","title":"\u7528ImageMagick\u6279\u5904\u7406\u56fe\u50cf","text":"<p>\u5199\u4e00\u4e2aMakefile\uff0c\u4e13\u95e8\u51c6\u5907\u4e00\u4e2a\u6587\u4ef6\u5939\u653e\u9700\u8981\u5904\u7406\u7684\u56fe\u7247\uff0c\u5728\u8fd9\u4e2a\u6587\u4ef6\u5939\u7ec8\u7aef\u8dd1</p> <pre><code>make small # \u5168\u90e8\u56fe\u7247\u751f\u6210640x480\u5927\u5c0f\u7684\u7248\u672c\uff0c\u5e76\u52a0\u4e0asmall\u524d\u7f00\nmake medium # \u540c\u7406\nmake all # \u751f\u6210480x360\u7684\u7279\u5c0f\u7248\u672c\u4f5c\u4e3athumb nail\u4f7f\u7528\uff0c\u5e76\u6302\u4e0ath\u524d\u7f00\nmake frame # \u56fe\u7247\u52a0\u4e0a\u9ed1\u8272\u7ec6\u8fb9\u6846\uff0c\u5e76\u52a0\u4e0a\u524d\u7f00f\nmake fancy # \u597d\u50cf\u662f\u56fe\u7247\u52a0\u4e0a\u9ed1\u8272\u5e26\u9634\u5f71\u7684\u8fb9\u6846\n</code></pre> <p>\uff08\u8865\u5145\uff1a\u7531\u4e8e\u7248\u672c\u8fed\u4ee3\uff0c\u9700\u8981\u987a\u5e94\u65b0\u7248\u7684imagemagick\uff0c\u4fee\u6539\u4e00\u4e0b\u4ee3\u7801\uff09</p> <pre><code>#conver image size\nall:\n    @for k in *.jpg; \\\n        do \\\n        echo $$k ; \\\n        name=`echo $$k | cut -f1 -d.`; \\\n        convert -geometry 480x360 $$k th$${name}.jpg; \\\n        done\nsmall:\n    @for k in *.jpg; \\\n        do \\\n        echo $$k ; \\\n        name=`echo $$k | cut -f1 -d.`; \\\n        convert -geometry 640x480 $$k small$${name}.jpg; \\\n        done\nmedium:\n    @for k in *.jpg; \\\n        do \\\n        echo $$k ; \\\n        name=`echo $$k | cut -f1 -d.`; \\\n        convert -geometry 800x600 $$k medium$${name}.jpg; \\\n        done\nframe:\n    @for k in *.jpg; \\\n        do \\\n        echo $$k ; \\\n        name=`echo $$k | cut -f1 -d.`; \\\n        magick $$k -bordercolor black -border 1 -depth 8 -colors 256 -quality 90 f$${name}.jpg; \\\n        done\n\nfancy:\n    @for k in *.jpg; \\\n        do \\\n        echo $$k ; \\\n        name=`echo $$k | cut -f1 -d.`; \\\n        convert -bordercolor black -border 1 $$k -background none \\( +clone -shadow 80x2+2+2 \\) +swap -background white -flatten -depth 8 -colors 256 -quality 90 f$${name}.jpg; \\\n        done\nfancy5:\n    @for k in *.jpg; \\\n        do \\\n        echo $$k ; \\\n        name=`echo $$k | cut -f1 -d.`; \\\n        convert -geometry 50%x50% -bordercolor black -border 1 $$k -background none \\( +clone -shadow 80x2+2+2 \\) +swap -background white -flatten -depth 8 -colors 256 -quality 90 f$${name}.jpg; \\\n        done\nfancy4:\n    @for k in *.jpg; \\\n        do \\\n        echo $$k ; \\\n        name=`echo $$k | cut -f1 -d.`; \\\n        convert -geometry 40%x40% -bordercolor black -border 1 $$k -background none \\( +clone -shadow 80x2+2+2 \\) +swap -background white -flatten -depth 8 -colors 256 -quality 90 f$${name}.jpg; \\\n        done\n\n</code></pre>"},{"location":"tech/mangowc/","title":"\u914d\u7f6e\u5e73\u94fa\u5f0f\u7a97\u53e3\u7ba1\u7406\u5668Mangowc","text":"<p>\u5bf9\u5e73\u94fa\u5f0f\u7a97\u53e3\u7ba1\u7406\u5668\u6211\u4e00\u76f4\u8dc3\u8dc3\u6b32\u8bd5\uff0c\u4f46\u662f\u6ca1\u6709\u5f88\u5408\u9002\u7684\u5207\u5165\u70b9\uff0c\u4e0d\u60f3\u5b8c\u5168\u4ece\u96f6 \u914d\u8d77\u3002\u4f46\u662fmangowc\u5438\u5f15\u4e86\u6211\u7684\u6ce8\u610f\uff0c\u7b80\u6d01\uff0c\u5bb9\u6613\u914d\u7f6e\uff0c\u5f00\u76d2\u7248\u672c\u5c31\u633a\u597d\u770b\u4e86\u3002 \u6700\u96be\u7684\u662f\uff0c\u4e2d\u6587\u652f\u6301\u987a\u6ed1\uff0c\u5168\u5c40\u8dd1\u4e00\u4e2afcitx5\uff0c\u5728\u6240\u6709\u5e94\u7528\u91cc\u90fd\u53ef\u4ee5\u8f93\u5165\u4e2d\u6587\uff0c \u5305\u62ec\u7ec8\u7aef\u3002\u8ba9\u6211\u8ba4\u771f\u4ecekde plasma6\u5207\u6362\u5230mangowc\u7684\u662f\uff0c\u5728plasma6\u4e0b\u6211\u628abrave \u7684\u4e2d\u6587\u8f93\u5165\u6eda\u6302\u4e86\uff0c\u7f51\u4e0a\u627e\u4e0d\u5230\u89e3\u51b3\u65b9\u6848\uff0c\u6bd5\u7adf\u672c\u6765brave\u5c31\u662f\u5c0f\u4f17\u3002</p> <p>mangowc\u7684\u7f3a\u70b9\uff1a\u6587\u6863\u975e\u5e38\u5c11\uff0c\u6709\u95ee\u9898\u4e00\u641c\uff0c\u51fa\u6765\u7684\u7ed3\u679c\u90fd\u662f\u8bb2\u6c34\u679c\u7684\u3002\u53ea\u80fd\u5f80 \u4e0a\u6e38\u627e\u653b\u7565\uff0c\u641cdwl, wayland\u76f8\u5173\u7684\u89e3\u51b3\u65b9\u6848\u3002</p>"},{"location":"tech/mangowc/#_1","title":"\u786c\u4ef6","text":""},{"location":"tech/mangowc/#_2","title":"\u952e\u76d8\u7ed1\u5b9a","text":"<ul> <li>SUPER+r\uff0c\u91cd\u65b0\u52a0\u8f7dmangowc\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u4fee\u6539\u5b8c\u540e\u4fdd\u5b58\uff0c\u5237\u65b0\uff0c\u65e0\u9700\u767b\u51fa\u3002</li> </ul> <p>\u952e\u76d8\u7ed1\u5b9a\u7684\u8bed\u6cd5\u662f\uff0c</p> <p><code>bind=&lt;modifyer key&gt;,&lt;regular key&gt;,&lt;arg1&gt;,&lt;arg2&gt;</code></p> <p>\u5176\u4e2dmodifyer key\u5982\u679c\u6ca1\u6709\uff0c\u8981\u5199none\uff0c\u4e0d\u80fd\u7a7a\u7740\u3002\u5982\u679c\u4e0d\u77e5\u9053\u67d0\u952e\u4f4d\u53eb\u5565\uff0c\u7528 wev\u6765\u627e\u3002\u7ec8\u7aef\u8dd1wev\uff0c\u6309\u952e\u5c31\u4f1a\u663e\u793a\u540d\u5b57\u3002</p>"},{"location":"tech/mangowc/#_3","title":"\u591a\u5a92\u4f53\u63a7\u5236","text":"<p>\u9ed8\u8ba4\u914d\u7f6e\u4e0b\u6211\u7684\u64ad\u653e\u63a7\u5236\u6309\u952e\u4e0d\u80fd\u6b63\u786e\u54cd\u5e94\uff0c\u6240\u4ee5\u5148\u6253\u5f00wev\uff0c\u628a\u51e0\u4e2a\u6309\u952e\u540d\u5b57 \u627e\u51fa\u6765\uff0c\u7ed1\u5b9a\u64ad\u653e\u63a7\u5236\u3002</p> <p>wev\u7ed9\u6211\u7684\u8f93\u51fa\u957f\u8fd9\u6837\uff1a</p> <pre><code>    sym: XF86AudioLowerVolume (269025041), utf8: ''\n[14:     wl_keyboard] key: serial: 13150; time: 5262817; key: 123; state: 1 (pressed)\n    sym: XF86AudioRaiseVolume (269025043), utf8: ''\n[14:     wl_keyboard] key: serial: 13151; time: 5262897; key: 123; state: 0 (released)\n    sym: XF86AudioRaiseVolume (269025043), utf8: ''\n</code></pre> <p>sym\u540e\u9762\u662f\u6309\u952e\u7684\u540d\u5b57\u3002\u63a7\u5236\u97f3\u91cf\u7528wpctl\uff0c\u63a7\u5236\u64ad\u653e\u7528playerctl</p> <pre><code>bind=none,XF86AudioLowerVolume,spawn,wpctl set-volume @DEFAULT_AUDIO_SINK@ 1%-\nbind=none,XF86AudioRaiseVolume,spawn,wpctl set-volume @DEFAULT_AUDIO_SINK@ 1%+\nbind=none,XF86AudioNext,spawn,playerctl next\nbind=none,XF86AudioPrevious,spawn,playerctl previous\nbind=none,XF86AudioStop,spawn,playerctl stop\nbind=none,XF86AudioPlay,spawn,playerctl play\n</code></pre>"},{"location":"tech/mangowc/#_4","title":"\u5c4f\u5e55\u4eae\u5ea6","text":"<p>\u5b89\u88c5swayosd\uff0c\u5728\u81ea\u542f\u52a8\u7a0b\u5e8f\u91cc\u52a0\u4e0a<code>swayosd-server &gt;/dev/null 2&gt;&amp;1 &amp;</code></p> <pre><code>bind=none,XF86KbdBrightnessUp,spawn,swayosd-client --brightness +2\nbind=none,XF86KbdBrightnessDown,spawn,swayosd-client --brightness -2\n</code></pre>"},{"location":"tech/mangowc/#_5","title":"\u9f20\u6807\uff0c\u89e6\u6478\u677f","text":"<p>\u8c03\u6210\u81ea\u7136\u6eda\u52a8\u6a21\u5f0f\uff0c\u5e76\u628a\u52a0\u901f\u5ea6\u62c9\u6ee1</p> <pre><code>trackpad_natural_scrolling=1\naccel_speed=1\n</code></pre>"},{"location":"tech/mangowc/#_6","title":"\u5916\u63a5\u663e\u793a\u5668","text":"<p>\u63a5\u4e0a\u5916\u754c\u663e\u793a\u5668\uff0c\u76f4\u63a5\u662f\u955c\u50cf\uff0c\u9700\u8981\u624b\u52a8\u5f00\u5173\u4e00\u6b21\u624d\u80fd\u72ec\u7acb\u4f7f\u7528\u3002\u4e8e\u662f\u641e\u4e86\u4e24\u4e2a \u5feb\u6377\u952e\u5e72\u8fd9\u4e2a\u86cb\u75bc\u7684\u4e8b\u3002</p> <pre><code>bind=alt+shift,m,spawn,wlr-randr --output DP-2 --on\nbind=alt+shift,n,spawn,wlr-randr --output DP-2 --off\n</code></pre> <p>\u663e\u793a\u5668\u540d\u5b57\u7528wlr-randr\u6765\u627e</p>"},{"location":"tech/mangowc/#_7","title":"\u5e94\u7528","text":""},{"location":"tech/mangowc/#rofi","title":"rofi","text":"<p>\u900f\u660e\u80cc\u666f\u5b9e\u73b0\uff0c\u4fee\u6539\u56fe\u6807\u4e3b\u9898\uff0c\u4fee\u6539\u9ed8\u8ba4\u7ec8\u7aef\uff1a</p> <pre><code>window {\n    transparency: \"real\";\n    background-color: rgba(36,39,58,0.9);\n    icon-theme: \"candy-icons\";\n    terminal: \"kitty\";\n    ...\n }\n</code></pre> <p>\u56fe\u6807\u53ea\u8ba4<code>~/.local/share/icons</code>\uff0c\u4e0d\u8ba4<code>/usr/share/icons</code></p>"},{"location":"tech/mangowc/#_8","title":"\u72b6\u6001\u680f","text":"<p>eww,polybar\u867d\u7136\u53ef\u4ee5\u7528\uff0c\u6211\u8fd8\u662f\u6362\u6210\u4e86waybar\u3002\u5176\u4e2d\u6bd4\u8f83\u70e7\u8111\u7684\u662f\u627e\u6b63\u786e\u7684\u6e29 \u5ea6\u76d1\u6d4b\u3002\u9700\u8981\u8dd1</p> <p><code>cat /sys/class/hwmon/hwmon*/name</code></p> <p>\u6765\u627e\u54ea\u4e2a\u5305\u542bcoretemp\uff0c\u518d\u628a\u5bf9\u5e94\u7684\u8def\u5f84\u8bbe\u7f6e\u5230temperature\u6a21\u5757\u91cc\u3002</p> <p>cava\u7684\u6837\u5f0f\u6ce8\u610f\u5b57\u4f53\u4e00\u5b9a\u8981\u7b49\u5bbd\uff0c\u5426\u5219\u4f1a\u53cd\u590d\u6a2a\u8df3\u3002</p>"},{"location":"tech/mangowc/#_9","title":"\u684c\u9762\u58c1\u7eb8","text":"<p>\u53c2\u8003\u8fd9\u7bc7\u5b9e\u73b0\u52a8\u6001\u684c\u9762\uff0c\u81ea\u542f\u52a8\u91cc\u52a0\u4e0a\u4e00\u6761\uff1a</p> <p><code>swaybg -i ~/.config/wallpaper/wall.jpg &gt;/dev/null 2&gt;&amp;1 &amp;</code></p> <p>\u66f4\u65b0\uff1a\u4e0d\u5982\u7528wpaperd</p>"},{"location":"tech/mangowc/#_10","title":"\u4e2d\u6587\u8f93\u5165","text":"<p>\u6709\u4e2a\u795e\u5947\u7684bug\uff0c\u542f\u52a8fcitx5\u540e\u6240\u6709\u65b0\u7a97\u53e3\u7684\u8f93\u5165\u5b8c\u5168\u5931\u6548\u3002\u89e3\u51b3\u529e\u6cd5\uff1a\u7528\u5feb\u6377 \u952e\u6253\u5f00qmenu\u6216\u8005rofi\uff0c\u968f\u4fbf\u8f93\u4e2a\u5b57\u6bcd\u518d\u8df3\u51fa\uff0c\u5c31\u4fee\u597d\u4e86\uff1f\uff1f</p>"},{"location":"tech/mangowc/#_11","title":"\u5e94\u7528\u7a97\u53e3\u7ba1\u7406","text":"<p>mangowc\u652f\u6301\u5404\u79cd\u6392\u5217\uff0c\u6211\u7528\u7684\u6bd4\u8f83\u591a\u7684\u662f  * super+n\uff1a\u5207\u6362\u5404\u79cd\u6a21\u5f0f  * alt+\u65b9\u5411\u952e\uff1a\u5207\u6362\u7a97\u53e3  * super+\u65b9\u5411\uff1a\u5207\u6362\u684c\u9762\uff08\u5c31\u662ftag\uff09  * ctlr+super+\u65b9\u5411\uff1a\u5f53\u524d\u7a97\u53e3\u79fb\u5230\u522b\u7684\u684c\u9762\u53bb  * shift+super+\u65b9\u5411\uff1a\u79fb\u52a8\u7a97\u53e3\u7684\u76f8\u5bf9\u4f4d\u7f6e\uff0c\u4f46\u662f\u4fdd\u6301\u540c\u4e00\u79cd\u6a21\u5f0f\u3002  * ctrl+alt+\u65b9\u5411\uff1a\u6539\u53d8\u7a97\u53e3\u5927\u5c0f</p>"},{"location":"tech/mangowc/#autostart","title":"autostart","text":"<p>\u5728.config/mango/autostart.sh</p>"},{"location":"tech/mkdocs/","title":"\u7528mkdocs+github pages\u505a\u4e2a\u4eba\u7f51\u7ad9","text":"<p>\u7528mkdocs+github pages\u505a\u4e2a\u4eba\u7f51\u7ad9\uff0c\u5b66\u4e60\u66f2\u7ebf\u6bd4\u8f83\u5e73\uff0c\u9002\u5408\u4e0d\u592a\u60f3\u5b66\u65b0\u8bed\u8a00\u7684\u3002 \u6309\u7167github\u5b98\u7f51\u7684\u6559\u7a0b\u8bbe\u7f6e\u4e2a\u4eba\u7f51\u7ad9\uff0c\u65b0\u5efa\u4e00\u4e2arepo\uff0c\u7136\u540e\u6839\u636emkdocs\u7684\u6559 \u7a0b  \u6765\u53d1\u5e03\u5230github\u4e0a\u3002\u6709\u4e9b\u5c0f\u6539\u52a8\uff1a</p> <ul> <li>\u5982\u679c\u7528miniconda\u88c5\u7684python\uff0c\u6709\u53ef\u80fd\u51fa\u73b0\u7cfb\u7edf\u81ea\u5e26python \u627e\u4e0d\u5230pip\u5305\u7684\u62a5\u9519\uff0c\u5219\u9700\u8981\u8fdb\u5165<code>/usr/bin/mkdocs</code>\u4fee\u6539python\u8def\u5f84\u3002</li> <li>github\u4e0a \u7684\u9ed8\u8ba4branch\u73b0\u5728\u662fmain\uff0c\u9700\u8981\u4fee\u6b63\u547d\u4ee4\u4e3a<code>mkdocs gh-deploy --config-file &lt;path&gt;/mkdocs.yml --remote-branch main</code>\u3002\u6211\u628a\u8fd9\u4e2a\u5199\u5230deploy.sh\u91cc\uff0c\u76f4\u63a5 \u8dd1.deploy.sh\u3002</li> </ul> <p>\u5982\u679c\u94fe\u63a5\u9700\u8981\u5728\u6d4f\u89c8\u5668\u5f00\u65b0tab\uff0c\u5728\u94fe\u63a5\u7ed3\u5c3e\u52a0\u4e0a<code>{:target=\"_blank\"}</code>\u3002\u5b8c\u6574\u7248\u672c \u662f<code>[text](example.com){:target=\"_blank\"}</code></p> <p>yaml\u91cc\u52a0\u4e0a</p> <pre><code>markdown_extensions:\n  - def_list\n  - pymdownx.tasklist:\n      custom_checkbox: true\n  - attr_list\n\nplugins:\n  - search\n  - mkdocs-jupyter:\n      execute: false\n      include_requirejs: true\n      custom_mathjax_url: \"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe\"\n</code></pre>"},{"location":"tech/neovim/","title":"Neovim\u914d\u7f6e","text":"<p>\u9996\u5148\uff0c\u7528\u7684\u662flazyvim\uff0c\u505a\u6709\u9650\u7684\u6298\u817e</p> <p>\u88c5python\u76f8\u5173\u7684\u652f\u6301\u3002\u5176\u4e2d\u9700\u8981\u624b\u52a8\u7684\u662f</p> <pre><code>pip install pandas-stubs pynvim\n</code></pre> <p>\u5176\u5b83\u7684\u4e00\u6982\u770b\u4e0d\u61c2\uff0c\u8fdevimscript\u548clua\u6211\u90fd\u5206\u4e0d\u6e05\uff0c\u5c31\u8fd9\u4e48\u5f00\u59cb\u7528\u4e86</p> <p>\uff08\u8bef\uff09\u7136\u540e\u6253\u5f00vscode\u5f00\u59cb\u5e72\u6d3b</p> <p>\u61d2\u4eba\u6548\u679c\uff1a</p> <p></p>"},{"location":"tech/pythagorea/","title":"Pythagorea \u6bd5\u8fbe\u54e5\u62c9\u65af\u624b\u6e38","text":"<p>\u722c\u683c\u5b50\u7684\u51e0\u4f55\u624b\u6e38\uff0c\u5faa\u5e8f\u6e10\u8fdb\u5206\u4e3a\u51e0\u4e2a\u5927\u7c7b\uff0c\u4ece\u6700\u57fa\u7840\u7684\u7ebf\u6bb5\u5230\u590d\u6742\u4e00\u4e9b\u7684\u91cd\u5fc3\u3001 \u591a\u8fb9\u5f62\uff0c\u5207\u7ebf\u7b49\u7b49\u3002\u96be\u5ea6\u5bf9\u4e8e\u5b66\u9f84\u4eba\u6765\u8bf4\u4e00\u822c\uff0c\u5bf9\u6211\u5c31\u6b63\u597d\u3002\u672c\u8d28\u4e0a\u662f\u6709\u523b\u5ea6\u7684 \u5c3a\u5b50\u4f5c\u56fe\uff0c\u6ca1\u6709\u5706\u89c4\uff0c\u683c\u5b50\u6446\u5728\u90a3\u5c31\u662f\u4e3a\u4e86\u8ba9\u4f60\u53bb\u627e\u6b63\u597d\u662f\u6574\u6570\u7684\u8fb9\uff0c\u6216\u8005\u88ab\u7b49 \u5206\u7684\u8fb9\u3002\u503c\u5f97\u8bdf\u75c5\u7684\u662f\uff0c\u5f3a\u884c\u7528\u7eb8\u5f20\u4e0d\u591f\u5927\u6765\u5236\u9020\u96be\u5ea6\uff0c\u903c\u7740\u4f60\u627e\u5c0f\u6570\u70b9\u5750\u6807\u3002</p>"},{"location":"tech/pythagorea/#_1","title":"\u89e3\u9898\u601d\u8def","text":"<ul> <li>\u89c2\u5bdf\u54ea\u4e9b\u70b9\u6b63\u597d\u5728\u6574\u6570\u7eb5\u8f74\u6216\u8005\u6a2a\u8f74\u4e0a\u3002\u6709\u6574\u7684\u70b9\u5c31\u76f8\u5f53\u4e8e\u6709\u5c3a\u5b50\uff0c\u53ef\u4ee5\u8ba1    \u7b97\u5e76\u5229\u7528\u957f\u5ea6\u3002\u6240\u6709\u9760\u659c\u7387\u89e3\u7684\u9898\u90fd\u53ef\u4ee5\u4ee5\u6b64\u4e3a\u57fa\u7840\u3002</li> <li>\u5982\u679c\u9700\u8981\u627eC\u70b9\uff0c\u5c31\u627e\u4e24\u6761\u90fd\u7a7f\u8fc7C\u70b9\u7684\u76f4\u7ebf\uff0c\u4ea4\u70b9\u4e3aC\u3002</li> <li>\u5e73\u884c\u7ebf = \u659c\u7387\u76f8\u7b49\uff0c\u5373dy/dx\u76f8\u7b49\u3002\u53ef\u4ee5\u901a\u8fc7y\u8f74\u7684\u89c4\u5f8b\u6765\u627ex\u8f74\u7684\u89c4\u5f8b\uff0c\u53cd    \u4e4b\u4ea6\u7136\u3002</li> <li>\u52fe\u80a1\u5b9a\u7406345\u3002\uff08\u5176\u5b9e\u7528\u5f97\u5f88\u5c11\uff09</li> </ul>"},{"location":"tech/pythagorea/#_2","title":"\u5e38\u7528\u8f85\u52a9\u7ebf","text":"<ul> <li>\u7ebf\u6bb5\u5e73\u5206\u3002</li> </ul> <p>\u7528\u4ee5\u4e0a\u7c73\u5b57\u811a\u624b\u67b6\uff0c\u53ef\u4ee5\u5f97\u5230\u4e00\u6761\u628a\u6240\u6709\u8def\u8fc7\u7684\u6c34\u5e73\u7ebf\u90fd\u5bf9\u534a\u5206\u7684\u5782\u76f4\u7ebf \u4e86\u3002\u8fd9\u6837\u7684\u8f85\u52a9\u7ebf\u53ef\u4ee5\u7528\u6765\u5728\u683c\u5b50\u4e0a\u5206\u51fax.5, x.25,\u751a\u81f3x.125\u7684\u523b\u5ea6\u3002</p> <p></p> <p>\u7531k\u53d1\u6563\u51fa\u53bb\u7684\u51e0\u4e2a\u4e09\u89d2\u5f62\uff0c\u5171\u4eab\u5782\u76f4\u8fb9\u3002\u6c34\u5e73\u8fb9\u4e4b\u95f4\u7684\u6bd4\u4f8b\u662f\u8ddf\u89d2\u76f8\u5173\u7684\uff0c\u6240 \u4ee5\u5e73\u79fb\u4e4b\u540e\u8fd8\u662f\u540c\u6837\u7684\u6bd4\u4f8b\u3002\u4eceO\u5230L\u7684\u56db\u7b49\u5206\uff0c\u53ef\u4ee5\u7528\u5e73\u79fb\u6765\u4fdd\u6301\u3002\u6bd4\u5982\u5782\u76f4\u65b9 \u5411\u4e0a\u518d\u5e73\u79fb\u4e00\u4e2a\u5355\u4f4d\uff0c\u5c31\u53ef\u4ee5\u8fbe\u6210\u628a\u957f\u5ea6\u4e3a3\u7684\u6c34\u5e73\u7ebf\u6bb54\u7b49\u5206\u3002\u5f53\u7136\uff0c\u7528\u811a\u624b\u67b6 \u627e0.75\u4e5f\u53ef\u4ee5\u8fbe\u5230\u7c7b\u4f3c\u6548\u679c\u3002</p> <p></p> <p>\u53e6\u4e00\u79cd\u5e73\u5206\u65b9\u5f0f\uff0c\u5229\u7528\u76f8\u4f3c\u4e09\u89d2\u5f62\u3002\u6bd4\u5982\u627ex=1/3\u7684A\u70b9\u3002y\u8f74\u4e0a\u7684\u6bd4\u4f8b\u662f1/3\uff0c\u6240 \u4ee5x\u8f74\u4e0a\u4e5f\u662f1/3\u3002</p> <ul> <li>\u7279\u6b8a\u6446\u76d8</li> </ul> <p>\u4e0b\u9762\u8fd9\u6837\u7684\u6bd4\u4f8b\uff0c\u6b63\u597d\u6240\u6709\u70b9\u5728\u6574\u6570\u4e0a\uff0c\u5e76\u4e14\u662f\u4e00\u4e2a\u6b63\u65b9\u5f62\u3002\u6240\u4ee5AC\u6b63\u597d\u628a\u89d2 DAB\u5e73\u5206\u4e3a\u4e24\u4e2a45\u5ea6\u89d2\u3002\u6240\u4ee5\u9898\u76ee\u4e2d\u67091\uff1a2,1\uff1a3\u7684\u6446\u76d8\u5c31\u6ce8\u610f\u4e86\uff0c\u8fd9\u4fe9\u521a\u597d\u80fd\u51d1 \u51fa\u4e00\u4e2a45\u5ea6\u89d2\u3002</p> <p></p> <p>\u4e0b\u56fe\u7684345\u76f4\u89d2\u4e09\u89d2\u5f62\u7c7b\u4f3c\uff0c\u4ee5\u6700\u957f\u8fb9\u4e3a\u8170\u505a\u51fa\u4e00\u4e2a\u957f\u8fb9\u4e3a5\u7684\u7b49\u8170\u4e09\u89d2\u5f62\uff0c\u5e95\u5ea7\u521a\u597d\u659c\u73871\uff1a3\u3002</p> <p></p>"},{"location":"tech/pythagorea/#_3","title":"\u9898\u76ee","text":"<p>\u7b2c1\u30013\u30015\u7ae0\u90fd\u6ca1\u6709\u4ec0\u4e48\u96be\u70b9\uff0c\u57fa\u672c\u5c31\u662f\u6570\u6570\u6216\u8005\u627e\u6574\u6570\u70b9\u3002\u903c\u6025\u4e86\u751a\u81f3\u53ef\u4ee5\u679a \u4e3e\u3002</p>"},{"location":"tech/pythagorea/#2","title":"2.\u5e73\u884c\u7ebf","text":""},{"location":"tech/pythagorea/#216","title":"2.16","text":"<p>\u811a\u624b\u67b6\u70ed\u8eab\uff1a\u7ed9\u5b9a\u7684\u659c\u7ebf\uff08\u6df1\u9ed1\uff09\u659c\u7387\u662f3\uff1a5\u3002B\u70b9\u90a3\u4e2a\u4e2d\u5fc3\u4f4d\u7f6e\uff0c\u521a\u597d\u79bbA\u662f1.5/2.5\u3002</p>"},{"location":"tech/pythagorea/#219","title":"2.19","text":"<p>\u5e73\u884c\u7ebf\u7684\u659c\u7387\u89815\uff1a6\u3002\u7eb8\u53ea\u6709\u8fd9\u4e48\u5927\uff0c\u5f97\u627e\uff1a1.25\uff1a1.5\u3002\u4e0b\u65b9\u7684\u811a\u624b\u67b6\u627e\u6a2a\u5411 \u5de6\u79fb1.5\uff0c\u53f3\u4e0a\u7684\u811a\u624b\u67b6\u627e\u5b8c0.5\u518d\u5207\u4e00\u6b21\u627e0.25\uff0c\u51d1\u51fa\u4e2a1.25\u3002B\u70b9\u5c31\u51fa\u6765\u4e86\u3002</p>"},{"location":"tech/pythagorea/#4medians-and-mid-segments","title":"4.medians and mid-segments","text":"<p>\u4e00\u662f\u6309\u7167\u4e2d\u7ebf\u7684\u5b9a\u7406\uff0c\u4e8c\u662f\u5229\u7528\u91cd\u5fc3\u628a\u4e2d\u7ebf\u5206\u62101:2\u7684\u7279\u70b9</p>"},{"location":"tech/pythagorea/#411","title":"4.11","text":"<p>\u8fde\u63a5\u7ed9\u5b9a\u7684\u9876\u70b9\u548c\u91cd\u5fc3M\uff0c\u627e\u5230\u4e00\u4e2a\u4e2d\u7ebf\u548c\u8fb9\u7684\u4ea4\u70b9A\u3002\u7528\u811a\u624b\u67b6\u5e73\u5206\u7ed9\u5b9a\u7684\u8fb9\uff0c \u627e\u5230\u4e2d\u70b9B\u3002\u8fde\u63a5\u76f8\u5173\u7684\u70b9\u5c31\u628a\u8fb9\u51d1\u51fa\u6765\u4e86\u3002</p>"},{"location":"tech/pythagorea/#412","title":"4.12","text":"<p>\u901a\u8fc7\u70b9\u7684\u7279\u6b8a\u4f4d\u7f6e\uff08\u6570\u8f74\u4e0a\u7684\u6574\u6570\uff09\uff0c\u628a\u901a\u8fc7M\u7684\u4e2d\u7ebf\u5206\u522b\u5ef6\u957f\u4e00\u534a\u7684\u957f\u5ea6\uff0c\u5f97 \u5230\u4e2d\u70b9A,B\u3002\u53e6\u5916\u4e24\u6761\u8fb9\u5c31\u51fa\u6765\u4e86\u3002</p>"},{"location":"tech/pythagorea/#6-13","title":"6-13","text":"<p>\u5168\u90e8\u80fd\u7528\u9876\u7aef\u8bf4\u7684\u89e3\u9898\u601d\u8def\u5b8c\u6210</p>"},{"location":"tech/pythagorea/#14","title":"14.\u957f\u5ea6\u6bd4\u4f8b","text":"<p>\u4e3b\u8981\u601d\u8def\u662f\u627e\u6574\u6570\u548c\u6574\u6570\u7684\u7b49\u5206\u70b9\uff0c\u4f9d\u9760\u811a\u624b\u67b6\u6765\u5e73\u79fb\u7b49\u5206\u70b9\u5230\u6b63\u786e\u7684\u4f4d\u7f6e\u3002</p>"},{"location":"tech/pythagorea/#1407","title":"\u4f8b\u989814.07","text":"<p>\u52fe\u80a1\u5b9a\u7406\u5f97\u77e5AB\u7684\u659c\u7387\u6b63\u597d\u662f345\u4e09\u89d2\u5f62,\u627e1\u5c31\u662f\u4e94\u7b49\u5206\u3002\u5229\u7528\u6784\u9020\u76f8\u4f3c\u4e09\u89d2\u5f62 \u6765\u627e\u5230x\u8f74\u4e0a\u8ddd\u79bb\u4e3a3/5\u7684\u70b9C\u548cD,\u5f97\u5230CD\u7684\u5ef6\u957f\u7ebf\u8ddfAB\u7684\u4ea4\u70b9\uff0c\u659c\u8fb9\u957f\u5c31\u662f(3/5)/3*5=1</p>"},{"location":"tech/pythagorea/#148","title":"14.8","text":"<p>x\u8f74\u662f\u969c\u773c\u6cd5\u3002\u6839\u672c\u4e0d\u7528\u770bx\uff0c\u76f4\u63a5\u5e73\u5206y\u8f74\u5373\u53ef\u3002</p> <p></p>"},{"location":"tech/pythagorea/#149","title":"14.9","text":"<p>B\u70b9\u4e3a\u659c\u7387\u4e3a3\u7684\u659c\u7ebf\u548c\u6574\u6570x\u7684\u4ea4\u70b9\u3002\u6240\u4ee5y=n+1/3\uff0cAB\u4e4b\u95f4\u7684y\u8f74\u8ddd\u79bb\u662f2+1/3, \u4e2d\u70b9\u79bbA\u8ddd\u79bb\u4e3a1+1/6,\u7528\u811a\u624b\u67b6\u505a\u51fa\u4e2a1/6\u7684\u7ebf\u5373\u53ef\u3002</p> <p></p>"},{"location":"tech/pythagorea/#15","title":"15.\u8ddd\u79bb","text":"<p>\u627e\u76f8\u540c\u659c\u7387\uff0c\u5e76\u4f9d\u6b64\u627e\u7b49\u8ddd\u79bb</p>"},{"location":"tech/pythagorea/#155","title":"15.5","text":"<p>AB\u7684\u659c\u7387\u662f1\uff1a5,\u6240\u4ee5\u6211\u4eec\u8981\u7528\u811a\u624b\u67b6\u627e\u8fc7C\u70b9\u7684\u659c\u7387\u4e3a0.5:2.5\u7684\u5e73\u884c\u7ebf\u3002</p> <p></p>"},{"location":"tech/pythagorea/#1510","title":"15.10","text":"<p>\u627e\u76f4\u7ebf\u4e0a\u8ddfA\u3001B\u7684\u8ddd\u79bb\u603b\u548c\u6700\u77ed\u7684\u70b9\uff0c\u7b49\u4e8e\u628a\u8fd9\u6761\u7ebf\u5f53\u955c\u5b50\u627eA\u6216B\u5728\u53e6\u4e00\u8fb9\u5bf9\u79f0 \u7684\u70b9\u3002\u8fd9\u6761\u7ebf\u659c\u73871\uff1a3,\u521a\u597d\u4eceB\u51fa\u53d1\u80fd\u5236\u9020\u4e00\u6761\u659c\u73873\uff1a1\u7684\u7ebf\uff0c\u6b63\u597d\u5782\u76f4\u8fc7\u53bb\u3002</p> <p></p>"},{"location":"tech/pythagorea/#1514","title":"15.14","text":"<p>\u627eAB\u4e3a\u5e95\u8fb9\u7684\u7b49\u8170\u4e09\u89d2\u5f62\u3002AB\u659c\u73871\uff1a4\u6bd4\u8f83\u6574\u9f50\uff0c\u6240\u4ee5\u4ee5\u4e2d\u70b9C\u51fa\u53d1\uff0c\u5f80\u5de6\u8d701\u683c\uff0c \u5f80\u4e0a\u8d704\u683c\uff0c\u5230\u8fbe1.5+4=5.5\u7684\u4f4d\u7f6e\u3002</p> <p></p>"},{"location":"tech/pythagorea/#16","title":"16.\u4e8c\u7b49\u5206\u89d2","text":""},{"location":"tech/pythagorea/#163","title":"16.3","text":"<p>\u51d1\u7b49\u8170\u4e09\u89d2\u5f62</p> <p></p>"},{"location":"tech/pythagorea/#167","title":"16.7","text":"<p>\u7279\u6b8a\u6446\u76d8\u3002A\u70b9\u51fa\u53d1\u7684\u4e24\u6761\u7ebf\u659c\u7387\u5206\u522b\u4e3a1\uff1a2\u548c2\uff1a1,\u6240\u4ee5\u4e92\u76f8\u4e3a90\u5ea6\u3002\u627e\u89d2\u5e73\u5206 \u7ebf\u5c31\u662f\u627e45\u5ea6\u89d2\u3002\u627eA\u51fa\u53d1\u7684\u659c\u7387\u4e3a1\uff1a3\u7684\u7ebf\u5373\u53ef\u3002</p> <p></p>"},{"location":"tech/pythagorea/#169","title":"16.9","text":"<p>\u8fd9\u5c31\u662f\u6211\u8bf4\u7684\u96be\u5ea6\u4e0d\u591f\uff0c\u5207\u7eb8\u6765\u51d1\u3002\u672c\u8d28\u4e0a\u662f\u627e\u51fa\u8fb9\u957f\u4e3a5\u7684\u7b49\u8170\u4e09\u89d2\u5f62\uff0c\u7136\u540e \u628a\u659c\u73871\uff1a3\u7684\u5e95\u8fb9\u5e73\u5206\u3002\u4f46\u8fd9\u9898\u628a\u8be5\u7b49\u8170\u4e09\u89d2\u7684\u9876\u70b9\u653e\u7eb8\u5916\u9762\u53bb\u4e86\uff0c\u6240\u4ee5\u8981\u5728\u4e09 \u89d2\u5f62\u7684\u809a\u76ae\u90a3\u91cc\u753b\u6761\u5e73\u884c\u7ebf\u627e\u4e2d\u70b9\u3002</p> <p></p> <p></p>"},{"location":"tech/systemd/","title":"\u7528systemd\u8bbe\u7f6e\u5b9a\u65f6\u4f5c\u4e1a","text":"<p>\u9700\u8981\u4e24\u4e2a\u6587\u4ef6\uff0c\u4e00\u4e2a\u8bbe\u7f6eservice</p> <p><code>/etc/systemd/system/update_wallpaper.service</code></p> <p>\u6211\u5199\u4e86\u4e00\u4e2a\u7528\u6765\u968f\u673a\u66f4\u65b0\u684c\u9762\u7684</p> <pre><code>[Unit]\nDescription=update wallpaper\n\n[Service]\nExecStart=/home/yli/scripts/update_wallpaper_systemd.sh\nType=oneshot\nUser=yli\n</code></pre> <p>\u5199\u597d\u7684\u6587\u4ef6\u53ef\u4ee5\u5982\u4e0b\u68c0\u67e5\u8bed\u6cd5\uff1a</p> <p><code>sudo systemd-analyze verify update_wallpaper.service</code></p> <p>\u5982\u679c\u4ec0\u4e48\u90fd\u6ca1\u6709\u8f93\u51fa\uff0c\u5c31\u6ca1\u6709\u8bed\u6cd5\u9519\u8bef\u3002</p> <p>\u7b2c\u4e8c\u4e2a\u6587\u4ef6\u7528\u6765\u8ba1\u65f6\uff0c<code>/etc/systemd/system/update_wallpaper.timer</code></p> <pre><code>[Unit]\nDescription=5min\n\n[Timer]\nOnCalendar=*:0/5\nPersistent=false\nUnit=update_wallpaper.service\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>\u68c0\u67e5\u597d\u540e\u5237\u65b0</p> <p><code>systemctl daemon-reload</code></p> <p>\u5f00\u59cb\u8ba1\u65f6</p> <pre><code>sudo systemctl enable update_wallpaper.timer\nsudo systemctl start update_wallpaper.timer\n</code></pre> <p>\u68c0\u67e5\u72b6\u6001\uff1a</p> <p><code>systemctl status update_wallpaper.timer</code></p> <p>\u68c0\u67e5\u65e5\u5fd7\uff1a</p> <p><code>journalctl -u update_wallpaper.service</code></p>"},{"location":"tech/update_root_partition/","title":"Update root partition \u4fee\u6539\u6839\u5206\u533a\u6240\u5728\u786c\u76d8\u6216\u8005\u5206\u533a","text":"<p>\u53ea\u9002\u7528\u4e8eEFI\u7cfb\u7edf</p> <p>\u4e00\u822c\u6765\u8bf4root\u5206\u533a\u4e00\u65e6\u6572\u5b9a\u5c31\u4e0d\u9700\u8981\u6539\u4e86\uff0c\u9700\u8981\u8fd9\u4e2a\u86cb\u75bc\u64cd\u4f5c\u7684\u65f6\u5019\u5e76\u4e0d\u591a\uff0c\u53ea \u6709\u4e00\u5f00\u59cb\u786c\u76d8\u5206\u533a\u6ca1\u6709\u89c4\u5212\u597d\uff0c\u6216\u8005\u4e70\u4e86\u65b0\u786c\u76d8\u9700\u8981\u8fd9\u4e48\u505a\u3002\u6240\u4ee5\u641c\u4e86\u5f88\u4e45\u624d\u62fc \u63a5\u51fa\u89e3\u51b3\u65b9\u6848\uff0c\u6211\u6ca1\u6709\u627e\u5230\u4e00\u7bc7\u80fd\u76f4\u63a5\u91c7\u7528\u7684\uff0c\u6240\u4ee5\u8bb0\u5f55\u4e00\u4e0b\u3002</p> <p>\u6211\u4e4b\u6240\u4ee5\u9700\u8981\u91cd\u65b0\u6302\u8f7d\uff0c\u662f\u56e0\u4e3a\u8981\u628alinux\u5206\u533a\u6574\u4e2a\u642c\u5bb6\uff1a\u5f53\u521d\u8ddf\u7740\u6559\u7a0b\u8d70\u6ca1\u8fc7 \u8111\u5b50\uff0c\u8ba9linux\u5206\u533a\u8ddf\u5176\u5b83\u5206\u533a\u95f4\u9694\u4e86\u4e00\u4e2aEFI OS\uff0c\u5bfc\u81f4\u65e0\u6cd5\u6269\u5c55\uff08\u4e0d\u8fde\u7eed\u4e0d\u53ef \u6363\uff09\uff0c\u5927\u6982\u662f\u8fd9\u6837\u7684\uff1a</p> <pre><code>windows 450G | shared 400G | EFI OS | linux 100G\n</code></pre> <p>\u8981\u6269\u5c55arch linux\u7684\u7a7a\u95f4\uff0c\u53ea\u80fd\u5e72\u8106\u8ddf\u5171\u4eab\u5206\u533a\u6389\u4e2a\u4e2a\u513f\u3002\u6b64\u5904\u53ef\u4ee5\u7528gparted live CD\u6765\u5904\u7406\uff0c\u628alinux\u5168\u76d8\u590d\u5236\u5230shared\u7684\u4f4d\u7f6e\u3002\u65b0\u7535\u8111shared\u91cc\u6ca1\u4e1c\u897f\u6240\u4ee5 \u4e0d\u7528\u5907\u4efd\u3002\u6709\u4e00\u4e2a\u9690\u60a3\u662f\uff0cgparted\u4f1a\u628a\u65e7\u5206\u533a\u7684UUID\u590d\u5236\u6210\u65b0\u5206\u533a\u7684\uff0c\u5f97\u6539\u6389\u3002 \u6211\u9009\u62e9\u7ed9\u65e7\u5206\u533a\u65b0\u751f\u6210\u4e00\u4e2aUUID\u3002</p> <p>\u7a7a\u95f4\u6574\u7406\u597d\uff0c\u4e0b\u4e00\u6b65\u662f\u8ba9\u542f\u52a8\u7a0b\u5e8f\u8ba4\u5f97\u65b0\u7684\u5206\u533a\u3002\u8fd9\u4e2a\u9700\u8981\u7528arch linux live CD\u3002\u91cd\u65b0\u641e\u4e00\u4e0busb\uff0c\u6362ISO\u6587\u4ef6\u3002</p> <p>\u7528arch linux live\u542f\u52a8\u540e\uff0c\u5148\u505a\u4e00\u4e9b\u67e5\u770b\u5de5\u4f5c</p> <pre><code># \u5217\u51fa\u6240\u6709\u5206\u533a\uff0c\u8bb0\u4e0b\u5404\u81ea\u662f\u4ec0\u4e48\u76d8\nfdisk -l\n\n# \u5217\u51fa\u5206\u533a\u7684\u4e00\u4e9b\u53f7\uff0c\u8bb0\u5f97UUID\u548cPARTUUID\u662f\u4e0d\u540c\u7684\u4e1c\u897f\uff0c\u5206\u522b\u8981\u7528\nblkid\n</code></pre> <p>\u7136\u540e\u628a\u65b0\u5206\u533a\u548cEFI OS\u90fd\u6302\u8f7d\u3002\u65e7\u5206\u533a\u968f\u610f\uff0c\u6211\u6ca1\u6709\u7528\u4e0a\u5c31\u5ffd\u7565\u4e86</p> <pre><code># \u65b0root\nmkdir /mnt/new_root\nmount /dev/&lt;\u65b0\u5206\u533a\u8def\u5f84&gt; /mnt/new_root\n\n# \u6302\u8f7d\u542f\u52a8\u76d8\nmkdir /mnt/new_root/boot/efi\nmount /dev/&lt;EFI OS\u8def\u5f84&gt; /mnt/new_root/boot/efi\n</code></pre> <p>\u8fdb\u5165\u65b0\u7684\u5206\u533a\u5e76\u5047\u88c5\u5b83\u662f\u7cfb\u7edf\u76d8</p> <pre><code>arch-chroot /mnt/new_root\n</code></pre> <p>\u6309\u9700\u4fee\u6539fstab\uff0c\u628a<code>/</code>\u5bf9\u5e94\u7684UUID\u4fee\u6539\u6210\u65b0\u5206\u533a\u7684\u3002\u8fd9\u662f\u4e3a\u4e86\u8fdb\u64cd\u4f5c\u7cfb\u7edf\u65f6\u80fd\u6b63\u786e\u6302\u8f7d\u3002</p> <pre><code>nano /etc/fstab\n</code></pre> <p>\u4f46\u8fd9\u4e00\u6b65\u4e0d\u591f\uff0c\u56e0\u4e3aEFI OS\u8fd8\u662f\u4f1a\u4ece\u65e7\u7684\u5206\u533a\u53bb\u542f\u52a8\u7cfb\u7edf\uff0c\u4e0d\u8ba4\u8bc6\u65b0\u5206\u533a\u3002\u6b64\u5904 \u6211\u627e\u5230\u7684\u6559\u7a0b\u90fd\u5efa\u8bae\u91cd\u65b0\u88c5\u4e2abootloader\u6bd4\u5982grub, systemd-boot\u3002\u4f46\u662fEFI OS \u672c\u8eab\u5c31\u8db3\u591f\u4f5c\u4e3abootloader\uff0c\u672c\u7740\u7cbe\u7b80\u7684arch(btw)\u539f\u5219\uff0c\u6211\u51b3\u5b9a\u76f4\u63a5\u6539EFI OS \u8bbe\u7f6e\uff0c\u7ed3\u679c\u610f\u5916\u7684\u5bb9\u6613\u3002\u542f\u52a8\u754c\u9762\u5217\u51fa\u6765\u7684\u9009\u9879\u5168\u90fd\u5728 <code>/boot/efi/loader/entries/</code>\uff0c\u524d\u63d0\u662f\u8fdb\u884c\u8fc7\u4e0a\u9762\u7684\u6b65\u9aa4\u6302\u8f7d\u4e86EFI\u76d8\u3002\u5728\u91cc\u9762 \u627e\u51fa\u5305\u542barch linux\u7684\u6587\u4ef6\uff08\u540d\u5b57\u53ef\u80fd\u4e0d\u660e\u663e\uff0c\u5f97\u770b\u5185\u5bb9\uff09\uff0c\u628a\u4e4b\u524d\u6284\u4e0b\u6765\u7684 PARTUUID\u66ff\u6362\u8fc7\u6765\u5c31\u5b8c\u6210\u4e86\u3002</p> <p>\u91cd\u542f\uff0c\u8bb0\u5f97\u628aUSB\u62d4\u6389\u3002\u767b\u9646\u8fdb\u53bb\u68c0\u67e5\u4e00\u4e0b\u786c\u76d8\uff0c\u786e\u8ba4\u6210\u529f\u3002</p> <p>\u603b\u7ed3\uff0c\u56de\u5934\u770b\u6765\u5176\u5b9e\u5f88\u7b80\u5355\uff0c\u5c31\u4fe9\u5173\u952e\u6587\u4ef6\uff0c<code>/etc/fstab</code>\uff0c <code>/efi/loader/entries/*</code>\u3002\u6211\u611f\u89c9\u8fd9\u4e2a\u65b9\u6848\u5e94\u8be5\u6bd4\u8f83\u80fd\u5e7f\u6cdb\u5e94\u7528\uff0c\u6ca1\u6709\u7275\u626f\u5230 \u592a\u591aarch linux\u7279\u8272\u3002\u66f4\u591a\u662f\u9488\u5bf9EFI\u7cfb\u7edf\u542f\u52a8\u7ba1\u7406\u65b9\u6848\u3002</p>"},{"location":"tech/website_from_scratch/","title":"\u4ece\u96f6\u5f00\u59cb\u505a\u4e2a\u4eba\u7f51\u7ad9\uff0c\u4e00\u4e2a\u5931\u8d25\u7684\u9879\u76ee\u56de\u987e","text":"<p>\u5168\u6808\u4ee3\u7801\u653e\u5728github</p>"},{"location":"tech/website_from_scratch/#_2","title":"\u5927\u81f4\u6846\u67b6","text":"<p>\u5927\u81f4\u5982\u6b64\uff0c\u65f6\u9694\u592a\u4e45\u8bb0\u4e0d\u6e05\u7ec6\u8282\u3002python+fast api\u642d\u540e\u7aef\uff0creact\u642d\u524d\u7aef\uff0c\u56f4\u7ed5 AWS API Gateway\u5b9e\u73b0\u524d\u540e\u7aef\u7684\u8854\u63a5\u3002\u7531\u4e8e\u6d41\u91cf\u5c0f\uff0cAWS\u51e0\u4e4e\u53ef\u4ee5\u767d\u5ad6\u3002\u9700\u8981\u82b1\u94b1 \u7684\u662f\u7f51\u7ad9\u6258\u7ba1\u90e8\u5206\uff0c\u56e0\u4e3a\u9700\u8981\u63d0\u4f9b\u6570\u636e\u5e93\u548cFTP\u652f\u6301\u3002FTP\u7528\u6765\u5b58\u53d6\u56fe\u7247\uff0c\u6570\u636e\u5e93 \u7528\u6765\u5b58\u53d6\u7f51\u7ad9\u5404\u4e2a\u90e8\u4ef6\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u6bd4\u5982\u6bcf\u4e2a\u76ee\u5f55\u4e0b\u7684\u5217\u8868\u3001\u6807\u9898\u548c\u6587\u672c\u7684\u5bf9\u5e94\u7b49\u7b49\u3002</p>"},{"location":"tech/website_from_scratch/#_3","title":"\u793a\u4f8b","text":"<p>\u6548\u679c\u56fe\uff0c\u6e32\u67d3\u51fa\u6765\u7684\u6548\u679c\u4e0e\u666e\u901a\u7f51\u7ad9\u65e0\u5f02\uff0c\u65b9\u4fbf\u6d4f\u89c8\u3002</p> <p></p> <p>\u7f16\u8f91\u9875\u97621\u3002\u65b0\u5efa\u6587\u6863\uff0c\u57fa\u672c\u529f\u80fd\u90fd\u6709\uff0c\u6bd4\u5982\u9009\u62e9\u7c7b\u522b\uff0c\u64a4\u9500\uff0c\u4e0a\u4f20\u56fe\u7247\u3002\u9057\u61be \u7684\u662f\uff0c\u4e0a\u4f20\u56fe\u7247\u5927\u5c0f\u9650\u5236\u57283M\uff0c\u518d\u5f80\u4e0a\u52a0\u5c31\u8981\u7279\u6b8a\u5904\u7406\u3002\u672c\u8d28\u4e0a\u662f\u4f20\u8f93\u6587\u4ef6\u7684\u9650 \u5236\uff0c\u4e0d\u5bb9\u6613\u89e3\u51b3\u3002\u8fd9\u4e00\u6761\u5c31\u628a\u624b\u673a\u9ad8\u6e05\u56fe\u4e0a\u4f20\u7ed9\u5899\u58c1\u4e86\u3002</p> <p></p> <p>\u7f16\u8f91\u9875\u97622\u3002\u4e0a\u4f20\u56fe\u7247\u540e\uff0c\u6587\u4ef6\u8def\u5f84\u4f1a\u81ea\u52a8\u52a0\u5165\u5230\u7f16\u8f91\u5668\u3002\u6b64\u5904\u9700\u8981\u76f4\u63a5\u5199html\u3002</p> <p></p>"},{"location":"tech/website_from_scratch/#_4","title":"\u5f00\u53d1\u7ec6\u8282\u95ee\u9898\u793a\u4f8b","text":"<p>\u6709\u5f88\u591a\u7ec6\u8282\u95ee\u9898\uff0c\u8fb9\u505a\u8fb9\u5192\u51fa\u6765\uff0c\u4e3e\u4e00\u4e9b\u6817\u5b50\u3002</p> <ul> <li>\u4e3a\u4e86\u4e0d\u8ba9\u6587\u6863\u76f4\u63a5\u88ab\u8bbf\u95ee\uff0c\u9700\u8981\u5728\u7f51\u7ad9\u6258\u7ba1\u670d\u52a1\u5668\u7aef\u505a\u4e00\u4e9b\u8bbe\u7f6e\u3002</li> <li>\u56e0\u4e3a\u8de8\u670d\u52a1\u5668\uff08aws vs \u7f51\u7edc\u6258\u7ba1\u670d\u52a1\uff09\uff0c\u8981\u8bbe\u7f6e\u4e92\u76f8\u4fe1\u4efb\u3002</li> <li>\u7f51\u7edc\u6258\u7ba1\u670d\u52a1\u4e0d\u63d0\u4f9b\u514d\u8d39https\u9700\u8981\u7684\u6570\u5b57\u8bc1\u4e66\u3002\u8fd9\u4e00\u6761\u5176\u5b9e\u5982\u679c\u4e00\u5f00\u59cb\u77e5\u9053\uff0c    \u5c31\u76f4\u63a5\u529d\u9000\u4e86\uff0c\u5982\u679c\u4e0d\u662f\u60f3\u5199\u4e2a\u5168\u6808\u7ec3\u7ec3\u624b\u7684\u8bdd\u3002</li> <li>\u6211\u81f3\u4eca\u7528\u4e0d\u597dReact\uff0c\u6211\u4e0d\u77e5\u9053\u524d\u7aef\u7a0b\u5e8f\u733f\u600e\u4e48\u6d3b\u4e0b\u6765\u7684\u3002</li> </ul>"},{"location":"tech/website_from_scratch/#_5","title":"\u9879\u76ee\u5931\u8d25\u5206\u6790","text":"<p>\u5373\u4f7f\u4e0d\u8003\u8651https\u7684\u95ee\u9898\uff0c\u8fd9\u4e2a\u7f51\u7ad9\u65b9\u6848\u4e5f\u53ef\u4ee5\u8bf4\u662f\u5931\u8d25\u4e86\u3002\u65e0\u4ed6\uff0c\u96be\u7528\u3002\u867d\u7136 \u52c9\u5f3a\u8bf4\u57fa\u672c\u529f\u80fd\u90fd\u6709\uff0c\u4f46\u5c31\u662f\u5168\u65b9\u4f4d\u7684\u7528\u7740\u96be\u53d7\u3002\u5b83\u7684\u5b9a\u4f4d\u5f88\u5c34\u5c2c\uff0c\u5982\u679c\u8bf4\u662f\u5b9a \u4f4d\u975e\u6280\u672f\u7528\u6237\uff0c\u90a3\u4e48\u56fe\u7247\u7684\u975e\u6240\u89c1\u5373\u6240\u5f97\u5c31\u662f\u5927\u95ee\u9898\uff0c\u4e0a\u4f20\u4e4b\u540e\u53ea\u80fd\u770b\u5230\u4e2a\u8def\u5f84\uff0c \u7f16\u8f91\u5668\u91cc\u65e0\u6cd5\u9884\u89c8\u3002\u5982\u679c\u8bf4\u662f\u5b9a\u4f4d\u6280\u672f\u4eba\u5458\uff0c\u90a3\u5b83\u7684\u5f88\u591a\u4e1c\u897f\u5c31\u662f\u5357\u8f95\u5317\u8f99\u3002\u867d \u7136\u672c\u5730\u53ef\u4ee5\u5f00\u53d1\u548c\u6d4b\u8bd5\uff0c\u4f46\u7531\u4e8e\u6574\u4e2a\u7f51\u7ad9\u7684\u7ed3\u6784\u5b58\u50a8\u5728\u8fdc\u7a0b\u6570\u636e\u5e93\u91cc\uff0c\u672c\u5730\u65e0\u6cd5 \u76f4\u63a5\u4fee\u6539\u5c31\u975e\u5e38\u4e0d\u65b9\u4fbf\uff0c\u6ca1\u6cd5\u4e00\u4e2a\u7f16\u8f91\u5668\u5f00N\u4e2a\u6587\u6863\u8fd9\u6837\u3002</p> <p>\u8981\u8bf4\u95ea\u5149\u70b9\uff0c\u5c31\u662f\u80fd\u5199\u5230\u7b80\u5386\u91cc\uff0c\u9762\u8bd5\u7684\u65f6\u5019\u5439\u4e00\u4e0b\u5168\u6808\u5199\u4e86\u4e2a\u6d3b\u7f51\u7ad9\u3002</p>"},{"location":"tech/whining/","title":"\u5410\u69fdlinux","text":"<ul> <li>linux\u7528\u5f97\u8d8a\u591a\uff0c\u8d8a\u89c9\u5f97win,mac\u662f\u5f88\u597d\u7684\u7cfb\u7edf\u3002</li> <li>\u4e3a\u4ec0\u4e48\u4e00\u8fb9\u5410\u69fd\u4e00\u8fb9\u7528\uff1f\u56e0\u4e3a\u719f\u6089\u4e86\uff0c\u987a\u624b\u4e86\uff0c\u4ee5\u53ca\u6363\u9f13\u914d\u7f6e\u7684\u9178\u723d\u3002</li> <li>2025\u5e74\u4e86\uff0c\u4e3a\u4ec0\u4e48\u88c5\u4e2a\u4e2d\u6587\u8f93\u5165\u6cd5\u8fd8\u80fd\u662f\u4ef6\u503c\u5f97\u5e86\u8d3a\u7684\u6210\u5c31</li> <li>\u4e3a\u4ec0\u4e48wayland\u7cfb\u5217\u7684\u6211\u5c31\u88c5\u4e0d\u4e0a\uff0c\u5168\u5728\u767b\u9646\u8fdb\u53bb\u7684\u65f6\u5019\u5c31\u6b7b\u673a</li> <li>openbox\u6ca1\u4eba\u7ef4\u62a4\u4e86\uff0c\u5230\u54ea\u91cc\u627e\u80fd\u7528\u7684\u8f7b\u91cf\u7ea7\u5f00\u76d2\u5373\u7528\u684c\u9762\u7ba1\u7406\u7cfb\u7edf\uff1ftiling    \u7cfb\u5217\u7684\u6211\u4e00\u4e2a\u90fd\u88c5\u4e0d\u597d\u3002</li> <li>\u4e3a\u4ec0\u4e48\u5f53\u521d\u6211\u9009\u4e86emacs\u800c\u4e0d\u662fvi\uff1f</li> <li>\u4e3a\u4ec0\u4e48systemd\u8fd9\u6837\u8fdd\u53cdunix\u539f\u5219\u7684\u4e1c\u897f\u80fd\u88ab\u63a8\u5e7f</li> <li>\u6211\u6709\u751f\u4e4b\u5e74\uff0c\u4f1a\u4e0d\u4f1alinux\u5c31\u6ca1\u4ec0\u4e48\u4eba\u7528\uff0c\u4e00\u573a\u7a7a\u4e86\uff1f</li> <li>\u6298\u817e\u914d\u7f6e\u6709\u4ec0\u4e48\u610f\u4e49\uff0c\u51e0\u5e74\u524d\u914d\u597d\u4e86openbox+conky\uff0c\u73b0\u5728\u90fd\u51c9\u900f\u4e86</li> <li>\u574f\u6d88\u606f\uff1aSDDM\u88ab\u641e\u574f\u4e86\uff0c\u4fee\u4e0d\u597d\u3002\u597d\u6d88\u606f\uff1a\u542c\u8bf4kde plasma\u90a3\u8fb9\u60f3\u628aSDDM\u7ed9    \u8fed\u4ee3\u4e86\u3002</li> <li>\u53cc\u663e\u8fd8\u80fd\u51fa\u9519\uff1f\u8fde\u4e86\u5916\u63a5\u663e\u793a\u5668\u540e\uff0c\u6709\u4e00\u5b9a\u51e0\u7387\u5bfc\u81f4\u539f\u672c\u7684\u663e\u793a\u5668\u80cc\u666f\u6d88\u5931\uff0c    \u7cfb\u7edf\u6258\u76d8\u6d88\u5931\u3002\u89e3\u51b3\u65b9\u6cd5\u662f<code>kquitapp6 plasmashell</code>\uff0c\u4f1a\u5173\u6389\u5e76\u81ea\u52a8\u91cd\u542f    plasmashell\uff0c\u5176\u5b83\u5e94\u7528\u4e0d\u53d7\u5f71\u54cd\u3002</li> </ul>"},{"location":"tech/ml/basic_neural_net/","title":"Implement a basic neural network from scratch","text":"<p>A lot of the basics of this note is built upon the perceptron algorithm. Read that note first. Apology upfront for all the cumbersum notations below. Those will be useful in the next notebook on RNN, so I'll keep it consistent here.</p> <p>This is a terribly drawn but still valid fully connected neural net with 3 input features (x1, x2, x3), 1 hidden layer (node1, node2), and 1-dimensional output (y). Not shown in this picture is a bias term, to prevent trivial prediction at <code>x=[0, 0, 0]</code> regardless of weights. We could introduced the bias term by a constant node with value=1, but I decided to separate it out for easier reading later.</p> <p>We say it's fully connected because each node is connected with all nodes in the next layer. This can be seen as a generic representation. A sparse network is simply one with most weights=0.</p> <p>We notate the weights as: <code>w[i,j]</code> is the weight between node_i and input x_j</p> <p></p> <p>Before getting into details, we claim that neural network can help us with cognitive tasks by telling us about whether an object is. I wanted to build my own with some funny objects, but decided that's too much work. So we'll use the number dataset that's readily available.</p> In\u00a0[167]: Copied! <pre>import math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n</pre> import math import matplotlib.pyplot as plt import numpy as np import pandas as pd In\u00a0[20]: Copied! <pre>from keras.datasets import mnist\n\n(x_train_raw, y_train),(x_test_raw, y_test) = mnist.load_data()\nx_train_raw.shape, y_train.shape, x_test_raw.shape, y_test.shape\n</pre> from keras.datasets import mnist  (x_train_raw, y_train),(x_test_raw, y_test) = mnist.load_data() x_train_raw.shape, y_train.shape, x_test_raw.shape, y_test.shape <pre>2025-09-12 23:55:26.990868: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-09-12 23:55:27.063891: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757735727.085664  886704 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757735727.092652  886704 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-09-12 23:55:27.172324: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n</pre> Out[20]: <pre>((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))</pre> <p>I'll compress the images to simplify calculations later</p> In\u00a0[523]: Copied! <pre>def compress(img, fold=2):\n    n = img.shape[0]\n    com = [img[i] for i in range(n) if i % fold == 0]\n    com = [[c[i] for i in range(len(c)) if i % fold == 0] for c in com]\n    com = com[1:-2]\n    com = [c[1:-2] for c in com]\n    return np.array(com)\n\nx_train = np.array([compress(x, 2) for x in x_train_raw])\nx_test = np.array([compress(x, 2) for x in x_test_raw])\n</pre> def compress(img, fold=2):     n = img.shape[0]     com = [img[i] for i in range(n) if i % fold == 0]     com = [[c[i] for i in range(len(c)) if i % fold == 0] for c in com]     com = com[1:-2]     com = [c[1:-2] for c in com]     return np.array(com)  x_train = np.array([compress(x, 2) for x in x_train_raw]) x_test = np.array([compress(x, 2) for x in x_test_raw]) In\u00a0[24]: Copied! <pre>print(\"before compression\")\nimgs = x_train_raw[[i for i in range(16)]]\nfig, axes = plt.subplots(4,4)\nfor i in range(16):\n    axes[int(i/4)][i%4].imshow(imgs[i])\n</pre> print(\"before compression\") imgs = x_train_raw[[i for i in range(16)]] fig, axes = plt.subplots(4,4) for i in range(16):     axes[int(i/4)][i%4].imshow(imgs[i]) <pre>before compression\n</pre> In\u00a0[31]: Copied! <pre>print(\"after compression\")\nimgs = x_train[[i for i in range(16)]]\nfig, axes = plt.subplots(4,4)\nfor i in range(16):\n    axes[int(i/4)][i%4].imshow(imgs[i])\nprint(imgs.shape)\n</pre> print(\"after compression\") imgs = x_train[[i for i in range(16)]] fig, axes = plt.subplots(4,4) for i in range(16):     axes[int(i/4)][i%4].imshow(imgs[i]) print(imgs.shape) <pre>after compression\n(16, 11, 11)\n</pre> <p>OK, I can live with 2 fold compression</p> <p>In the matrix form, the formulation is easily extendable to more complex networks. The same principal/math applies. To do the number classification, we obviously need to go beyond just 3 input features. In fact, the image is a 11x11 grid (after compression), meaning 121 input features. So instead of $X^i$ representing $[x_1, x_2, x_3]$, we have it represent $[x_1, x_2, ..., x_{121}]$. The same goes with w.</p> In\u00a0[399]: Copied! <pre>class NaiveNeuralNet:\n    def __init__(self):\n        self.w = np.random.randn(20,121)\n        self.v = np.random.randn(10,20)\n        self.b = np.random.randn(20)\n        self.d = np.random.randn(10)\n\n    @staticmethod\n    def softmax(x):\n        x = np.array(x, dtype=np.float128)\n        # to avoid overflow due to super large x, we subtract the max\n        xx = x - np.max(x)\n        exp = np.exp(xx)\n        return exp / exp.sum(axis=1, keepdims=True)\n\n    @staticmethod\n    def sigmoid(x):\n        x = np.array(x, dtype=np.float128)\n        return 1 / (1 + np.exp(-x))\n\n    def predict_scores(self, x):\n        # feed forward. Here x is in the same format as the training samples\n        num_samples = x.shape[0]\n        # flatten the samples into 121 features.\n        x_flat = np.array([a.flatten() for a in x])\n        # Hidden layer\n        # add a bias term for each sample.\n        bias_1 = np.array([self.b for i in range(num_samples)])\n        sum_1 = x_flat @ self.w.T + bias_1\n        activated_1 = self.sigmoid(sum_1)\n        # Output layer\n        bias_2 = np.array([self.d for i in range(num_samples)])\n        sum_2 = activated_1 @ self.v.T + bias_2\n        activated_2 = self.softmax(sum_2)\n        return activated_2\n\n    def predict(self, x):\n        pred = self.predict_scores(x)\n        indices = [np.argmax(p) for p in pred]\n        return np.array(indices)\n</pre> class NaiveNeuralNet:     def __init__(self):         self.w = np.random.randn(20,121)         self.v = np.random.randn(10,20)         self.b = np.random.randn(20)         self.d = np.random.randn(10)      @staticmethod     def softmax(x):         x = np.array(x, dtype=np.float128)         # to avoid overflow due to super large x, we subtract the max         xx = x - np.max(x)         exp = np.exp(xx)         return exp / exp.sum(axis=1, keepdims=True)      @staticmethod     def sigmoid(x):         x = np.array(x, dtype=np.float128)         return 1 / (1 + np.exp(-x))      def predict_scores(self, x):         # feed forward. Here x is in the same format as the training samples         num_samples = x.shape[0]         # flatten the samples into 121 features.         x_flat = np.array([a.flatten() for a in x])         # Hidden layer         # add a bias term for each sample.         bias_1 = np.array([self.b for i in range(num_samples)])         sum_1 = x_flat @ self.w.T + bias_1         activated_1 = self.sigmoid(sum_1)         # Output layer         bias_2 = np.array([self.d for i in range(num_samples)])         sum_2 = activated_1 @ self.v.T + bias_2         activated_2 = self.softmax(sum_2)         return activated_2      def predict(self, x):         pred = self.predict_scores(x)         indices = [np.argmax(p) for p in pred]         return np.array(indices) In\u00a0[400]: Copied! <pre>net = NaiveNeuralNet()\nnet.predict_scores(imgs[:3])\n</pre> net = NaiveNeuralNet() net.predict_scores(imgs[:3]) Out[400]: <pre>array([[9.35205551e-01, 1.67069978e-03, 3.01476927e-03, 1.65483827e-02,\n        5.51737888e-04, 2.71009969e-05, 3.91528375e-02, 7.73713606e-05,\n        2.21130439e-04, 3.53041919e-03],\n       [2.31454335e-01, 2.78708098e-04, 1.07199313e-03, 6.77965641e-01,\n        2.51910835e-02, 3.69573108e-05, 4.92609756e-02, 4.87169969e-03,\n        2.35934251e-03, 7.50926412e-03],\n       [3.77629835e-01, 2.57955233e-04, 2.71079986e-03, 5.18559401e-01,\n        1.70052707e-02, 3.04029674e-05, 5.29631525e-03, 5.85975271e-02,\n        1.87688003e-02, 1.14369200e-03]], dtype=float128)</pre> In\u00a0[401]: Copied! <pre>net = NaiveNeuralNet()\nnet.predict(imgs)\n</pre> net = NaiveNeuralNet() net.predict(imgs) Out[401]: <pre>array([8, 8, 8, 5, 8, 5, 8, 8, 8, 8, 8, 8, 1, 8, 8, 8])</pre> <p>Of course, the predictions are all over the places! A prediction from randomly generated parameters is expected to be very random. Therefore we need to adjust the parameters via model training.</p> In\u00a0[9]: Copied! <pre>def entropy(y_true, y_pred):\n    s = 0\n    for a, b in zip(y_true, y_pred):\n        s -= a * math.log(b)\n    return s\n\ny_true = [0, 1, 0, 0]\ny_pred = [0.001, 0.0001, 0.0002, 0.001]  # predicts nothing\nprint(entropy(y_true, y_pred))\n\ny_true = [0, 1, 0, 0]\ny_pred = [0.001, 0.999, 0.0002, 0.001]  # fairly good\nprint(entropy(y_true, y_pred))\n\ny_true = [0, 1, 0, 0]\ny_pred = [0.001, 0.501, 0.5002, 0.001]  # predicts maybe ok\nprint(entropy(y_true, y_pred))\n\ny_true = [0, 1, 0, 0]\ny_pred = [0.001, 0.801, 0.5002, 0.001]  # predicts fairly ok\nprint(entropy(y_true, y_pred))\n\ny_true = [0, 1, 0, 0]\ny_pred = [0.001, 0.201, 0.9002, 0.001]  # predicts fairly bad\nprint(entropy(y_true, y_pred))\n</pre> def entropy(y_true, y_pred):     s = 0     for a, b in zip(y_true, y_pred):         s -= a * math.log(b)     return s  y_true = [0, 1, 0, 0] y_pred = [0.001, 0.0001, 0.0002, 0.001]  # predicts nothing print(entropy(y_true, y_pred))  y_true = [0, 1, 0, 0] y_pred = [0.001, 0.999, 0.0002, 0.001]  # fairly good print(entropy(y_true, y_pred))  y_true = [0, 1, 0, 0] y_pred = [0.001, 0.501, 0.5002, 0.001]  # predicts maybe ok print(entropy(y_true, y_pred))  y_true = [0, 1, 0, 0] y_pred = [0.001, 0.801, 0.5002, 0.001]  # predicts fairly ok print(entropy(y_true, y_pred))  y_true = [0, 1, 0, 0] y_pred = [0.001, 0.201, 0.9002, 0.001]  # predicts fairly bad print(entropy(y_true, y_pred)) <pre>9.210340371976182\n0.0010005003335835344\n0.6911491778972723\n0.22189433191377778\n1.6044503709230613\n</pre> <p>close to 0 for correct predictions and large for bad predictions. It goes to infinity as the predicted score goes to 0, due to the ln() term, as $\\lim\\limits_{x\\to 0}\\ln(x)=-\\infty$</p> <p>The overall cost function is the average of all losses</p> In\u00a0[15]: Copied! <pre>def cost(y_pred, y_true):\n    e = 1e-10  # to avoid actually having ln(0)\n    y_pred = np.clip(y_pred, e, 1.0)\n    loss = -np.sum(y_true * np.log(y_pred))  # dot product then sum\n    return loss / y_pred.shape[0]\n\ny_pred = [[0, 0.1, 0], [0.5, 0.2, 0.3]]\ny_true = [[1, 0, 0], [1, 0, 0]]\nprint(\"bad prediction:\", cost(np.array(y_pred), np.array(y_true)))\n\ny_pred = [[0.9, 0.1, 0], [0.7, 0.2, 0.3]]\ny_true = [[1, 0, 0], [1, 0, 0]]\nprint(\"good one:\", cost(np.array(y_pred), np.array(y_true)))\n</pre> def cost(y_pred, y_true):     e = 1e-10  # to avoid actually having ln(0)     y_pred = np.clip(y_pred, e, 1.0)     loss = -np.sum(y_true * np.log(y_pred))  # dot product then sum     return loss / y_pred.shape[0]  y_pred = [[0, 0.1, 0], [0.5, 0.2, 0.3]] y_true = [[1, 0, 0], [1, 0, 0]] print(\"bad prediction:\", cost(np.array(y_pred), np.array(y_true)))  y_pred = [[0.9, 0.1, 0], [0.7, 0.2, 0.3]] y_true = [[1, 0, 0], [1, 0, 0]] print(\"good one:\", cost(np.array(y_pred), np.array(y_true))) <pre>bad prediction: 11.8594990552502\ngood one: 0.23101772979827936\n</pre> <p>Based on the chain rule, we can breakdown the partial derivatives for w and v into LARGE chains like so:</p> <p>$$ \\large \\frac{\\partial L}{\\partial v_{jk}} = \\frac{\\partial L}{\\partial s_{ok}}\\frac{\\partial s_{ok}}{\\partial v_{jk}} $$</p> <p>$$ \\large \\frac{\\partial L}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial s_{ok}} \\frac{\\partial s_{ok}}{\\partial a_{hj}} \\frac{\\partial a_{hj}}{\\partial s_{hj}} \\frac{\\partial s_{hj}}{\\partial w_{ij}} $$</p> <p>Where $s_o, s_h$ are the weighted sum at the outer and hidden layers, respectively. $a_h$ is the activated value at the hidden layer.</p> <p>Going layer by layer back, we start with derivative of cost function</p> <p>For simplicity, let $\\sigma(s_i)$ be the softmax at class i (a.k.a output node i). Recall that $s_i$ is the weighted sum plus bias, over all edges from the hidden layer to the output layer node i.</p> <p>Therefore, going from the lost function to the hidden-&gt;output layer weights, we have</p> <p>$$\\large \\frac{\\partial L}{\\partial v_{ik}} = \\frac{\\partial L}{\\partial s_i} \\frac{\\partial s_i}{\\partial v_{ik}}= (\\hat{y_i}-y_i) x_k $$</p> <p>For the bias term, we replace $x_k$ with 1.</p> <p>We already have $\\large \\frac{\\partial L}{\\partial s_{ok}} = \\hat{y_k}-y_k$</p> <p>Also, $\\large \\frac{\\partial s_{ok}}{\\partial a_{hj}} = \\frac{\\partial (a_h V_k + d)}{\\partial a_{hj}} = v_{jk}$ because only $v_{jk}$ will be multiplied with $a_{hj}$ out of all these terms.</p> <p>Next we solve $\\large \\frac{\\partial a_{hj}}{\\partial s_{hj}}$, which is the derivative of sigmoid, as $a_{hj} = \\sigma(s_{hj})$</p> <p>Let $u = s_{hj}$, we have $$\\large \\begin{align} \\frac{\\partial \\sigma(u)}{\\partial u} &amp;= (-\\frac{1}{1+e^{-u}})' \\\\ &amp;= -\\frac{1}{(1+e^{-u})^2}(e^{-u})' \\\\ &amp;= -\\frac{1}{(1+e^{-u})^2}(-e^{-u}) \\\\ &amp;= \\frac{e^{-u}}{(1+e_{-u})^2} \\\\ &amp;= \\frac{1}{1+e^{-u}}\\frac{e^{-u}}{1+e^{-u}} \\\\ &amp;= \\frac{1}{1+e^{-u}}(1-\\frac{1}{1+e^{-u}}) \\\\ &amp;= \\sigma(u)(1-\\sigma(u)) \\end{align}  $$</p> <p>The last item in the chain, $\\large \\frac{\\partial s_{hj}}{\\partial w_{ij}}=\\frac{\\partial (XW_j+b)}{\\partial w_{ij}} = x_{i}$ since only $x_i$ will be multiplied with $w_{ij}$</p> <p>Combining all together, we get</p> <p>$$\\large \\begin{align} \\frac{\\partial L}{\\partial w_{ij}} &amp;= (\\hat{y_k} -y_k) \\cdot v_{jk} \\cdot (\\sigma_{sigmoid}(s_{hj})(1-\\sigma_{sigmoid}(s_{hj})) x_i \\\\ &amp;= (\\hat{y_k} -y_k) \\cdot v_{jk} \\cdot a_{hj} \\cdot (1-a_{hj}) \\cdot x_i \\end{align} $$</p> <p>Again, for the bias term, replace $x_i$ with 1.</p> <p>With all the tools above, let's expand upon the native neural net to make one that can backpropagate</p> In\u00a0[545]: Copied! <pre>class SmartNeuralNet(NaiveNeuralNet):\n    def __init__(self, learning_rate=0.1, n_nodes=20):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.w = np.random.randn(n_nodes,121)\n        self.v = np.random.randn(10,n_nodes)\n        self.b = np.random.randn(n_nodes)\n\n    @staticmethod\n    def cost(y_pred, y_true):\n        e = 1e-10  # to avoid actually having ln(0)\n        y_pred = np.clip(y_pred, e, 1.0)\n        loss = -np.sum(y_true * np.log(y_pred))  # dot product then sum\n        return loss / y_pred.shape[0]\n\n    def backpropagate(self, x, y):\n        num_samples = x.shape[0]\n        bias_hidden = np.array([self.b for i in range(num_samples)])\n        bias_out = np.array([self.d for i in range(num_samples)])\n        \n        # output to hidden layer\n        s_hidden = x @ self.w.T + bias_hidden\n        # print(z_hidden.shape, x.shape, self.w.T.shape, bias_hidden.shape)\n        a_hidden = self.sigmoid(s_hidden)\n        # print(a_hidden.shape)\n\n        s_out = a_hidden @ self.v.T + bias_out\n        a_out = self.softmax(s_out)\n        # print(y.shape, a_out.shape, a_hidden.shape)\n        delta_v = (-y + a_out).T @ a_hidden\n        delta_bias_out = (-y + a_out)\n        assert delta_v.shape == self.v.shape\n\n        # hidden to input layer\n        # (-y - sigmoid(z_out)) * v\n        delta_w_step_1 = (-y + a_out) @ self.v\n        # print(delta_w_step_1.shape, self.v.shape)\n        # * a_hidden * (1-a_hidden) @ x\n        delta_w_step_2 = delta_w_step_1 * a_hidden * (1 - a_hidden)\n        delta_w = delta_w_step_2.T @ x\n        delta_bias_hidden = delta_w_step_2\n\n        # update weights w, v, biases. Do this at the end since all calculations\n        # above involve w and v and need to be done first. Subtract delta because\n        # we want to move aganist the direction where delta is going. We want to\n        # reduce the gap\n        #print(delta_v.sum())\n        self.w -= self.learning_rate * delta_w\n        self.v -= self.learning_rate * delta_v\n        # for bias we need to do a sum over all the samples \n        self.b -= self.learning_rate * delta_bias_hidden.sum(axis=0)\n        self.d -= self.learning_rate * delta_bias_out.sum(axis=0)\n\n    @staticmethod\n    def label_to_classes(y):\n        # turn a single digit label to multiclass labels\n        labels = np.zeros((len(y), 10), dtype=int)\n        for i, yy in enumerate(y):\n            labels[i, yy] = 1\n        return labels\n\n    def fit(self, x, y, epochs=200, verbose=True):\n        x_flat = np.array([a.flatten() for a in x])\n        for i in range(epochs):\n            # print out current cost\n            if verbose and ((i % 200 == 0) or (i &lt; 20 and i % 5 == 0) or (i &lt; 100 and i % 20 == 0)):\n                y_curr = self.predict_scores(x)\n                cost = self.cost(y_curr, y)\n                y_pred = self.predict(tx)\n                indices = [np.argmax(p) for p in y]\n                correct = (y_pred == indices).sum()\n                print(f\"cost at epoch {i}: {cost}. Predicted {correct} samples correctly, or {round(100.0*correct/x.shape[0],2)} percent\")\n            # backprop to update weights\n            self.backpropagate(x_flat, y)\n</pre> class SmartNeuralNet(NaiveNeuralNet):     def __init__(self, learning_rate=0.1, n_nodes=20):         super().__init__()         self.learning_rate = learning_rate         self.w = np.random.randn(n_nodes,121)         self.v = np.random.randn(10,n_nodes)         self.b = np.random.randn(n_nodes)      @staticmethod     def cost(y_pred, y_true):         e = 1e-10  # to avoid actually having ln(0)         y_pred = np.clip(y_pred, e, 1.0)         loss = -np.sum(y_true * np.log(y_pred))  # dot product then sum         return loss / y_pred.shape[0]      def backpropagate(self, x, y):         num_samples = x.shape[0]         bias_hidden = np.array([self.b for i in range(num_samples)])         bias_out = np.array([self.d for i in range(num_samples)])                  # output to hidden layer         s_hidden = x @ self.w.T + bias_hidden         # print(z_hidden.shape, x.shape, self.w.T.shape, bias_hidden.shape)         a_hidden = self.sigmoid(s_hidden)         # print(a_hidden.shape)          s_out = a_hidden @ self.v.T + bias_out         a_out = self.softmax(s_out)         # print(y.shape, a_out.shape, a_hidden.shape)         delta_v = (-y + a_out).T @ a_hidden         delta_bias_out = (-y + a_out)         assert delta_v.shape == self.v.shape          # hidden to input layer         # (-y - sigmoid(z_out)) * v         delta_w_step_1 = (-y + a_out) @ self.v         # print(delta_w_step_1.shape, self.v.shape)         # * a_hidden * (1-a_hidden) @ x         delta_w_step_2 = delta_w_step_1 * a_hidden * (1 - a_hidden)         delta_w = delta_w_step_2.T @ x         delta_bias_hidden = delta_w_step_2          # update weights w, v, biases. Do this at the end since all calculations         # above involve w and v and need to be done first. Subtract delta because         # we want to move aganist the direction where delta is going. We want to         # reduce the gap         #print(delta_v.sum())         self.w -= self.learning_rate * delta_w         self.v -= self.learning_rate * delta_v         # for bias we need to do a sum over all the samples          self.b -= self.learning_rate * delta_bias_hidden.sum(axis=0)         self.d -= self.learning_rate * delta_bias_out.sum(axis=0)      @staticmethod     def label_to_classes(y):         # turn a single digit label to multiclass labels         labels = np.zeros((len(y), 10), dtype=int)         for i, yy in enumerate(y):             labels[i, yy] = 1         return labels      def fit(self, x, y, epochs=200, verbose=True):         x_flat = np.array([a.flatten() for a in x])         for i in range(epochs):             # print out current cost             if verbose and ((i % 200 == 0) or (i &lt; 20 and i % 5 == 0) or (i &lt; 100 and i % 20 == 0)):                 y_curr = self.predict_scores(x)                 cost = self.cost(y_curr, y)                 y_pred = self.predict(tx)                 indices = [np.argmax(p) for p in y]                 correct = (y_pred == indices).sum()                 print(f\"cost at epoch {i}: {cost}. Predicted {correct} samples correctly, or {round(100.0*correct/x.shape[0],2)} percent\")             # backprop to update weights             self.backpropagate(x_flat, y) In\u00a0[529]: Copied! <pre>tx = x_train[:1000]\nty_raw = y_train[:1000]\ntx = [a.flatten() for a in tx]\ntx = np.array(tx)\nty = nn.label_to_classes(ty_raw)\n</pre> tx = x_train[:1000] ty_raw = y_train[:1000] tx = [a.flatten() for a in tx] tx = np.array(tx) ty = nn.label_to_classes(ty_raw) In\u00a0[520]: Copied! <pre>nn = SmartNeuralNet(learning_rate=0.00001, n_nodes=100)\nnn.fit(tx, ty, epochs=500, verbose=True)\n</pre> nn = SmartNeuralNet(learning_rate=0.00001, n_nodes=100) nn.fit(tx, ty, epochs=500, verbose=True) <pre>cost at epoch 0: 13.198930496483507. Predicted 92 samples correctly\ncost at epoch 5: 12.167644632532499. Predicted 101 samples correctly\ncost at epoch 10: 11.417701696942665. Predicted 101 samples correctly\ncost at epoch 15: 10.83024405785545. Predicted 103 samples correctly\ncost at epoch 20: 10.320146829744136. Predicted 107 samples correctly\ncost at epoch 40: 8.940267426342567. Predicted 130 samples correctly\ncost at epoch 60: 8.07253134511341. Predicted 134 samples correctly\ncost at epoch 80: 7.489950654295265. Predicted 141 samples correctly\ncost at epoch 200: 5.982117800522912. Predicted 157 samples correctly\ncost at epoch 400: 5.081140574251207. Predicted 179 samples correctly\n</pre> In\u00a0[532]: Copied! <pre>txx = x_test[:1000]\ntyy_raw = y_test[:1000]\ntxx = [a.flatten() for a in txx]\ntxx = np.array(txx)\ntyy = nn.label_to_classes(tyy_raw)\n\ny_predd = nn.predict(txx)\ncor = (y_predd == tyy_raw).sum()\nprint(f\"Predicted {cor} samples from the test set, or {round(100.0*cor/txx.shape[0],2)} percent\")\n</pre> txx = x_test[:1000] tyy_raw = y_test[:1000] txx = [a.flatten() for a in txx] txx = np.array(txx) tyy = nn.label_to_classes(tyy_raw)  y_predd = nn.predict(txx) cor = (y_predd == tyy_raw).sum() print(f\"Predicted {cor} samples from the test set, or {round(100.0*cor/txx.shape[0],2)} percent\") <pre>Predicted 152 samples from the test set, or 15.2 percent\n</pre> <p>Try the above for a few more times. Maybe for different parameters.</p> In\u00a0[540]: Copied! <pre>nn = SmartNeuralNet(learning_rate=0.0001, n_nodes=100)\nnn.fit(tx, ty, epochs=500, verbose=True)\n\ny_predd = nn.predict(txx)\ncor = (y_predd == tyy_raw).sum()\nprint(f\"Predicted {cor} samples from the test set, or {round(100.0*cor/txx.shape[0],2)} percent\")\n</pre> nn = SmartNeuralNet(learning_rate=0.0001, n_nodes=100) nn.fit(tx, ty, epochs=500, verbose=True)  y_predd = nn.predict(txx) cor = (y_predd == tyy_raw).sum() print(f\"Predicted {cor} samples from the test set, or {round(100.0*cor/txx.shape[0],2)} percent\") <pre>cost at epoch 0: 10.0488708348618. Predicted 99 samples correctly, or 9.9 percent\ncost at epoch 5: 6.886981741943602. Predicted 108 samples correctly, or 10.8 percent\ncost at epoch 10: 5.830659017113628. Predicted 119 samples correctly, or 11.9 percent\ncost at epoch 15: 5.314125473940891. Predicted 134 samples correctly, or 13.4 percent\ncost at epoch 20: 4.991783899894754. Predicted 155 samples correctly, or 15.5 percent\ncost at epoch 40: 4.215592478986276. Predicted 202 samples correctly, or 20.2 percent\ncost at epoch 60: 3.745509546814985. Predicted 243 samples correctly, or 24.3 percent\ncost at epoch 80: 3.3774466478225027. Predicted 289 samples correctly, or 28.9 percent\ncost at epoch 200: 2.1660509565047277. Predicted 480 samples correctly, or 48.0 percent\ncost at epoch 400: 1.4274599810089086. Predicted 617 samples correctly, or 61.7 percent\nPredicted 496 samples from the test set, or 49.6 percent\n</pre> In\u00a0[543]: Copied! <pre># some spot checking to make sure it's not an error\nprint(y_predd[:20])\nprint(tyy_raw[:20])\n</pre> # some spot checking to make sure it's not an error print(y_predd[:20]) print(tyy_raw[:20]) <pre>[7 3 1 7 9 1 4 4 4 9 0 2 9 0 1 4 9 7 5 4]\n[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tech/ml/basic_neural_net/#implement-a-basic-neural-network-from-scratch","title":"Implement a basic neural network from scratch\u00b6","text":"<ul> <li>Prerequisite: basic concepts of derivatives. Partial derivative, chain rule, quotient rule etc.</li> </ul>"},{"location":"tech/ml/basic_neural_net/#formulation","title":"Formulation\u00b6","text":"<p>Here's the formula for the simplified network at the very top, given the feature values of a single data point (x1, x2). The actual representation might vary in different tutorials but the idea is the same. You multiply each feature value with its weight towards nodes. The value at each node is the sum of value*weight of all features connected to it. Finally, a bias term <code>b</code> is added before the activation function.</p> <p>For activation, we use the good old sigmoid function to go from input layer to the hidden layer, aka first layer. As a reminder, the sigmoid function is defined as</p> <p>$$\\sigma(s_i) = \\frac{1}{1 + e^{-s_i}}$$</p> <p>where $s_i$ is the weighted sum at hidden layer node i. However, we'll do it differently at the output layer. As we want to predict the probabilities of the output being 0-9 respectively, it makes sense to have them add up to 1. The softmax function forces the sum of all probabilites to be 1, so we'll use it for the output layer</p> <p>$$\\sigma(s_i) = \\frac{e^{s_i}}{\\sum_{j=1}^{j=m}(e^{-s_j})}$$</p> <p>where m is the number of nodes in the output layer and $s_i$ is the weighted sum at output node i. Note that softmax at node_i depends on the weighted sum at other nodes.</p> <p>Writing all items out, we have</p> <p>Hidden layer:</p> <p>$$ \\begin{bmatrix} node1 \\\\ node2 \\\\ \\end{bmatrix} = \\sigma_{sigmoid}( \\begin{bmatrix}  w_{11} &amp; w_{12} &amp; w_{13} \\\\ w_{21} &amp; w_{22} &amp; w_{23} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix} ) $$</p> <p>Output layer</p> <p>$$ y =  \\sigma_{softmax}( \\begin{bmatrix} v_1 &amp; v_2 \\end{bmatrix} \\begin{bmatrix} node1 \\\\ node2 \\\\ \\end{bmatrix} + d ) $$</p> <p>$w$ and $v$ are the weights and $b, d$ are the bias terms.</p>"},{"location":"tech/ml/basic_neural_net/#feed-forward-from-input-to-prediction","title":"Feed forward, from input to prediction\u00b6","text":""},{"location":"tech/ml/basic_neural_net/#hidden-layer","title":"Hidden layer\u00b6","text":"<p>Suppose there are n samples, to go from those $x$ values to the hidden layer, we include all samples and layout the entire thing in a matrix form. Here</p> <ul> <li>$N^i$ means the vector $[node_1, node_2]$ calculated from the ith sample.</li> <li>$X^i$ means $[x_1, x_2, x_3]$ of the ith sample</li> <li>$W$ means the entire weight vector, transposed to whichever shape it needs to be for matrbix match, either 2x3 or 3x2</li> <li>Similarly, $B$ is the bias term vector.</li> </ul> <p>$$ \\begin{bmatrix}  N^1 \\\\ N^2 \\\\ ... \\\\ N^n \\end{bmatrix} = \\sigma( \\begin{bmatrix}  X^1 \\\\ X^2 \\\\ ... \\\\ X^n \\end{bmatrix} W + \\begin{bmatrix} B \\\\ B \\\\ ... \\\\ B \\\\ \\end{bmatrix} ) $$</p> <p>which is really:</p> <p>$$ \\begin{bmatrix}  node_{11} &amp; node_{12} \\\\ node_{21} &amp; node_{22} \\\\ ... \\\\ node_{n1} &amp; node_{n2} \\end{bmatrix} = \\sigma( \\begin{bmatrix}  x_{11} &amp; x_{12} &amp; x_{12} \\\\ x_{21} &amp; x_{22} &amp; x_{23} \\\\ ... \\\\ x_{n1} &amp; x_{n2} &amp; x_{n3} \\end{bmatrix} \\begin{bmatrix} w_{11} &amp; w_{21} \\\\ w_{12} &amp; w_{22} \\\\ w_{13} &amp; w_{23} \\end{bmatrix} + \\begin{bmatrix} b_1 &amp; b_2 \\\\ b_1 &amp; b_2 \\\\ ... \\\\ b_1 &amp; b_2 \\\\ \\end{bmatrix} ) $$</p>"},{"location":"tech/ml/basic_neural_net/#output-layer","title":"Output layer\u00b6","text":"<p>Similarly, to get to the output, we have</p> <p>$$ \\hat{Y} = \\begin{bmatrix} \\hat{y_1} \\\\ \\hat{y_2} \\\\ ... \\\\ \\hat{y_n} \\end{bmatrix} = \\sigma( \\begin{bmatrix}  node_{11} &amp; node_{12} \\\\ node_{21} &amp; node_{22} \\\\ ... \\\\ node_{n1} &amp; node_{n2} \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} + \\begin{bmatrix} d \\\\ d \\\\ ..\\\\ d \\end{bmatrix} ) = \\sigma(Nodes\\times V + D) $$</p> <p>($\\sigma$ represents either sigmoid or softmax)</p>"},{"location":"tech/ml/basic_neural_net/#lets-go-even-further-and-do-some-predictions","title":"Let's go even further and do some predictions!\u00b6","text":"<p>To tackle the number prediction problem, we use a larger network such that</p> <ul> <li>input layer has 121 features</li> <li>1st layer has 20 nodes</li> <li>output layer has 10 nodes, representing 0-9.</li> </ul> <p>Since we don't know what the weights and biases are, we will just use some random values and parameterized the functions. Summarizing all the calculations above, we have a basic naive predictor:</p>"},{"location":"tech/ml/basic_neural_net/#how-to-find-best-set-of-parameters","title":"How to find best set of parameters?\u00b6","text":"<p>Unlike linear regression, there's no algebraic solution to find optimal coefficients. We need to iteratively \"guess\" a set of weights, calculate the errors, and infer from the errors how we should modify the weights towards a better direction (lower errors).</p> <p>N x (update weights -&gt; calculate error -&gt; somehow find out how to update weights based on the error)</p>"},{"location":"tech/ml/basic_neural_net/#loss-function-and-cost","title":"Loss function and cost\u00b6","text":"<p>The function that calculates such errors is called the loss function. The sum of the loss over all the samples is called the cost. The practice of using errors at output layer to update all the weights is called back propagation, since we are going backwards. How do we use errors? Would be nice if someone can tell me something like \"if you change $w_1$ by 0.2, then you can reduce the errors by 0.3\". Keep in mind that while this tells us the direction of the next move, it doesn't tell us how much to move before we reach a minimum.</p> <p>To get such direction, we want to know the derivative of loss function with respect to each weight and bias. The prerequisite is that the loss function must be differentiable w.r.t all weights and biases.</p> <p>For a classification problem of n classes, let's use cross entropy as the loss:</p> <p>$$ L = -(y_{1}\\ln\\hat{y_1} + y_{2}\\ln\\hat{y_2}...+y_{n}\\hat{y_n}) $$ where vector $y$ is the truth and vector $\\hat{y}$ is the prediction. The predictioins come from softmax at the corresponding node, so we can also write the loss as</p> <p>$$L = -\\sum y_i \\ln \\sigma (s_i)$$</p> <p>where $s_i$ is the weighted sum plus bias at the output layer.</p> <p>To get an intuition of this function, we look at some examples</p>"},{"location":"tech/ml/basic_neural_net/#backpropagation","title":"Backpropagation\u00b6","text":"<p>With the cost calculated, we are ready to backpropagate! The idea is to trace the error all the way back to see to which direction we should move the weights. As we attribute the errors to each weight, a lot of partial derivatives are involved. Here's a simple demo of a single path from input to output layer. We want to take derivatives along such paths.</p> <p></p> <p>Take a deep breath and then take the derivative of cost over the weights w</p>"},{"location":"tech/ml/basic_neural_net/#some-math-magic-if-you-are-interesetd-else-jump-to-the-code-implementation","title":"Some math magic if you are interesetd, else jump to the code implementation\u00b6","text":"<p>To do some prep work for implementation, we need to do derivatives. We rely on the following rules:</p>"},{"location":"tech/ml/basic_neural_net/#chain-rule","title":"Chain rule\u00b6","text":"<p>$$\\frac{dy}{du} = \\frac{dy}{dx} \\frac{dx}{du}$$</p>"},{"location":"tech/ml/basic_neural_net/#quotient-rule","title":"Quotient rule\u00b6","text":"<p>if $h(x) = \\frac{f(x)}{g(x)}$, then $$h'(x) = \\frac{f'(x)g(x)-g'(x)f(x)}{g(x)^2}$$</p>"},{"location":"tech/ml/basic_neural_net/#useful-derivatives","title":"Useful derivatives\u00b6","text":"<p>$$ (e^x)' = e^x $$</p> <p>$$ (\\ln x)' = \\frac{1}{x} $$ From now on, I'll use $\\frac{dy}{dx}$ style notation and $f'$ style interchangeably for my convenience. Apology for potential confusions.</p>"},{"location":"tech/ml/basic_neural_net/#deriavtive-of-cross-entropy-loss","title":"Deriavtive of cross entropy loss\u00b6","text":"<p>With some chain rule trick, we get $$ \\large \\begin{align} \\frac{\\partial L}{\\partial s_i} &amp;= \\frac{\\partial}{\\partial s_i} (-\\sum y_k \\ln \\sigma (s_k)) \\\\ &amp;= -\\sum y_k \\frac{\\partial}{\\partial s_i} \\ln \\sigma(s_k) \\\\ &amp;= -\\sum y_k \\frac{\\partial \\ln \\sigma (s_k)}{\\partial \\sigma (s_k)} \\frac{\\partial \\sigma(s_k)}{\\partial s_i} \\\\ &amp;= -\\sum y_k \\frac{1}{\\sigma(s_k)} \\frac{\\partial (\\sigma(s_k))}{\\partial s_i} \\end{align} $$</p> <p>We already have $\\sigma(s_k)$ calculated during feed forward. We next derive $\\Large \\frac{\\partial (\\sigma(s_k))}{\\partial s_i}$</p>"},{"location":"tech/ml/basic_neural_net/#derivative-of-softmax","title":"Derivative of softmax\u00b6","text":"<p>The partial derivative of softmax at class i w.r.t. $s_j$ is:</p> <p>$$  \\large  \\frac{\\partial(\\sigma(s_i))}{\\partial(s_j)} = \\dfrac{\\partial \\frac{e^{s_i}}{\\sum_{1}^{m}(e^{s_k})}}{\\partial(s_j)} $$</p> <p>There are two cases, $i==j$ or $i\\neq j$</p> <p>if $i==j$, then $\\Large \\frac{\\partial}{\\partial s_i}(\\sum e^{s_k})$ reduces to $\\Large \\frac{\\partial}{\\partial s_i} e^{s_i}$, because $s_i$ and $s_k$s are independent</p> <p>$$ \\large \\begin{align} \\dfrac{\\partial \\frac{e^{s_i}}{\\sum_{1}^{m}(e^{s_k})}}{\\partial(s_i)} &amp;= \\frac{(e^{s_i})'\\sum(e^{s_k})-(\\sum e^{s_k})'e^{s_i}}{(\\sum e^{s_k})^2} \\\\ &amp;= \\frac{e^{s_i}\\sum e^{s_k} - e^{s_i}e^{s_i}}{(\\sum e^{s_k})^2} \\\\ &amp;= \\frac{e^{s_i} (\\sum e^{s_k} - e^{s_i})}{\\sum e^{s_k} \\sum e^{s_k}} \\\\ &amp;= \\frac{e^{s_i}}{\\sum e^{s_k}} (1 - \\frac{e^{s_i}}{\\sum e^{s_k}}) = \\sigma(s_i)(1-\\sigma(s_i)) \\end{align} $$</p> <p>This is nice, we can express the derivative of softmax in terms of the softmax itself</p> <p>if $i\\neq j$, we have $\\Large \\frac{\\partial}{\\partial e_j} e^{s_i}=0$, and $\\Large \\frac{\\partial}{\\partial s_j}(\\sum e^{s_k}) = \\frac{\\partial}{\\partial s_j} e^{s_j}$, therefore</p> <p>$$ \\large \\begin{align} \\dfrac{\\partial(\\frac{e^{s_i}}{\\sum_{1}^{m}(e^{s_k})})}{\\partial(s_j)} &amp;= \\frac{(\\frac{\\partial}{\\partial s_j}e^{s_i})\\sum(e^{s_k})-(\\frac{\\partial}{\\partial s_j}\\sum e^{s_k})e^{s_i}}{(\\sum e^{s_k})^2} \\\\ &amp;= \\frac{0-e^{s_j}e^{s_i}}{\\sum e^{s_k}\\sum e^{s_k}} \\\\ &amp;= -\\sigma(s_i)\\sigma(s_j) \\end{align} $$</p> <p>This is nice too. Now we have both cases covered for the derivative of softmax.</p>"},{"location":"tech/ml/basic_neural_net/#derivative-of-the-last-layer","title":"Derivative of the last layer\u00b6","text":"<p>Pulling all together, we have</p> <p>$$ \\large \\begin{align} \\frac{\\partial L}{\\partial s_i} &amp;= -y_i\\frac{1}{\\sigma(s_i)}(\\sigma(s_i)(1-\\sigma(s_i)) \\\\ &amp;- \\sum_{k\\neq i}y_k\\frac{1}{\\sigma(s_k)}(-\\sigma(s_i)\\sigma(s_k)) \\\\ &amp;= -y_i(1-\\sigma(s_i)) - \\sum_{k\\neq i} -y_k\\sigma(s_i) \\\\ &amp;= -y_i + \\sigma(s_i)(y_i + \\sum_{k\\neq i} y_k) \\end{align} $$</p> <p>Obviously, $y_i + \\sum_{k\\neq i} y_k$ equals 1 as there's only one object in the image. For only one class can the truth label be 1. So the whole thing simplifies to</p> <p>$$ \\large \\frac{\\partial L}{\\partial s_i} = -y_i + \\sigma(s_i) $$</p> <p>which is exactly $\\hat{y_i} - y_i$. Magic!</p>"},{"location":"tech/ml/basic_neural_net/#derivativer-over-weights-between-hidden-and-outupt-layers","title":"Derivativer over weights between hidden and outupt layers\u00b6","text":"<p>The last piece is the derivative of softmax over the weights, since weights are what need to be adjusted.</p> <p>$$ \\large \\frac{\\partial s_i}{\\partial v_{ik}} = \\frac{\\partial (v_{i1}x_1 + v_{i2}x_2 ... + v_{ik}x_k + ...)}{\\partial v_{ik}} = x_k $$</p> <p>Where $\\large v_{ik}$s are the weights of edges connecting the hidden layer nodes with the output layer node i, and x here is the node from the hidden layer, Not the input layer! I know I should have better notations</p>"},{"location":"tech/ml/basic_neural_net/#derivative-over-weights-between-input-and-hidden-layers","title":"Derivative over weights between input and hidden layers\u00b6","text":"<p>Updates to weights $w$ is similar. Recall that earlier we have</p> <p></p> <p>$$ \\large \\frac{\\partial L}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial s_{ok}} \\frac{\\partial s_{ok}}{\\partial a_{hj}} \\frac{\\partial a_{hj}}{\\partial s_{hj}} \\frac{\\partial s_{hj}}{\\partial w_{ij}} $$</p> <p>For the weight between input i and hidden layer j, from the loss function attributed to output node k</p>"},{"location":"tech/ml/basic_neural_net/#neural-network","title":"Neural network\u00b6","text":"<p>It took me some trial and errors to get the dimensions right for vectorization. I'll also add the option to modify the size of the hidden layer</p>"},{"location":"tech/ml/basic_neural_net/#train-and-test-modes","title":"Train and test modes\u00b6","text":"<p>Make a new model. Take a subset of training samples to train (fit) it and monitor the cost over epochs. To assess the effectiveness of this algorithm, we also look at how many samples we predict correctly.</p>"},{"location":"tech/ml/basic_neural_net/#performance-on-the-test-set","title":"Performance on the test set\u00b6","text":"<p>Recall that we saved a subset as the test set. Let's see how it performs on the test set. We hope it's not too much worse than the train set. It's almost guaranteed that it would be worse than the train set, because the model iterrates over and over on the train set whereas it hasn't \"seen\" the test set yet.</p>"},{"location":"tech/ml/basic_neural_net/#open-questions","title":"Open questions\u00b6","text":"<p>Try above a few times and pay attention to a few things:</p> <ul> <li>What's the initial quality of prediction, before any back propagation? How about when training is done?</li> <li>How does the quality of prediction correlate with the cost?</li> <li>How does cost go down as we do more epochs? How many epochs did it take to become flat?</li> <li>Does cost always reduce or sometimes increarse as well?</li> <li>How varied are the results each time you start fresh with a new network</li> <li>What happens if you adjust the number of nodes, number of epochs, and/or learning rate?</li> <li>What happens if you use more/fewer training samples?</li> </ul> <p>Further topics for research</p> <ul> <li>Regularization</li> <li>Overfitting</li> <li>Factors impacting the performance of neural networks</li> </ul>"},{"location":"tech/ml/genetic_algorithm/","title":"Genetic algorithm","text":"<p>There's already a good explanation here: https://towardsdatascience.com/introduction-to-genetic-algorithms-including-example-code-e396e98d8bf3</p> <p>but I'll use a different example to demonstrate how to do optimization by selecting the most fit individuals and allowing them to produce progency of potentially higher fitness at each generation.</p> <p>Problem statement: given a shuriken (ninja dart) represented as segments, I want to write a method that draws another shuriken as close to this shuriken as possible. Another way to say it is that I want to minimize the distance between the new and old shuriken.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nfrom numpy import random\nimport pylab as pl\nfrom matplotlib import collections  as mc\n</pre> import numpy as np from numpy import random import pylab as pl from matplotlib import collections  as mc In\u00a0[2]: Copied! <pre># A method to return coordinates for the segments of a shuriken\n\ndef make_shuriken(x, y):\n     return [[(x, y - 1.5), (x + 0.5, y - 0.5)],\n             [(x - 0.5, y - 0.5), (x, y - 1.5)],\n             [(x + 0.5, y - 0.5), (x + 1.5, y)],\n             [(x + 1.5, y), (x + 0.5, y + 0.5)],\n             [(x + 0.5, y + 0.5), (x, y + 1.5)],\n             [(x, y + 1.5), (x - 0.5, y + 0.5)],\n             [(x - 0.5, y + 0.5), (x - 1.5, y)],\n             [(x - 1.5, y), (x - 0.5, y - 0.5)]\n            ]\n</pre> # A method to return coordinates for the segments of a shuriken  def make_shuriken(x, y):      return [[(x, y - 1.5), (x + 0.5, y - 0.5)],              [(x - 0.5, y - 0.5), (x, y - 1.5)],              [(x + 0.5, y - 0.5), (x + 1.5, y)],              [(x + 1.5, y), (x + 0.5, y + 0.5)],              [(x + 0.5, y + 0.5), (x, y + 1.5)],              [(x, y + 1.5), (x - 0.5, y + 0.5)],              [(x - 0.5, y + 0.5), (x - 1.5, y)],              [(x - 1.5, y), (x - 0.5, y - 0.5)]             ] In\u00a0[3]: Copied! <pre># plot shurikens\nfrom itertools import cycle\ncycol = cycle('bgrcmk')\n\ndef plot_shurikens(shurikens, colors=None):\n    fig, ax = pl.subplots()\n    if not colors:\n        colors = [random.rand(3,) for _ in shurikens]\n    for shuriken, color in zip(shurikens, colors):\n        lc = mc.LineCollection(shuriken, linewidths=2, color=color)\n        ax.add_collection(lc)\n    ax.plot()\n    pl.xlim((-5, 5))\n    pl.ylim((-5, 5))\n    pl.gca().set_aspect('equal', adjustable='box')\n</pre> # plot shurikens from itertools import cycle cycol = cycle('bgrcmk')  def plot_shurikens(shurikens, colors=None):     fig, ax = pl.subplots()     if not colors:         colors = [random.rand(3,) for _ in shurikens]     for shuriken, color in zip(shurikens, colors):         lc = mc.LineCollection(shuriken, linewidths=2, color=color)         ax.add_collection(lc)     ax.plot()     pl.xlim((-5, 5))     pl.ylim((-5, 5))     pl.gca().set_aspect('equal', adjustable='box') In\u00a0[4]: Copied! <pre>shuriken1 = make_shuriken(1, 1)\nshuriken2 = make_shuriken(2, 2)\nplot_shurikens([shuriken1, shuriken2], colors=['blue', 'red'])\n</pre> shuriken1 = make_shuriken(1, 1) shuriken2 = make_shuriken(2, 2) plot_shurikens([shuriken1, shuriken2], colors=['blue', 'red']) <p>Workflow of genetic algorithm:</p> <pre><code>1. Generate population randomly\n&lt;--------&gt;\n&lt;+++-----&gt;\n&lt;---+----&gt;\n&lt;++++++++&gt;\n...\n\n2. Calculate fitness for each chromosome in the population\n&lt;--------&gt; 193\n&lt;+++-----&gt; 12\n&lt;---+----&gt; 34\n&lt;++++++++&gt; 392\n...\n\n3. Pick top individuals\n&lt;--------&gt; 193\n&lt;++++++++&gt; 392\n\n4. crossover to get two children. This generates the next population\n&lt;---+++++&gt;\n&lt;+++-----&gt;\n...\n\n5. Calculate fitness again\n&lt;---+++++&gt; 422\n&lt;+++-----&gt; 235\n\nGo back to 2 and repeat\n</code></pre> <p>You might notice that there's a reduction in population in step 4, but above graph is a simplified representation. In implementation, we take multiple combinations of the best fit parents to produce many children until the population size is the same as previous generation.</p> In\u00a0[5]: Copied! <pre>from numpy import random\n\ndef make_chromosome():\n    x, y = -4 + 8 * random.random(), -4 + 8 * random.random()\n    return [[(x, y - 2 * random.random()), (x + 2 * random.random(), y - 2 * random.random())],\n             [(x - 2 * random.random(), y - 0.5), (x, y - 2 * random.random())],\n             [(x + 2 * random.random(), y - 0.5), (x + 2 * random.random(), y)],\n             [(x + 2 * random.random(), y), (x + 0.5, y + 2 * random.random())],\n             [(x + 2 * random.random(), y + 0.5), (x, y + 2 * random.random())],\n             [(x, y + 2 * random.random()), (x - 0.5, y + 2 * random.random())],\n             [(x - 2 * random.random(), y + 0.5), (x - 2 * random.random(), y)],\n             [(x - 2 * random.random(), y), (x - 0.5, y - 2 * random.random())]\n            ]\n\ndef make_population(size=100):\n    return [make_chromosome() for _ in range(size)]\n\nplot_shurikens(make_population(size=4),\n           ['blue', 'red', 'orange', 'green'])\n</pre> from numpy import random  def make_chromosome():     x, y = -4 + 8 * random.random(), -4 + 8 * random.random()     return [[(x, y - 2 * random.random()), (x + 2 * random.random(), y - 2 * random.random())],              [(x - 2 * random.random(), y - 0.5), (x, y - 2 * random.random())],              [(x + 2 * random.random(), y - 0.5), (x + 2 * random.random(), y)],              [(x + 2 * random.random(), y), (x + 0.5, y + 2 * random.random())],              [(x + 2 * random.random(), y + 0.5), (x, y + 2 * random.random())],              [(x, y + 2 * random.random()), (x - 0.5, y + 2 * random.random())],              [(x - 2 * random.random(), y + 0.5), (x - 2 * random.random(), y)],              [(x - 2 * random.random(), y), (x - 0.5, y - 2 * random.random())]             ]  def make_population(size=100):     return [make_chromosome() for _ in range(size)]  plot_shurikens(make_population(size=4),            ['blue', 'red', 'orange', 'green']) In\u00a0[6]: Copied! <pre>from scipy.spatial.distance import cdist\nfrom copy import deepcopy\n\nshuriken = make_shuriken(1, 1)\n\ndef distance(p1, p2):\n    # distance between two points\n    return ((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2) ** 0.5\n\ndef fitness(chromosome):\n    dist = 0\n    list1 = deepcopy(shuriken)\n    list2 = deepcopy(chromosome)\n    list1.sort(key=lambda x: x[0])\n    list2.sort(key=lambda x: x[0])\n    for i in range(8):\n        dist += distance(list1[i][0], list2[i][0])\n        dist += distance(list1[i][1], list2[i][1])\n    return dist\nfitness(shuriken2)\n</pre> from scipy.spatial.distance import cdist from copy import deepcopy  shuriken = make_shuriken(1, 1)  def distance(p1, p2):     # distance between two points     return ((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2) ** 0.5  def fitness(chromosome):     dist = 0     list1 = deepcopy(shuriken)     list2 = deepcopy(chromosome)     list1.sort(key=lambda x: x[0])     list2.sort(key=lambda x: x[0])     for i in range(8):         dist += distance(list1[i][0], list2[i][0])         dist += distance(list1[i][1], list2[i][1])     return dist fitness(shuriken2) Out[6]: <pre>22.62741699796953</pre> In\u00a0[7]: Copied! <pre>population = make_population(10)\npopulation_fitness = [(chromosome, fitness(chromosome)) for chromosome in population]\nprint([f[1] for f in population_fitness])\nplot_shurikens(population + [shuriken])\n</pre> population = make_population(10) population_fitness = [(chromosome, fitness(chromosome)) for chromosome in population] print([f[1] for f in population_fitness]) plot_shurikens(population + [shuriken]) <pre>[40.666787609293785, 45.670299596809585, 82.28526328078193, 54.44839135656313, 59.7893497391738, 48.942168138652235, 37.291500819971255, 25.956759623441698, 27.60023418787563, 22.21189442824818]\n</pre> In\u00a0[8]: Copied! <pre>population_fitness.sort(key=lambda x: x[1])\nbest = [p[0] for p in population_fitness][:5]\nplot_shurikens(best + [shuriken])\n</pre> population_fitness.sort(key=lambda x: x[1]) best = [p[0] for p in population_fitness][:5] plot_shurikens(best + [shuriken]) In\u00a0[9]: Copied! <pre>def crossover(ch1, ch2):\n    \"\"\"\n    For each gene, pick randomly whether it is from dad or mom\n    \"\"\"\n    new_chromosome = []\n    for i in range(8):\n        if random.random() &lt; 0.5:\n            new_chromosome.append(ch1[i])\n        else:\n            new_chromosome.append(ch2[i])\n    return new_chromosome\n</pre> def crossover(ch1, ch2):     \"\"\"     For each gene, pick randomly whether it is from dad or mom     \"\"\"     new_chromosome = []     for i in range(8):         if random.random() &lt; 0.5:             new_chromosome.append(ch1[i])         else:             new_chromosome.append(ch2[i])     return new_chromosome In\u00a0[10]: Copied! <pre>new_population = []\nfor _ in range(10):\n    # pick two parents randomly\n    idx1, idx2 = random.choice(len(best), 2, replace=False)\n    parent1, parent2 = best[idx1], best[idx2]\n    new_population.append(crossover(parent1, parent2))\n</pre> new_population = [] for _ in range(10):     # pick two parents randomly     idx1, idx2 = random.choice(len(best), 2, replace=False)     parent1, parent2 = best[idx1], best[idx2]     new_population.append(crossover(parent1, parent2)) <p>Not perfect, but the new generation is not as spreaded over as the first population</p> In\u00a0[11]: Copied! <pre>plot_shurikens(new_population + [shuriken])\n</pre> plot_shurikens(new_population + [shuriken]) <p>Let's then evolve over a few generations</p> In\u00a0[12]: Copied! <pre>n_iter = 10\n\ncurrent_genration = best\n\nfor i in range(n_iter):\n    population_fitness = [(chrom, fitness(chrom)) for chrom in current_genration]\n    population_fitness.sort(key=lambda x: x[1])\n    best = [p[0] for p in population_fitness][:5]\n\n    new_population = []\n    for _ in range(10):\n        # pick two parents randomly\n        idx1, idx2 = random.choice(len(best), 2, replace=False)\n        parent1, parent2 = best[idx1], best[idx2]\n        new_population.append(crossover(parent1, parent2))\n    current_genration = new_population\n</pre> n_iter = 10  current_genration = best  for i in range(n_iter):     population_fitness = [(chrom, fitness(chrom)) for chrom in current_genration]     population_fitness.sort(key=lambda x: x[1])     best = [p[0] for p in population_fitness][:5]      new_population = []     for _ in range(10):         # pick two parents randomly         idx1, idx2 = random.choice(len(best), 2, replace=False)         parent1, parent2 = best[idx1], best[idx2]         new_population.append(crossover(parent1, parent2))     current_genration = new_population <p>The population converges to one solution, i.e. local optimal. Several factors affect how far it is from the actual optimal:</p> <ol> <li>How good is the original population?</li> <li>Number of iterations</li> <li>How genetic contents are passed from one to the next generation. In this example, only a simple crossover operation is performed</li> </ol> <p>and many many more</p> In\u00a0[13]: Copied! <pre>plot_shurikens(current_genration + [shuriken])\n</pre> plot_shurikens(current_genration + [shuriken]) In\u00a0[14]: Copied! <pre>def evolve(initial_population, n_iter):\n    population_size = len(initial_population)\n    current_generation = initial_population\n    for i in range(n_iter):\n        population_fitness = [(chrom, fitness(chrom)) for chrom in current_generation]\n        population_fitness.sort(key=lambda x: x[1])\n        best = [p[0] for p in population_fitness][:int(population_size / 4)]\n\n        new_population = []\n        for _ in range(population_size):\n            # pick two parents randomly\n            idx1, idx2 = random.choice(len(best), 2, replace=False)\n            parent1, parent2 = best[idx1], best[idx2]\n            new_population.append(crossover(parent1, parent2))\n        current_generation = new_population\n    return current_generation\n    \ndef optimize(population_size, n_iter):\n    initial_population = make_population(population_size)\n    last_generation = evolve(initial_population, n_iter)\n    population_fitness = [(chrom, fitness(chrom)) for chrom in last_generation]\n    population_fitness.sort(key=lambda x: x[1])\n    return population_fitness[0]\n</pre> def evolve(initial_population, n_iter):     population_size = len(initial_population)     current_generation = initial_population     for i in range(n_iter):         population_fitness = [(chrom, fitness(chrom)) for chrom in current_generation]         population_fitness.sort(key=lambda x: x[1])         best = [p[0] for p in population_fitness][:int(population_size / 4)]          new_population = []         for _ in range(population_size):             # pick two parents randomly             idx1, idx2 = random.choice(len(best), 2, replace=False)             parent1, parent2 = best[idx1], best[idx2]             new_population.append(crossover(parent1, parent2))         current_generation = new_population     return current_generation      def optimize(population_size, n_iter):     initial_population = make_population(population_size)     last_generation = evolve(initial_population, n_iter)     population_fitness = [(chrom, fitness(chrom)) for chrom in last_generation]     population_fitness.sort(key=lambda x: x[1])     return population_fitness[0] In\u00a0[15]: Copied! <pre>def test(pop_size, n_iter):\n    new_shuriken = optimize(pop_size, n_iter)\n    plot_shurikens([new_shuriken[0], shuriken], colors=['blue', 'red'])\n</pre> def test(pop_size, n_iter):     new_shuriken = optimize(pop_size, n_iter)     plot_shurikens([new_shuriken[0], shuriken], colors=['blue', 'red']) In\u00a0[16]: Copied! <pre>test(10, 10)\n</pre> test(10, 10) In\u00a0[17]: Copied! <pre>test(100, 10)\n</pre> test(100, 10) In\u00a0[18]: Copied! <pre>test(100, 50)\n</pre> test(100, 50) In\u00a0[19]: Copied! <pre>test(500, 50)\n</pre> test(500, 50) In\u00a0[20]: Copied! <pre>test(1000, 10)\n</pre> test(1000, 10) In\u00a0[21]: Copied! <pre>test(1000, 30)\n</pre> test(1000, 30) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tech/ml/genetic_algorithm/#yet-another-implementation-of-genetic-algorithm","title":"Yet another implementation of genetic algorithm\u00b6","text":""},{"location":"tech/ml/genetic_algorithm/#setting-up","title":"Setting up\u00b6","text":""},{"location":"tech/ml/genetic_algorithm/#now-write-a-function-to-produce-a-random-chromosome","title":"Now, write a function to produce a random chromosome\u00b6","text":"<p>Here instead of making a shuriken, just draw 8 segments randomly. They don't even need to be connected to each other. Also write a method to create a population of chromosomes.</p>"},{"location":"tech/ml/genetic_algorithm/#next-define-fitness-function","title":"Next, define fitness function\u00b6","text":"<p>as distance between the original shuriken and the input segments. For simplicity, simply sum up euclidean distance between the points.</p>"},{"location":"tech/ml/genetic_algorithm/#ready-first-population","title":"READY, first population\u00b6","text":""},{"location":"tech/ml/genetic_algorithm/#then-we-take-the-top","title":"Then we take the top.\u00b6","text":"<p>Here, best fit = minimal fitness function value for the set up of this problem</p>"},{"location":"tech/ml/genetic_algorithm/#generate-next-population","title":"Generate next population\u00b6","text":"<p>by crossing over the top parents</p>"},{"location":"tech/ml/genetic_algorithm/#bonus","title":"Bonus\u00b6","text":"<p>Let's put everything together and do some clean up, to make it easier to test a few scenarios</p> <p>We write a function <code>evolve</code> that does a few iterations of evolution and a function <code>optimize</code> that allows you to specify population size and the number of generations.</p> <p>Lastly, a method <code>test</code> to run optimization and then plot the optimization result along with the original shuriken.</p>"},{"location":"tech/ml/genetic_algorithm/#then-run-different-combinations-of-population-size-and-number-of-generations","title":"Then run different combinations of population size and number of generations\u00b6","text":"<p>Note that since this method is stochastic, if you run this notebook again, it's almost impossible to get the same results. Also there's no guarantee that larger population and number of iterations will produce better results. They may or may not. In practice, it takes a lot of experimentation to find the most suitable implementation for the problem you are interested in.</p>"},{"location":"tech/ml/knn/","title":"K Nearest Neighbors From Scratch","text":"In\u00a0[\u00a0]: Copied!"},{"location":"tech/ml/knn/#k-nearest-neighbors-from-scratch","title":"K Nearest Neighbors From Scratch\u00b6","text":"<p>KNN is a conceptually simple but effective algorithm that is commonly used in classification tasks. With existing knowledge about how data points of different classes are located in different clusters, we make predictions on the classes of new data points. Basic requisite for KNN to work well:</p> <ul> <li>Different classes indeed form well-isolated clusters</li> <li>We have a good distance function such that d(p1, p2) is small when p1 and p2 are from the same class and big when p1 and p2 are from different classes.</li> </ul>"},{"location":"tech/ml/linear_regression/","title":"Implement a linear regression model by minimizing sum of squared errors","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"darkgrid\")\n</pre> import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt sns.set(style=\"darkgrid\") In\u00a0[2]: Copied! <pre># Generate sample data points\n# let y = 3x + 4\nx = np.random.randint(low=0, high=100, size=100)\ndf = pd.DataFrame({'x': x})\ndf['base_y'] = df.x.apply(lambda x: 3 * x + 4)\ndf['mod_y'] = df.base_y.apply(lambda y: y + 10 * np.random.normal())\n</pre> # Generate sample data points # let y = 3x + 4 x = np.random.randint(low=0, high=100, size=100) df = pd.DataFrame({'x': x}) df['base_y'] = df.x.apply(lambda x: 3 * x + 4) df['mod_y'] = df.base_y.apply(lambda y: y + 10 * np.random.normal()) In\u00a0[3]: Copied! <pre>sns.scatterplot(data=df, x='x', y='mod_y')\n</pre> sns.scatterplot(data=df, x='x', y='mod_y') Out[3]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x11ccd6710&gt;</pre> In\u00a0[4]: Copied! <pre>class LinearRegressor:\n    def __init__(self):\n        self.slope = None\n        self.intercept = None\n    def fit(self, x, y):\n        x_mean = np.mean(x)\n        y_mean = np.mean(y)\n        n = len(x)\n        a_numerator = sum([x_i*y_i for x_i, y_i in zip(x, y)]) - n * x_mean * y_mean\n        a_denominator = sum([x_i**2 for x_i in x]) - n * (x_mean ** 2)\n        self.slope = 1.0 * a_numerator / a_denominator\n        self.intercept = y_mean - self.slope * x_mean\n    def predict(self, x):\n        if self.slope is None:\n            raise ValueError(\"Please fit first\")\n        if isinstance(x, float):\n            return self.slope * x + self.intercept\n        elif isinstance(x, pd.DataFrame):\n            return x.apply(lambda val: val * self.slope + self.intercept)\n        else:\n            return [val * self.slope + self.intercept for val in x]\n    def summary(self):\n        return self.slope, self.intercept\n</pre> class LinearRegressor:     def __init__(self):         self.slope = None         self.intercept = None     def fit(self, x, y):         x_mean = np.mean(x)         y_mean = np.mean(y)         n = len(x)         a_numerator = sum([x_i*y_i for x_i, y_i in zip(x, y)]) - n * x_mean * y_mean         a_denominator = sum([x_i**2 for x_i in x]) - n * (x_mean ** 2)         self.slope = 1.0 * a_numerator / a_denominator         self.intercept = y_mean - self.slope * x_mean     def predict(self, x):         if self.slope is None:             raise ValueError(\"Please fit first\")         if isinstance(x, float):             return self.slope * x + self.intercept         elif isinstance(x, pd.DataFrame):             return x.apply(lambda val: val * self.slope + self.intercept)         else:             return [val * self.slope + self.intercept for val in x]     def summary(self):         return self.slope, self.intercept In\u00a0[5]: Copied! <pre>model = LinearRegressor()\nmodel.fit(df.x, df.mod_y)\ndf['predicted'] = model.predict(df.x)\n</pre> model = LinearRegressor() model.fit(df.x, df.mod_y) df['predicted'] = model.predict(df.x) In\u00a0[6]: Copied! <pre>sns.lineplot(data=df, x='x', y='predicted', color='r')\nsns.scatterplot(data=df, x='x', y='mod_y', alpha=0.8)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\n</pre> sns.lineplot(data=df, x='x', y='predicted', color='r') sns.scatterplot(data=df, x='x', y='mod_y', alpha=0.8) plt.xlabel(\"x\") plt.ylabel(\"y\") plt.show() In\u00a0[7]: Copied! <pre>from mpl_toolkits.mplot3d import axes3d, Axes3D\n\nfig = plt.figure(figsize=(7, 7))\nax = fig.add_subplot(1, 1, 1, projection='3d')\n\n# y = 4 * x1 + 3 * x2\nx1 = np.random.randint(low=0, high=100, size=100)\nx2 = np.random.randint(low=0, high=100, size=100)\ny = [4 * x1_i + 3 * x2_i for x1_i, x2_i in zip(x1, x2)]\ny = [val + 50 * np.random.normal() for val in y]\nax.scatter(x1, x2, y, s=50, alpha=1, edgecolors='w')\n\nax.set_xlabel('X1')\nax.set_ylabel('X2')\nax.set_zlabel('Y')\nplt.show()\n</pre> from mpl_toolkits.mplot3d import axes3d, Axes3D  fig = plt.figure(figsize=(7, 7)) ax = fig.add_subplot(1, 1, 1, projection='3d')  # y = 4 * x1 + 3 * x2 x1 = np.random.randint(low=0, high=100, size=100) x2 = np.random.randint(low=0, high=100, size=100) y = [4 * x1_i + 3 * x2_i for x1_i, x2_i in zip(x1, x2)] y = [val + 50 * np.random.normal() for val in y] ax.scatter(x1, x2, y, s=50, alpha=1, edgecolors='w')  ax.set_xlabel('X1') ax.set_ylabel('X2') ax.set_zlabel('Y') plt.show() <p>Suppose the space is $n + 1$ dimensions and there are $m &gt;= n + 1$ sampless. Each target variable $y$ has the form $y = w_0 + w_1x_1 + w_2x_2 + ... w_nx_n$. Error for each sample is</p> <p>$e^2 = (y - w_0 - w_1x_1 - w_2x_2 - ... w_nx_n)^2$, giving the following overal loss function,</p> <p>$\\Sigma_1^m e_i^2 = \\Sigma_1^m (y_i - w_0 - w_1x_{i1} - w_2x_{i2} - ... -w_nx_{in})^2$</p> <p>Following the same approach as single-variable linear regresion, take derivative w.r.t. each weight and set to 0. We get $n + 1$ equations</p> <p>$$\\Sigma (y_i - w_0 - w_1x_{i1} - w_2x_{i2} ... - w_nx_{in}) = 0$$ $$\\Sigma x_{i1}(y_i - w_0 - w_1x_{i1} - w_2x_{i2} ... - w_nx_{in}) = 0$$ $$...$$ $$\\Sigma x_{in}(y_i - w_0 - w_1x_{i1} - w_2x_{i2} ... - w_nx_{in}) = 0$$</p> <p>A more intuitive arrangement is</p> <p>$$\\Sigma y_i = \\Sigma (w_0 + w_1x_{i1} + w_2x_{i2} ... + w_nx_{in})$$ $$\\Sigma y_i x_{i1} = \\Sigma x_{i1}(w_0 + w_1x_{i1} + w_2x_{i2} ... + w_nx_{in})$$ $$...$$ $$\\Sigma y_i x_{in} = \\Sigma x_{in} (w_0 + w_1x_{i1} + w_2x_{i2} ... + w_nx_{in})$$</p> <p>Putting to matrix form, left hand side is</p> <p>$$ \\begin{bmatrix}  1 &amp; 1 &amp; 1 &amp; ... &amp; 1 \\\\ x_{11} &amp; x_{21} &amp; x_{31} &amp; ... &amp; x_{m1} \\\\ ... \\\\ x_{1n} &amp; x_{2n} &amp; ... &amp; ... &amp; x_{mn} \\end{bmatrix} \\begin{bmatrix}  y_1 \\\\ y_2 \\\\ .\\\\ .\\\\ y_m \\end{bmatrix}  = X^T Y $$</p> <p>Right hand side is</p> <p>$$ \\begin{bmatrix}  \\Sigma 1 &amp; \\Sigma x_{i1} &amp; \\Sigma x_{i2} &amp; ... &amp; \\Sigma x_{in} \\\\ \\Sigma x_{i1} &amp; \\Sigma x_{i1}x_{i1} &amp; \\Sigma x_{i1}x_{i2} &amp; ... &amp; \\Sigma x_{i1}x_{in} \\\\ ... \\\\ \\Sigma x_{in} &amp; \\Sigma x_{in}x_{i1} &amp; ... &amp; ... &amp; \\Sigma x_{in}x_{in} \\end{bmatrix} \\begin{bmatrix}  w_0 \\\\ w_1 \\\\ .\\\\ .\\\\ w_n \\end{bmatrix} =  \\begin{bmatrix}  1 &amp; 1 &amp; 1 &amp; ... &amp; 1 \\\\ x_{11} &amp; x_{21} &amp; x_{31} &amp; ... &amp; x_{m1} \\\\ ... \\\\ x_{1n} &amp; x_{2n} &amp; ... &amp; ... &amp; x_{mn} \\end{bmatrix} \\begin{bmatrix}  1 &amp; x_{11} &amp; x_{12} &amp; ... &amp; x_{1n} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; ... &amp; x_{2n} \\\\ ... \\\\ 1 &amp; x_{m1} &amp; ... &amp; ... &amp; x_{mn} \\end{bmatrix} \\begin{bmatrix}  w_0 \\\\ w_1 \\\\ .\\\\ .\\\\ w_n \\end{bmatrix} = X^T X w $$</p> <p>left = right means $X^T Y = X^T X w$, or $$w = (X^T X)^{-1} X^T Y$$</p> <p>What if $X^T X$ is not invertible? Then there must be redundancy in the variables and there can be multiple solutions. Use professional libraries in such case.</p> In\u00a0[8]: Copied! <pre>class MultvariantRegressor:\n    def __init__(self):\n        self.weights = None\n\n    def fit(self, x, y):\n        if isinstance(x, list):\n            X = np.vstack(x).transpose()\n        else:\n            X = x\n        Y = np.array(y).transpose()\n        inversed = np.linalg.inv(np.matmul(X.transpose(), X))\n        self.weights = np.matmul(np.matmul(inversed, X.transpose()), Y)\n    def predict(self, x):\n        if isinstance(x, list):\n            X = np.vstack(x).transpose()\n        else:\n            X = x\n        return np.matmul(X, self.weights)\n    def summary(self):\n        return self.weights\n</pre> class MultvariantRegressor:     def __init__(self):         self.weights = None      def fit(self, x, y):         if isinstance(x, list):             X = np.vstack(x).transpose()         else:             X = x         Y = np.array(y).transpose()         inversed = np.linalg.inv(np.matmul(X.transpose(), X))         self.weights = np.matmul(np.matmul(inversed, X.transpose()), Y)     def predict(self, x):         if isinstance(x, list):             X = np.vstack(x).transpose()         else:             X = x         return np.matmul(X, self.weights)     def summary(self):         return self.weights In\u00a0[9]: Copied! <pre>model = MultvariantRegressor()\nmodel.fit([x1, x2], y)\npred = model.predict([x1, x2])\n</pre> model = MultvariantRegressor() model.fit([x1, x2], y) pred = model.predict([x1, x2]) In\u00a0[10]: Copied! <pre>fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(1, 1, 1, projection='3d')\n\nax.scatter(x1, x2, y, s=50, alpha=1, edgecolors='w', color='b', label='actual')\nax.scatter(x1, x2, pred, s=50, alpha=0.8, edgecolors='w', color='r', label='predicted')\n\ndef fun(val1, val2):\n    return model.weights[0] * val1 + model.weights[1] * val2\n\nx1_surface = x2_surface = np.arange(0, 120, 20)\nx1_surface_pt, x2_surface_pt = np.meshgrid(x1_surface, x2_surface)\ny_surface = np.array(fun(np.ravel(x1_surface_pt), np.ravel(x2_surface_pt)))\nZ = y_surface.reshape(x1_surface_pt.shape)\n\nax.plot_surface(x1_surface_pt, x2_surface_pt, Z, color='y', alpha=0.2,\n                shade=False)\n\nax.set_xlabel('X1')\nax.set_ylabel('X2')\nax.set_zlabel('Y')\nplt.legend(fontsize=15)\nplt.show()\n</pre> fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(1, 1, 1, projection='3d')  ax.scatter(x1, x2, y, s=50, alpha=1, edgecolors='w', color='b', label='actual') ax.scatter(x1, x2, pred, s=50, alpha=0.8, edgecolors='w', color='r', label='predicted')  def fun(val1, val2):     return model.weights[0] * val1 + model.weights[1] * val2  x1_surface = x2_surface = np.arange(0, 120, 20) x1_surface_pt, x2_surface_pt = np.meshgrid(x1_surface, x2_surface) y_surface = np.array(fun(np.ravel(x1_surface_pt), np.ravel(x2_surface_pt))) Z = y_surface.reshape(x1_surface_pt.shape)  ax.plot_surface(x1_surface_pt, x2_surface_pt, Z, color='y', alpha=0.2,                 shade=False)  ax.set_xlabel('X1') ax.set_ylabel('X2') ax.set_zlabel('Y') plt.legend(fontsize=15) plt.show() <p>Pretty good!</p>"},{"location":"tech/ml/linear_regression/#implement-a-linear-regression-model-by-minimizing-sum-of-squared-errors","title":"Implement a linear regression model by minimizing sum of squared errors\u00b6","text":"<p>with friendly explanations</p>"},{"location":"tech/ml/linear_regression/#single-variable","title":"Single variable\u00b6","text":"<p>Let's start simple</p>"},{"location":"tech/ml/linear_regression/#solution","title":"Solution\u00b6","text":"<p>Suppose the real line is $y = ax + b$, then the sum of squared error is</p> <p>$(ax_1+b-y_1)^2 + (ax_2+b-y_2)^2... + (ax_n+b-y_n)^2$</p> <p>To minimize error w.r.t. $b$, take derivative w.r.t. it and set to 0:</p> <p>$2(ax_1+b-y_1) + 2(ax_2+b-y_2)... + 2(ax_n+b-y_n) = 0$, simplified to</p> <p>$$a(\\Sigma_1^nx_i) + bn - \\Sigma_1^ny_i = an\\bar{x} + bn - n\\bar{y} = 0$$ $$b = \\bar{y} - a\\bar{x}$$</p> <p>To minimize the error with respect to $a$, take derivative w.r.t $a$ and set to 0:</p> <p>$2x_1(ax_1+b-y_1) + 2x_2(ax_2+b-y_2) + ... 2x_n(ax_n + b - y_n) = 0$, simplified to</p> <p>$$a\\Sigma x_i^2 + b\\Sigma x_i - \\Sigma x_iy_i = 0$$</p> <p>Substituting solution for $y$, we get</p> <p>$$a\\Sigma x_i^2 + \\Sigma x_i(\\bar{y} - a\\bar{x}) - \\Sigma x_iy_i = 0$$ $$a(\\Sigma x_i^2 - n\\bar{x}^2) = \\Sigma x_iy_i - n\\bar{y}\\bar{x}$$ $$a = \\frac{\\Sigma x_iy_i - n\\bar{y}\\bar{x}}{\\Sigma x_i^2 - n\\bar{x}^2}$$</p> <p>This means we don't need to \"fit\", we can just calculate!</p>"},{"location":"tech/ml/linear_regression/#multivariant-linear-regression","title":"Multivariant linear regression\u00b6","text":"<p>What if we have more than one dimensions in x?</p>"},{"location":"tech/ml/perceptron_algorithm/","title":"Perceptron from scratch","text":"<pre><code>make a super naive network like this:\n\nx1---w1\n       \\\nx2---w2--weighted sum--activation function--&gt;y\n       /\n1----w3(bias)\n</code></pre> <p>where we calculate a weighted sum of the input and then use a step function to determine y like so: y = 1 if sum &gt; 0 else 0. The input has a bias term defined by 1*w3 to provide flexibility. Without the bias term, f([0,0]) will always be 0 regardless of the weights, which might not be desired.</p> In\u00a0[310]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\nx_samples = np.random.randint(0, 10, size=(50, 2))\ny_samples = np.random.randint(0, 2, size=50)\n\ndf = pd.DataFrame(x_samples)\ndf.columns = ['x1', 'x2']\ndf['y'] = y_samples\nprint(f\"positive samples: {y_samples.sum()}\")\ndf.head()\n</pre> import numpy as np import pandas as pd from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay import matplotlib.pyplot as plt  x_samples = np.random.randint(0, 10, size=(50, 2)) y_samples = np.random.randint(0, 2, size=50)  df = pd.DataFrame(x_samples) df.columns = ['x1', 'x2'] df['y'] = y_samples print(f\"positive samples: {y_samples.sum()}\") df.head() <pre>positive samples: 22\n</pre> Out[310]: x1 x2 y 0 3 9 0 1 6 1 0 2 1 4 1 3 4 7 0 4 4 3 0 In\u00a0[311]: Copied! <pre>class NaivePerceptron:\n    def __init__(self):\n        self.w = np.random.randn(3)\n\n    def activate(self, weighted_sum):\n        return 1 if weighted_sum &gt; 0 else 0\n\n    def weighted_sum(self, x):\n        with_bias = np.c_[x, np.ones((x.shape[0]))]\n        return np.matmul(with_bias, self.w.transpose())\n\n    def predict_single(self, x):\n        return self.activate(np.dot(x + [1], self.w))\n\n    def predict(self, x):\n        weighted_sums = self.weighted_sum(x)\n        return [self.activate(s) for s in weighted_sums]\n</pre> class NaivePerceptron:     def __init__(self):         self.w = np.random.randn(3)      def activate(self, weighted_sum):         return 1 if weighted_sum &gt; 0 else 0      def weighted_sum(self, x):         with_bias = np.c_[x, np.ones((x.shape[0]))]         return np.matmul(with_bias, self.w.transpose())      def predict_single(self, x):         return self.activate(np.dot(x + [1], self.w))      def predict(self, x):         weighted_sums = self.weighted_sum(x)         return [self.activate(s) for s in weighted_sums] In\u00a0[312]: Copied! <pre>p = NaivePerceptron()\np.predict_single([1,2])\n</pre> p = NaivePerceptron() p.predict_single([1,2]) Out[312]: <pre>1</pre> <p>In this naive version, the weights are generated randomly, so there's no reason to believe it can do any meaningful prediction or find any pattern. We can show ineffectiveness of the naive prception by running the algorithm N times and observe the poor confusion matrices</p> In\u00a0[313]: Copied! <pre>fig, ax = plt.subplots(3, 3, figsize=(10,10))\n\nfor i in range(9):\n    p = NaivePerceptron()\n    predicted = p.predict(x_samples)\n    df['predicted'] = predicted\n    stats = confusion_matrix(y_samples, predicted)\n    \n    plot = ConfusionMatrixDisplay(confusion_matrix=stats)\n    plot.plot(ax=ax[int(i/3)][i%3])\nplt.show()\n</pre> fig, ax = plt.subplots(3, 3, figsize=(10,10))  for i in range(9):     p = NaivePerceptron()     predicted = p.predict(x_samples)     df['predicted'] = predicted     stats = confusion_matrix(y_samples, predicted)          plot = ConfusionMatrixDisplay(confusion_matrix=stats)     plot.plot(ax=ax[int(i/3)][i%3]) plt.show() In\u00a0[314]: Copied! <pre>class SmartPerceptron(NaivePerceptron):\n    def __init__(self, learning_rate=0.1):\n        self.learning_rate = learning_rate\n        super().__init__()\n\n    def fit(self, x, y, epochs, verbose=False):\n        for i in range(epochs):\n            total_err = 0\n            for x_i, y_i in zip(x, y):\n                total_err += abs(self.fit_single(x_i, y_i))\n            if verbose:\n                print(f\"error for epoch {i+1}: {total_err}\")\n\n    def fit_single(self, x, y):\n        pred = self.predict_single(x.tolist())\n        err = y - pred\n        x_with_bias = np.array(x.tolist() + [1])\n        self.w += self.learning_rate * err * x_with_bias\n        return err\n</pre> class SmartPerceptron(NaivePerceptron):     def __init__(self, learning_rate=0.1):         self.learning_rate = learning_rate         super().__init__()      def fit(self, x, y, epochs, verbose=False):         for i in range(epochs):             total_err = 0             for x_i, y_i in zip(x, y):                 total_err += abs(self.fit_single(x_i, y_i))             if verbose:                 print(f\"error for epoch {i+1}: {total_err}\")      def fit_single(self, x, y):         pred = self.predict_single(x.tolist())         err = y - pred         x_with_bias = np.array(x.tolist() + [1])         self.w += self.learning_rate * err * x_with_bias         return err <p>In the fit_single method, we move w by <code>self.w += self.learning_rate * err * x_with_bias</code>. Inuitively, the direction in which we adjust the weight is determined by whether we over- or under-estimate y. The magnitude of adjustment depends on the learning rate and the value of x. Why do we need to scale it by x? Think of it this way: if we are below the activation threshold even when x is large, then we need to increase w a lot to move the needle.</p> <p>Check the error at each epoch. We expect it to roughly reduce at each epoch.</p> In\u00a0[315]: Copied! <pre>p = SmartPerceptron()\np.fit(x_samples, y_samples, 10, verbose=True)\npred = p.predict(x_samples)\n</pre> p = SmartPerceptron() p.fit(x_samples, y_samples, 10, verbose=True) pred = p.predict(x_samples) <pre>error for epoch 1: 24\nerror for epoch 2: 20\nerror for epoch 3: 21\nerror for epoch 4: 21\nerror for epoch 5: 25\nerror for epoch 6: 21\nerror for epoch 7: 21\nerror for epoch 8: 26\nerror for epoch 9: 23\nerror for epoch 10: 26\n</pre> <p>Now check how the smart perceptron performs. For each perceptron, we look at the final results after trianing for 20 epoches.</p> In\u00a0[317]: Copied! <pre>fig, ax = plt.subplots(3, 3, figsize=(10,10))\n\nfor i in range(9):\n    p = SmartPerceptron()\n    p.fit(x_samples, y_samples, 5)\n    predicted = p.predict(x_samples)\n    df['predicted'] = predicted\n    stats = confusion_matrix(y_samples, predicted)\n    \n    plot = ConfusionMatrixDisplay(confusion_matrix=stats)\n    plot.plot(ax=ax[int(i/3)][i%3])\nplt.show()\n</pre> fig, ax = plt.subplots(3, 3, figsize=(10,10))  for i in range(9):     p = SmartPerceptron()     p.fit(x_samples, y_samples, 5)     predicted = p.predict(x_samples)     df['predicted'] = predicted     stats = confusion_matrix(y_samples, predicted)          plot = ConfusionMatrixDisplay(confusion_matrix=stats)     plot.plot(ax=ax[int(i/3)][i%3]) plt.show() <p>Interestingly, all converge to a naive approach of just assigning the same label for all samples! This is due to the randomness of the samples in the first place and the fact that this is a super small network. It's already doing the best it could.</p> <p>What if we artificially generate a set of samples that actually have a clear and simple class boundary? Let's generate some samples under the rules by the perceptron algorithm, based on w and activation function. Execute below until we get a relatively unbiased set of samples, e.g. not all samples are positive.</p> In\u00a0[338]: Copied! <pre>x_samples_smart = np.random.randint(0, 10, size=(50, 2))\n\nw = np.random.randn(3)\n\nwith_bias = np.c_[x_samples_smart, np.ones((x_samples_smart.shape[0]))]\nweighted_sum = np.matmul(with_bias, w.transpose())\n\ndef activate(s):\n    return 1 if s &gt; 0 else 0\n\ny_samples_smart = [activate(s) for s in weighted_sum]\nprint(f\"positive samples: {np.array(y_samples_smart).sum()}\")\n</pre> x_samples_smart = np.random.randint(0, 10, size=(50, 2))  w = np.random.randn(3)  with_bias = np.c_[x_samples_smart, np.ones((x_samples_smart.shape[0]))] weighted_sum = np.matmul(with_bias, w.transpose())  def activate(s):     return 1 if s &gt; 0 else 0  y_samples_smart = [activate(s) for s in weighted_sum] print(f\"positive samples: {np.array(y_samples_smart).sum()}\") <pre>positive samples: 32\n</pre> <p>As before, see what happens with untrained perceptron. It does pretty bad!</p> In\u00a0[339]: Copied! <pre>fig, ax = plt.subplots(3, 3, figsize=(10,10))\n\nfor i in range(9):\n    p = NaivePerceptron()\n    predicted = p.predict(x_samples_smart)\n    df['predicted'] = predicted\n    stats = confusion_matrix(y_samples_smart, predicted)\n    plot = ConfusionMatrixDisplay(confusion_matrix=stats)\n    plot.plot(ax=ax[int(i/3)][i%3])\nplt.show()\n</pre> fig, ax = plt.subplots(3, 3, figsize=(10,10))  for i in range(9):     p = NaivePerceptron()     predicted = p.predict(x_samples_smart)     df['predicted'] = predicted     stats = confusion_matrix(y_samples_smart, predicted)     plot = ConfusionMatrixDisplay(confusion_matrix=stats)     plot.plot(ax=ax[int(i/3)][i%3]) plt.show() <p>With training, it does fairly well with 30 epochs.</p> In\u00a0[340]: Copied! <pre>fig, ax = plt.subplots(3, 3, figsize=(10,10))\n\nfor i in range(9):\n    p = SmartPerceptron()\n    p.fit(x_samples_smart, y_samples_smart, 30)\n    predicted = p.predict(x_samples_smart)\n    df['predicted'] = predicted\n    stats = confusion_matrix(y_samples_smart, predicted)\n    \n    plot = ConfusionMatrixDisplay(confusion_matrix=stats)\n    plot.plot(ax=ax[int(i/3)][i%3])\nplt.show()\n</pre> fig, ax = plt.subplots(3, 3, figsize=(10,10))  for i in range(9):     p = SmartPerceptron()     p.fit(x_samples_smart, y_samples_smart, 30)     predicted = p.predict(x_samples_smart)     df['predicted'] = predicted     stats = confusion_matrix(y_samples_smart, predicted)          plot = ConfusionMatrixDisplay(confusion_matrix=stats)     plot.plot(ax=ax[int(i/3)][i%3]) plt.show() <p>This concludes perceptron, the foundational unit of neural network</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tech/ml/perceptron_algorithm/#perceptron-from-scratch","title":"Perceptron from scratch\u00b6","text":"<p>Perceptron is the simplest form of artificial neural network used for binary classifcation. In this notebook we'll implement one from scratch. This can help us understand the foundations of neural network.</p>"},{"location":"tech/ml/perceptron_algorithm/#problem-to-be-solved","title":"Problem to be solved\u00b6","text":"<p>Create an artificial data set where we want to predict the binary classification y from samples x generated randomly. (Note that this is a random problem by itself so we don't expected a perfect fit).</p>"},{"location":"tech/ml/perceptron_algorithm/#implement-a-naive-perceptron","title":"Implement a naive perceptron\u00b6","text":"<p>weighted sum is simply $\\Sigma_{i} x_i w_i$, the dot product of input x and weight vector w. When taking an array of inputs, we can use marix multiplication to generate weighted sums for all samples.</p>"},{"location":"tech/ml/perceptron_algorithm/#add-training-methods-to-the-perceptron","title":"Add training methods to the perceptron\u00b6","text":"<p>To improve the perceptron, we want it to adjust its weights while going through training samples one by one. We need to add a learning rate to determine how fast we go with gradient descent.</p> <p>To do gradient descent, we first calculate the difference between predicted and expected classification. This can only be 0, -1, or 1 and essentially means a direction rather than a magnitude. The magnitude is determined by x itself, tuned by the learning rate.</p> <p>To gain an intuition, suppose y=1 and prediction=0, this means the weighted sum is below the activation threshold. w_i needs to be bigger whenever x_i is positive and smaller whenever x_i is negative. So if we set the error term as y_true - y_pred, then we want to move towards the direction of this term.</p>"},{"location":"tech/ml/perceptron_algorithm/#try-a-different-set-of-samples","title":"Try a different set of samples\u00b6","text":""},{"location":"tech/ml/random_forest/","title":"Implement random forest from scratch, start from decision treee","text":"In\u00a0[1]: Copied! <pre>import math\nimport numpy as np\nimport pandas as pd\n\ndef entropy(Y):\n    \"\"\"\n    sum of p0 log2(p0) + p1 log2(p1)\n    \"\"\"\n    total_ent = 0\n    size = len(Y)\n    p = sum([y == 1 for y in Y]) / size\n    if p == 0 or p == 1:\n        return 0\n    else:\n        return -p * math.log(p, 2) - (1 - p) * math.log(1 - p, 2)\n</pre> import math import numpy as np import pandas as pd  def entropy(Y):     \"\"\"     sum of p0 log2(p0) + p1 log2(p1)     \"\"\"     total_ent = 0     size = len(Y)     p = sum([y == 1 for y in Y]) / size     if p == 0 or p == 1:         return 0     else:         return -p * math.log(p, 2) - (1 - p) * math.log(1 - p, 2) <p>Sanity check entropy calculation. It should be like a bell</p> In\u00a0[3]: Copied! <pre>import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nsamples = [[0] * x + [1] * (100 - x) for x in range(100)]\nproportions = [sum(s) / 100 for s in samples]\nentropies = [entropy(s) for s in samples]\nplt.plot()\nplt.scatter(proportions, entropies)\nplt.xlabel(\"Proportion of class 1\", fontsize=15)\nplt.ylabel(\"Entropy\", fontsize=15)\nplt.show()\n</pre> import matplotlib.pyplot as plt import seaborn as sns sns.set_style('darkgrid')  samples = [[0] * x + [1] * (100 - x) for x in range(100)] proportions = [sum(s) / 100 for s in samples] entropies = [entropy(s) for s in samples] plt.plot() plt.scatter(proportions, entropies) plt.xlabel(\"Proportion of class 1\", fontsize=15) plt.ylabel(\"Entropy\", fontsize=15) plt.show() <p>An example of how we can use entropy reduction to guide spliting</p> In\u00a0[4]: Copied! <pre>before = [0] * 10 + [1] * 10\nprint(\"Entropy before splitting:\", entropy(before))\n\nsplit1 = [0] * 8 + [1] *2\nsplit2 = [0] * 2 + [1] * 8\nentropy_good = entropy(split1) * len(split1) / len(before) \\\n    + entropy(split2) * len(split2) / len(before)\nprint(\"Entropy after a good split\", entropy_good)\n\nsplit1 = [0] * 6 + [1] *4\nsplit2 = [0] * 4 + [1] * 6\nentropy_bad = entropy(split1) * len(split1) / len(before) \\\n    + entropy(split2) * len(split2) / len(before)\nprint(\"Entropy after a bad split\", entropy_bad)\n</pre> before = [0] * 10 + [1] * 10 print(\"Entropy before splitting:\", entropy(before))  split1 = [0] * 8 + [1] *2 split2 = [0] * 2 + [1] * 8 entropy_good = entropy(split1) * len(split1) / len(before) \\     + entropy(split2) * len(split2) / len(before) print(\"Entropy after a good split\", entropy_good)  split1 = [0] * 6 + [1] *4 split2 = [0] * 4 + [1] * 6 entropy_bad = entropy(split1) * len(split1) / len(before) \\     + entropy(split2) * len(split2) / len(before) print(\"Entropy after a bad split\", entropy_bad) <pre>Entropy before splitting: 1.0\nEntropy after a good split 0.7219280948873623\nEntropy after a bad split 0.9709505944546686\n</pre> <p>For any given feature, we want to find the split where entropy reduction is maximized. Iterate through all possible splitting point to do so. To find the best feature to split first, do an exaustive search for all splitting points of all features.</p> <p>First implement a method to find a optimal splitting point for one feature</p> In\u00a0[5]: Copied! <pre>def split_entropy(values, labels):\n    zipped = list(zip(values, labels))\n    zipped.sort(key=lambda x: x[0])\n    best_split, best_entr = values[0], entropy(labels)\n    size = len(labels)\n    for point in zipped:\n        # split samples based on split value\n        small_points = [p[1] for p in zipped if p[0] &lt;= point[0]]\n        large_points = [p[1] for p in zipped if p[0] &gt; point[0]]\n        if not small_points or not large_points:\n            continue\n        # calculate new entropy\n        sum_entropy = entropy(small_points) * len(small_points) / size\\\n            + entropy(large_points) * len(large_points) / size\n        # update best entropy if necessary\n        if sum_entropy &lt; best_entr:\n            best_split, best_entr = point[0], sum_entropy\n    return best_split, best_entr\n\n# Example\nsplit_entropy([1, 2, 3, 4], [0, 0, 0, 0])\n</pre> def split_entropy(values, labels):     zipped = list(zip(values, labels))     zipped.sort(key=lambda x: x[0])     best_split, best_entr = values[0], entropy(labels)     size = len(labels)     for point in zipped:         # split samples based on split value         small_points = [p[1] for p in zipped if p[0] &lt;= point[0]]         large_points = [p[1] for p in zipped if p[0] &gt; point[0]]         if not small_points or not large_points:             continue         # calculate new entropy         sum_entropy = entropy(small_points) * len(small_points) / size\\             + entropy(large_points) * len(large_points) / size         # update best entropy if necessary         if sum_entropy &lt; best_entr:             best_split, best_entr = point[0], sum_entropy     return best_split, best_entr  # Example split_entropy([1, 2, 3, 4], [0, 0, 0, 0]) Out[5]: <pre>(1, 0)</pre> In\u00a0[6]: Copied! <pre>class Node:\n    def __init__(self):\n        self.feature = None\n        self.split = None\n        # left/right is either a node or a value. If it's a value then this node\n        # is a leaf\n        self.left = None\n        self.right = None\n    def move(self, val):\n        \"\"\"Move to left or right by comparing sample value to the splitting point\"\"\"\n        if val &lt;= self.split:\n            return self.left\n        else:\n            return self.right\n    def height(self):\n        \"\"\"Get the height of the tree rooted at this node\"\"\"\n        if isinstance(self.left, float):\n            return 1\n        else:\n            return 1 + max([self.left.height(), self.right.height()])\n</pre> class Node:     def __init__(self):         self.feature = None         self.split = None         # left/right is either a node or a value. If it's a value then this node         # is a leaf         self.left = None         self.right = None     def move(self, val):         \"\"\"Move to left or right by comparing sample value to the splitting point\"\"\"         if val &lt;= self.split:             return self.left         else:             return self.right     def height(self):         \"\"\"Get the height of the tree rooted at this node\"\"\"         if isinstance(self.left, float):             return 1         else:             return 1 + max([self.left.height(), self.right.height()]) In\u00a0[7]: Copied! <pre>class DecisionTree:\n    def __init__(self):\n        self.root = None\n\n    def fit(self, df, features, target):\n        self.root = self._make_node(df, features, target)\n\n    def predict(self, df):\n        # transverse the tree for each data point\n        pred = df.apply(lambda row: self._transverse(self.root, row), axis=1)\n        return pred\n\n    def _best_feature(self, df, features, target):\n        \"\"\"\n        return the best feature given a set of feature values and a target.\n        Store everything in a dataframe and let user specify which feature\n        columns to use and which columns corresponds to the binary target\n        \"\"\"\n        best_split, best_entro, best_feature = None, None, None\n        # Try all features. At each iteration update the best option.\n        for feature in features:\n            this_split, this_entro = split_entropy(df[feature].tolist(), df[target])\n            if best_split is None or best_entro &gt; this_entro:\n                best_split, best_entro, best_feature = this_split, this_entro, feature\n        return best_split, best_feature\n\n    def _make_node(self, df, features, target):\n        node = Node()\n        # Use the best splitting value of the best feature out of all.\n        node.split, node.feature = self._best_feature(df, features, target)\n        # Divide the samples into two subsets and use them to continue\n        # subtree construction\n        small_points = df[df[node.feature] &lt;= node.split]\n        large_points = df[df[node.feature] &gt; node.split]\n        # if No more split, return most frequent target value\n        if small_points.shape[0] == 0 or large_points.shape[0] == 0:\n            node.left = float(df[target].mode().iloc[0])\n            node.right = float(df[target].mode().iloc[0])\n        else:\n            node.left = self._make_node(small_points, features, target)\n            node.right = self._make_node(large_points, features, target)\n        return node\n\n    def _walk_node(self, node, row):\n        # Go to the next decision point or leaf\n        return node.move(row[node.feature])\n\n    def _transverse(self, node, row):\n        # Transverse nodes until a target (leaf) value is reached\n        next_location = self._walk_node(node, row)\n        while not isinstance(next_location, float):\n            next_location = self._walk_node(next_location, row)\n        return next_location\n\n    def depth(self):\n        return self.root.height()\n</pre> class DecisionTree:     def __init__(self):         self.root = None      def fit(self, df, features, target):         self.root = self._make_node(df, features, target)      def predict(self, df):         # transverse the tree for each data point         pred = df.apply(lambda row: self._transverse(self.root, row), axis=1)         return pred      def _best_feature(self, df, features, target):         \"\"\"         return the best feature given a set of feature values and a target.         Store everything in a dataframe and let user specify which feature         columns to use and which columns corresponds to the binary target         \"\"\"         best_split, best_entro, best_feature = None, None, None         # Try all features. At each iteration update the best option.         for feature in features:             this_split, this_entro = split_entropy(df[feature].tolist(), df[target])             if best_split is None or best_entro &gt; this_entro:                 best_split, best_entro, best_feature = this_split, this_entro, feature         return best_split, best_feature      def _make_node(self, df, features, target):         node = Node()         # Use the best splitting value of the best feature out of all.         node.split, node.feature = self._best_feature(df, features, target)         # Divide the samples into two subsets and use them to continue         # subtree construction         small_points = df[df[node.feature] &lt;= node.split]         large_points = df[df[node.feature] &gt; node.split]         # if No more split, return most frequent target value         if small_points.shape[0] == 0 or large_points.shape[0] == 0:             node.left = float(df[target].mode().iloc[0])             node.right = float(df[target].mode().iloc[0])         else:             node.left = self._make_node(small_points, features, target)             node.right = self._make_node(large_points, features, target)         return node      def _walk_node(self, node, row):         # Go to the next decision point or leaf         return node.move(row[node.feature])      def _transverse(self, node, row):         # Transverse nodes until a target (leaf) value is reached         next_location = self._walk_node(node, row)         while not isinstance(next_location, float):             next_location = self._walk_node(next_location, row)         return next_location      def depth(self):         return self.root.height() In\u00a0[8]: Copied! <pre>from sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import accuracy_score\n\ndata = load_breast_cancer()\ndf = pd.DataFrame(data['data'])\ncolumns = [c for c in df.columns]\ndf['target'] = data['target']\n# Count values to make sure accuracy makes sense (balanced class)\ndf['target'].value_counts()\n</pre> from sklearn.datasets import load_breast_cancer from sklearn.metrics import accuracy_score  data = load_breast_cancer() df = pd.DataFrame(data['data']) columns = [c for c in df.columns] df['target'] = data['target'] # Count values to make sure accuracy makes sense (balanced class) df['target'].value_counts() Out[8]: <pre>1    357\n0    212\nName: target, dtype: int64</pre> <p>Fit the tree with the entire sample. This should create a HUGE tree that is 100% overfit and gives 100% accuracy, since the tree keeps making nodes until all samples are settled into homogeneous leaves.</p> In\u00a0[9]: Copied! <pre>tree = DecisionTree()\ntree.fit(df, features=columns, target='target')\n</pre> tree = DecisionTree() tree.fit(df, features=columns, target='target') In\u00a0[10]: Copied! <pre>pred = tree.predict(df)\nacc = accuracy_score(y_pred=pred, y_true=df['target'])\nprint(\"Accuracy is\", acc)\nprint(\"Tree depth is\", tree.depth())\n</pre> pred = tree.predict(df) acc = accuracy_score(y_pred=pred, y_true=df['target']) print(\"Accuracy is\", acc) print(\"Tree depth is\", tree.depth()) <pre>Accuracy is 1.0\nTree depth is 11\n</pre> <p>Fit the tree with a subset of samples so that we can set aside test data. Compare performance on test vs training data to confirm overfitting.</p> In\u00a0[11]: Copied! <pre>tree = DecisionTree()\ntrain = df.head(200)\ntest = df.tail(300)\ntree.fit(train, features=columns, target='target')\npred = tree.predict(test)\nacc = accuracy_score(y_pred=pred, y_true=test['target'])\nprint(\"Accuracy is\", acc)\nprint(\"Tree depth is\", tree.depth())\n</pre> tree = DecisionTree() train = df.head(200) test = df.tail(300) tree.fit(train, features=columns, target='target') pred = tree.predict(test) acc = accuracy_score(y_pred=pred, y_true=test['target']) print(\"Accuracy is\", acc) print(\"Tree depth is\", tree.depth()) <pre>Accuracy is 0.83\nTree depth is 10\n</pre> In\u00a0[12]: Copied! <pre>class SimpleForest:\n    def __init__(self, n_trees=100):\n        self.trees = [DecisionTree() for _ in range(n_trees)]\n    \n    def fit(self, df, features, target):\n        for tree in self.trees:\n            subset = df.sample(frac=0.66)\n            sub_features = np.random.choice(features, int(2 * len(features) / 3))\n            tree.fit(subset, sub_features, target)\n    \n    def predict(self, df):\n        pred_list = [tree.predict(df) for tree in self.trees]\n        combined = pd.concat(pred_list, axis=1)\n        return combined.mode(axis=1).iloc[:, 0].tolist()\n</pre> class SimpleForest:     def __init__(self, n_trees=100):         self.trees = [DecisionTree() for _ in range(n_trees)]          def fit(self, df, features, target):         for tree in self.trees:             subset = df.sample(frac=0.66)             sub_features = np.random.choice(features, int(2 * len(features) / 3))             tree.fit(subset, sub_features, target)          def predict(self, df):         pred_list = [tree.predict(df) for tree in self.trees]         combined = pd.concat(pred_list, axis=1)         return combined.mode(axis=1).iloc[:, 0].tolist() <p>The following will take a few minutes due to inefficient implementation</p> In\u00a0[13]: Copied! <pre>forest = SimpleForest()\nforest.fit(train, features=columns, target='target')\n</pre> forest = SimpleForest() forest.fit(train, features=columns, target='target') In\u00a0[14]: Copied! <pre>pred = forest.predict(test)\n</pre> pred = forest.predict(test) In\u00a0[15]: Copied! <pre>pred = forest.predict(train)\nacc = accuracy_score(y_pred=pred, y_true=train['target'])\n\npred_tree = tree.predict(train)\ntree_acc = accuracy_score(y_pred=pred_tree, y_true=train['target'])\n\nprint(\"Training accuracy of random forest:\", acc)\nprint(\"Training accuracy of decision tree:\", tree_acc)\n\npred = forest.predict(test)\nacc = accuracy_score(y_pred=pred, y_true=test['target'])\n\npred_tree = tree.predict(test)\ntree_acc = accuracy_score(y_pred=pred_tree, y_true=test['target'])\n\nprint(\"Test accuracy of random forest:\", acc)\nprint(\"Test accuracy of decision tree:\", tree_acc)\n</pre> pred = forest.predict(train) acc = accuracy_score(y_pred=pred, y_true=train['target'])  pred_tree = tree.predict(train) tree_acc = accuracy_score(y_pred=pred_tree, y_true=train['target'])  print(\"Training accuracy of random forest:\", acc) print(\"Training accuracy of decision tree:\", tree_acc)  pred = forest.predict(test) acc = accuracy_score(y_pred=pred, y_true=test['target'])  pred_tree = tree.predict(test) tree_acc = accuracy_score(y_pred=pred_tree, y_true=test['target'])  print(\"Test accuracy of random forest:\", acc) print(\"Test accuracy of decision tree:\", tree_acc) <pre>Training accuracy of random forest: 1.0\nTraining accuracy of decision tree: 1.0\nTest accuracy of random forest: 0.93\nTest accuracy of decision tree: 0.83\n</pre> <p>This data set is simple enough that random forest manages to get 100% accuracy on training samples and is still robust against unseen test samples. In most cases, it does not achieve 100% training accuracy, but that's not a problem and probably a good thing.</p> <p>Optimizations of decision tree can be used to random forest as well. Besides, random forest has additional optimization such as</p> <ul> <li>Number of trees</li> <li>Number of features to be sampled at each tree</li> <li>How sampling is done for fitting each tree</li> <li>Voting method at prediction time</li> <li>Adding different weights to trees</li> <li>More...</li> </ul>"},{"location":"tech/ml/random_forest/#implement-random-forest-from-scratch-start-from-decision-treee","title":"Implement random forest from scratch, start from decision treee\u00b6","text":""},{"location":"tech/ml/random_forest/#decision-tree","title":"Decision tree\u00b6","text":"<p>Decision tree is a machine learning method that makes sequential binary choices at each step based on one feature value per step, until a final value is assigned to the target variable.</p> <p>A decision tree on whether to date or not</p> <pre><code>        weather\n        /     \\\n      cold   hot\n      /         \\\n    location   time\n    /  \\        / \\\n   in  out  early  late\n  /      \\    /     \\\n Yes     No  No    Yes\n</code></pre> <p>It's called decision tree because one needs to transverse through a tree to make a final decision. Machine model comes in when figuring out how the tree should be constructed, i.e. what features to use and what values are used to split at each features.</p>"},{"location":"tech/ml/random_forest/#entropy-where-to-split","title":"Entropy &amp; Where to split?\u00b6","text":"<p>At each step, ideally we split a heterogeneous pool into two homogeneous pools. To get close to that, we aim to reduce total entropy. This measure is also called information gain. We pick the value such that information gain is maximized when split optimally.</p> <p>Assuming binary classification, entropy is $$Entropy = -p_0\\log_2 p_0 - p_1\\log_2 p_1$$ Where $p_0, p_1$ is the proprotion of samples in the two classes respectively.</p>"},{"location":"tech/ml/random_forest/#tree-nodes","title":"Tree Nodes\u00b6","text":"<p>We use nodes to represent each decision point. The decision to branch left or right is determined by the feature value of the sample. For each node, we need to know which feature it's associated with and what value to use as the splitting point. For analytic purpose later, also make a <code>height</code> method to get height of the subtree rooted at the node.</p>"},{"location":"tech/ml/random_forest/#decision-tree","title":"Decision Tree\u00b6","text":"<p>A basic ID3 decision tree model is no more than a wrapper class that holds a tree. During training phase, it deterministically constructs nodes one by one, picking the best splitting value of the best feature at each iteration. This process is greedy and exaustive. To make a prediction, it transverse through the entire tree based on samples' values at each feature, until a leaf is reached.</p>"},{"location":"tech/ml/random_forest/#plant-some-trees","title":"Plant some trees\u00b6","text":"<p>Use above simple tree to fit breast cancer data, a binary classification problem</p>"},{"location":"tech/ml/random_forest/#overcome-overfitting","title":"Overcome overfitting\u00b6","text":"<p>There are many ways to avoid overfitting a tree, such as</p> <ul> <li>Limit tree depth</li> <li>Stop node construction once there are fewer than X number of samples in either split</li> <li>Prune the tree later by removing leaves that do not contribute to prediction much</li> <li>Use a subset of features</li> <li>Fit on a subset of samples</li> <li>Set a lower bound of information gain to continue node construction</li> <li>Many more!</li> </ul> <p>Those are all trade-offs between bias and variation to avoid fitting on errors in the samples. Here's where a technique called <code>random forest</code> comes in. The general idea is that by creating a forest of trees and all of them to make prediction, then we mitigate overfitting. This is an ensemble method, where multiple estimators collectively make a decision on the outcome.</p>"},{"location":"tech/ml/random_forest/#a-simple-random-forest-model","title":"A Simple Random Forest Model\u00b6","text":"<p>To demonstrate that random forest enhances the power of decision tree, let's implement a classifier with 100 trees. Instead of fitting all samples with all features, each tree is fed with 66% of traning samples and 2/3 of the features. See what happens</p>"},{"location":"tech/ml/recurrent_neural_net/","title":"Recurrent Neural Net From Scratch","text":"<p>I will skip the reasoning behind why we need RNN. There's plenty on the internet. I'll focus on learning the technical side, and hope that the answers to \"why\"s will come naturally. This is the same principal as other notebooks in my \"from scratch\" series. This is heavily based on the basic neural net, so read that one first.</p> <p>Recall that in a neural network, we have:</p> <p></p> <p>For recurrent neural network, we carry over info from the previous data point in the sequence, by incorporating the values of the hidden layer, with its own weight $u$.</p> <p></p> <p>$$ \\large \\frac{\\partial L_t}{\\partial V} = (\\hat{y_t} - y_t) \\cdot a_{h,t} $$</p> <p>Replace $a_h$ with 1 for the bias term</p> <p>From above setup, we can see that $s_{h,t}$ is an important term as it's a function of $W, U, a_{h,t-1}$. In backpropagation, the gradient will flow from $s_{h,t}$ to those components like the following, based on the formula in the feedforward section above:</p> <p>$$\\large \\begin{align} \\frac{\\partial L_t}{\\partial W} &amp;= \\frac{\\partial L_t}{\\partial s_{h,t}} \\cdot x_t \\\\ \\frac{\\partial L_t}{\\partial U} &amp;= \\frac{\\partial L_t}{\\partial s_{h,t}} \\cdot a_{h,t-1} \\\\ \\frac{\\partial L_t}{\\partial b_h} &amp;= \\frac{\\partial L_t}{\\partial s_{h,t}}  \\end{align} $$</p> <p>To get $\\Large \\frac{\\partial L_t}{\\partial s_{h,t}}$, we have</p> <p>$$\\large \\begin{align} \\frac{\\partial L_t}{\\partial s_{h,t}} &amp; = \\frac{\\partial L_t}{\\partial a_{h,t}}\\cdot \\frac{\\partial a_{h,t}}{\\partial s_{h,t}} \\\\ &amp; = \\frac{\\partial L_t}{\\partial a_{h,t}}\\cdot \\frac{\\partial \\sigma_h(s_{h,t})}{\\partial s_{h,t}} \\\\ &amp; = \\frac{\\partial L_t}{\\partial a_{h,t}}\\cdot (1-(tanh(s_{h,t}))^2)  \\end{align} $$</p> <p>All we have left to derive is $\\Large \\frac{\\partial L_t}{\\partial a_{h,t}}$. As $\\large a_{h,t}$ contributes to $L_{t+1}$, the gradient from $L_{t+1}$ also goes to $\\large a_{h,t}$. Therefore</p> <p>$$\\large \\begin{align} \\frac{\\partial L_t}{\\partial a_{h,t}} &amp;= \\frac{\\partial L_t}{\\partial s_{o,t}} \\cdot \\frac{\\partial s_{o,t}}{\\partial a_{h,t}} + \\frac{\\partial L_{t+1}}{\\partial s_{h,t+1}} \\cdot \\frac{\\partial s_{h,t+1}}{\\partial a_{h,t}} \\\\ &amp;= (\\hat{y_t} - y_t)\\cdot V + \\frac{\\partial L_{t+1}}{\\partial s_{h,t+1}} \\cdot U \\end{align} $$</p> In\u00a0[1]: Copied! <pre>import numpy as np\n\n# length of the sequence is the length of the problem. Keep it not too big for calculation, but not too small to be boring.\nlength = 50\n\nx_num = [np.random.randint(0, 3) for i in range(length)]\ny_num = [(x+1) % 3 for x in x_num]\nx_str = ''.join([str(i) for i in x_num])\ny_str = ''.join([str(i) for i in y_num])\nprint('input: ', x_str)\nprint('output:', y_str)\n</pre> import numpy as np  # length of the sequence is the length of the problem. Keep it not too big for calculation, but not too small to be boring. length = 50  x_num = [np.random.randint(0, 3) for i in range(length)] y_num = [(x+1) % 3 for x in x_num] x_str = ''.join([str(i) for i in x_num]) y_str = ''.join([str(i) for i in y_num]) print('input: ', x_str) print('output:', y_str) <pre>input:  11220020221211221211112201122011202120001210122112\noutput: 22001101002022002022220012200122010201112021200220\n</pre> <p>Repeat to make a dataset</p> In\u00a0[2]: Copied! <pre>def num_to_label(y, vocab=3):\n    # turn a single digit label to multiclass labels\n    labels = np.zeros((len(y), vocab), dtype=int)\n    for i, yy in enumerate(y):\n        labels[i, yy] = 1\n    return labels\n\ndef make_pairs(length=10, vocab=3):\n    x_num = [np.random.randint(0, vocab) for i in range(length)]\n    y_num = [(x+1) % vocab for x in x_num]\n    # One hot encode the input and output\n    x = num_to_label(x_num)\n    y = num_to_label(y_num)\n    return x, y\n</pre> def num_to_label(y, vocab=3):     # turn a single digit label to multiclass labels     labels = np.zeros((len(y), vocab), dtype=int)     for i, yy in enumerate(y):         labels[i, yy] = 1     return labels  def make_pairs(length=10, vocab=3):     x_num = [np.random.randint(0, vocab) for i in range(length)]     y_num = [(x+1) % vocab for x in x_num]     # One hot encode the input and output     x = num_to_label(x_num)     y = num_to_label(y_num)     return x, y In\u00a0[3]: Copied! <pre>size = 10\nx, y = [], []\nfor i in range(size):\n    xx, yy = make_pairs()\n    x.append(xx)\n    y.append(yy)\n</pre> size = 10 x, y = [], [] for i in range(size):     xx, yy = make_pairs()     x.append(xx)     y.append(yy) <p>Different from basic neural net, RNN does feed forward and backpropagation for the entire sequence, not at each step independently. First let's layout the components we need. This should give a rough breakdown to help writing the code.</p> In\u00a0[4]: Copied! <pre>class RNN():\n    def __init__(self):\n        # initialize a random set of weigths and biases\n        pass\n\n    def predict(self):\n        # this is the feed forward process\n        pass\n\n    def backpropagation(self):\n        # one round of backpropagation through the entire sequence\n        pass\n\n    def train(self, epochs):\n        pass\n\n    def total_loss(self):\n        pass\n\n    def loss(self, t):\n        return \n\n    def tanh(x):\n        pass\n\n    def softmax(x):\n        pass\n</pre> class RNN():     def __init__(self):         # initialize a random set of weigths and biases         pass      def predict(self):         # this is the feed forward process         pass      def backpropagation(self):         # one round of backpropagation through the entire sequence         pass      def train(self, epochs):         pass      def total_loss(self):         pass      def loss(self, t):         return       def tanh(x):         pass      def softmax(x):         pass <p>Then fill in the blanks. We'll first enable feed forward functions. In the init function, we not only need to initialize the parameters, but also reserve vectors to memorize all the calculations, as we'll rely on them in both feed forward and BTT. As we see above, terms like $\\Large \\frac{\\partial L_t}{\\partial s_{h,t}}$ are used multiple times and in different epochs.</p> In\u00a0[31]: Copied! <pre>import numpy as np\n\nclass RNN1():\n    def __init__(self, n_nodes=10, vocab_size=3):\n        # initialize a random set of weigths and biases\n        self.n_nodes = n_nodes\n        self.w = np.random.randn(n_nodes, vocab_size)\n        self.u = np.random.randn(n_nodes, n_nodes)\n        self.v = np.random.randn(vocab_size,n_nodes)\n        self.b_h = np.random.randn(n_nodes)\n        self.b_o = np.random.randn(vocab_size)\n        self.s_h = []\n        self.a_h = []\n        self.s_o = []\n        self.y_hat = []\n\n    def predict(self, x):\n        return [self.predict_single(xx) for xx in x]\n\n    def predict_scores(self, x):\n        return [self.feedforward(xx) for xx in x]\n    \n    def predict_single(self, x):\n        scores = self.feedforward(x)\n        return self.score_to_num(scores)\n\n    def reset_mem(self):\n        self.s_h = []\n        self.a_h = []\n        self.s_o = []\n        self.y_hat = []\n    \n    def feedforward(self, x):\n        self.reset_mem()\n        # this is the feed forward process\n        for t in range(len(x)):\n            # this follows straight from the formula\n            a_ht_prev = np.zeros(self.n_nodes) if t == 0 else self.a_h[t-1]\n            s_ht = x[t] @ self.w.T + a_ht_prev @ self.u + self.b_h\n            a_ht = self.tanh(s_ht)\n            s_ot = a_ht @ self.v.T + self.b_o\n            y_t = self.softmax(s_ot)\n\n            # memorize all above\n            self.s_h.append(s_ht)\n            self.a_h.append(a_ht)\n            self.s_o.append(s_ot)\n            self.y_hat.append(y_t)\n        return self.y_hat\n\n    def total_loss(self, y, y_hat):\n        return sum([self.loss(t, y, y_hat) for t in range(len(y))])\n\n    @staticmethod\n    def loss(t, y, y_hat):\n        if (y[t] == y_hat[t]).all():\n            return 0\n        return np.sum(-y[t]*np.log(y_hat[t]))\n\n    @staticmethod\n    def tanh(x):\n        return np.tanh(x)\n\n    @staticmethod\n    def softmax(x):\n        x = np.array(x, dtype=np.float128)\n        # to avoid overflow due to super large x, we subtract the max\n        xx = x - np.max(x)\n        exp = np.exp(xx)\n        return exp / exp.sum(keepdims=True)\n\n    @staticmethod\n    def score_to_num(scores):\n        indices = [np.argmax(p) for p in scores]\n        return np.array(indices)\n</pre> import numpy as np  class RNN1():     def __init__(self, n_nodes=10, vocab_size=3):         # initialize a random set of weigths and biases         self.n_nodes = n_nodes         self.w = np.random.randn(n_nodes, vocab_size)         self.u = np.random.randn(n_nodes, n_nodes)         self.v = np.random.randn(vocab_size,n_nodes)         self.b_h = np.random.randn(n_nodes)         self.b_o = np.random.randn(vocab_size)         self.s_h = []         self.a_h = []         self.s_o = []         self.y_hat = []      def predict(self, x):         return [self.predict_single(xx) for xx in x]      def predict_scores(self, x):         return [self.feedforward(xx) for xx in x]          def predict_single(self, x):         scores = self.feedforward(x)         return self.score_to_num(scores)      def reset_mem(self):         self.s_h = []         self.a_h = []         self.s_o = []         self.y_hat = []          def feedforward(self, x):         self.reset_mem()         # this is the feed forward process         for t in range(len(x)):             # this follows straight from the formula             a_ht_prev = np.zeros(self.n_nodes) if t == 0 else self.a_h[t-1]             s_ht = x[t] @ self.w.T + a_ht_prev @ self.u + self.b_h             a_ht = self.tanh(s_ht)             s_ot = a_ht @ self.v.T + self.b_o             y_t = self.softmax(s_ot)              # memorize all above             self.s_h.append(s_ht)             self.a_h.append(a_ht)             self.s_o.append(s_ot)             self.y_hat.append(y_t)         return self.y_hat      def total_loss(self, y, y_hat):         return sum([self.loss(t, y, y_hat) for t in range(len(y))])      @staticmethod     def loss(t, y, y_hat):         if (y[t] == y_hat[t]).all():             return 0         return np.sum(-y[t]*np.log(y_hat[t]))      @staticmethod     def tanh(x):         return np.tanh(x)      @staticmethod     def softmax(x):         x = np.array(x, dtype=np.float128)         # to avoid overflow due to super large x, we subtract the max         xx = x - np.max(x)         exp = np.exp(xx)         return exp / exp.sum(keepdims=True)      @staticmethod     def score_to_num(scores):         indices = [np.argmax(p) for p in scores]         return np.array(indices) <p>Run some random numbers to make sure matrix dimensions are good. We have written functions to</p> <ul> <li>Make predictinos by feed forward, using randomly generated parameters</li> <li>Calculate loss</li> </ul> In\u00a0[32]: Copied! <pre>rnn1 = RNN1()\npred_scores = rnn1.predict_scores(x)\npred = rnn1.predict(x)\nloss = rnn1.total_loss(y, pred_scores)\n</pre> rnn1 = RNN1() pred_scores = rnn1.predict_scores(x) pred = rnn1.predict(x) loss = rnn1.total_loss(y, pred_scores) In\u00a0[13]: Copied! <pre>pred, loss, \n</pre> pred, loss,  Out[13]: <pre>([array([1, 0, 0, 0, 1, 0, 0, 1, 1, 0]),\n  array([2, 0, 0, 0, 1, 0, 0, 1, 2, 0]),\n  array([1, 0, 2, 1, 0, 0, 2, 0, 0, 0]),\n  array([1, 0, 2, 1, 0, 0, 0, 0, 1, 0]),\n  array([1, 0, 0, 0, 1, 2, 2, 2, 0, 0]),\n  array([2, 0, 0, 0, 0, 1, 0, 0, 1, 1]),\n  array([1, 0, 2, 2, 0, 0, 1, 0, 1, 2]),\n  array([2, 0, 0, 0, 1, 0, 0, 1, 1, 0]),\n  array([2, 0, 0, 0, 0, 0, 1, 0, 0, 2]),\n  array([1, 0, 0, 0, 1, 0, 0, 1, 1, 0])],\n np.longdouble('164.06705644911090597'))</pre> In\u00a0[10]: Copied! <pre># validate correctness of the loss calculation\nloss0 = rnn1.total_loss(y[0], y[0])\nloss1 = rnn1.total_loss(pred_scores[0], pred_scores[0])\nassert loss0 == 0\nassert loss1 == 0\n</pre> # validate correctness of the loss calculation loss0 = rnn1.total_loss(y[0], y[0]) loss1 = rnn1.total_loss(pred_scores[0], pred_scores[0]) assert loss0 == 0 assert loss1 == 0 <p>Move ahead to BPTT!</p> In\u00a0[191]: Copied! <pre>class RNN2(RNN1):\n    def __init__(self, learning_rate=0.0001, n_nodes=10, vocab_size=3):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.delta_l_over_s_h = []\n\n    def backpropagation(self, x, y, y_hat):\n        # BPTT for a single sequence of sample. This function\n        # returns the deltas of W, V, U, b_h, and b_o\n        T = len(y)\n        du = np.zeros(self.u.shape)\n        dv = np.zeros(self.v.shape)\n        dw = np.zeros(self.w.shape)\n        dbh = np.zeros(self.b_h.shape)\n        dbo = np.zeros(self.b_o.shape)\n        # btt goes backwards, from time t to time 0\n        ds_ht_plus_1 = np.zeros(self.n_nodes)\n        for t in reversed(range(T)):\n            # one round of backpropagation at time t\n            # output layer to hidden layer.\n            # dLt/dbo = (y_hat_t - y_t)\n            # dLt/dV = (y_hat_t - y_t) * a_ht = dLt/dbo * a_ht\n            dbo_t = (y_hat[t] - y[t].T)\n            dv_t = dbo_t[:, np.newaxis] @ self.a_h[t][:, np.newaxis].T\n            dbo += dbo_t\n            dv += dv_t\n\n            # hidden layer to input layer\n            da_ht = (y_hat[t] - y[t]) @ self.v + ds_ht_plus_1 @ self.u\n            ds_ht = da_ht * (1-self.a_h[t] ** 2)\n            ds_ht_plus_1 = ds_ht\n            dw_t = ds_ht[:, np.newaxis] @ x[t][:, np.newaxis].T\n\n            if t-1 &gt;= 0:\n                a_ht_1 = self.a_h[t-1]\n            else:\n                a_ht_1 = np.zeros(self.n_nodes)\n            du_t = ds_ht @ a_ht_1\n\n            dbh_t = ds_ht\n\n            dw += dw_t\n            du += du_t\n            dbh += dbh_t\n            \n        return dw, dv, du, dbh, dbo\n\n    def train(self, epochs, x, y, verbose=False):\n        for i in range(epochs):\n            loss = 0\n            for j in range(len(x)):\n                # feed forward\n                y_hat = self.feedforward(x[j])\n                # current loss\n                loss += self.total_loss(y[j], y_hat)\n                # calculate deltas\n                dW, dV, dU, dbh, dbo = self.backpropagation(x[j], y[j], y_hat)\n                # update weights accordingly\n                self.u -= self.learning_rate * dU\n                self.w -= self.learning_rate * dW\n                self.v -= self.learning_rate * dV\n                self.b_h -= self.learning_rate * dbh\n                self.b_o -= self.learning_rate * dbo\n            if verbose and i % 100 == 0:\n                print(f\"loss at epoch {i}: {loss}\")\n</pre> class RNN2(RNN1):     def __init__(self, learning_rate=0.0001, n_nodes=10, vocab_size=3):         super().__init__()         self.learning_rate = learning_rate         self.delta_l_over_s_h = []      def backpropagation(self, x, y, y_hat):         # BPTT for a single sequence of sample. This function         # returns the deltas of W, V, U, b_h, and b_o         T = len(y)         du = np.zeros(self.u.shape)         dv = np.zeros(self.v.shape)         dw = np.zeros(self.w.shape)         dbh = np.zeros(self.b_h.shape)         dbo = np.zeros(self.b_o.shape)         # btt goes backwards, from time t to time 0         ds_ht_plus_1 = np.zeros(self.n_nodes)         for t in reversed(range(T)):             # one round of backpropagation at time t             # output layer to hidden layer.             # dLt/dbo = (y_hat_t - y_t)             # dLt/dV = (y_hat_t - y_t) * a_ht = dLt/dbo * a_ht             dbo_t = (y_hat[t] - y[t].T)             dv_t = dbo_t[:, np.newaxis] @ self.a_h[t][:, np.newaxis].T             dbo += dbo_t             dv += dv_t              # hidden layer to input layer             da_ht = (y_hat[t] - y[t]) @ self.v + ds_ht_plus_1 @ self.u             ds_ht = da_ht * (1-self.a_h[t] ** 2)             ds_ht_plus_1 = ds_ht             dw_t = ds_ht[:, np.newaxis] @ x[t][:, np.newaxis].T              if t-1 &gt;= 0:                 a_ht_1 = self.a_h[t-1]             else:                 a_ht_1 = np.zeros(self.n_nodes)             du_t = ds_ht @ a_ht_1              dbh_t = ds_ht              dw += dw_t             du += du_t             dbh += dbh_t                      return dw, dv, du, dbh, dbo      def train(self, epochs, x, y, verbose=False):         for i in range(epochs):             loss = 0             for j in range(len(x)):                 # feed forward                 y_hat = self.feedforward(x[j])                 # current loss                 loss += self.total_loss(y[j], y_hat)                 # calculate deltas                 dW, dV, dU, dbh, dbo = self.backpropagation(x[j], y[j], y_hat)                 # update weights accordingly                 self.u -= self.learning_rate * dU                 self.w -= self.learning_rate * dW                 self.v -= self.learning_rate * dV                 self.b_h -= self.learning_rate * dbh                 self.b_o -= self.learning_rate * dbo             if verbose and i % 100 == 0:                 print(f\"loss at epoch {i}: {loss}\") <p>Some helper function to visualize the quality of the model before and after training</p> In\u00a0[214]: Copied! <pre>def array2str(arr):\n    return ''.join(str(a) for a in arr)\n\ndef evaluate_model(model, x, y_true):\n    true_pretty = [array2str(RNN2.score_to_num(yy)) for yy in y_true]\n    y_pred = model.predict(x)\n    pred_pretty = [array2str(yy) for yy in y_pred]\n    y_true_total = ''.join(true_pretty)\n    y_pred_total = ''.join(pred_pretty)\n    overlap = 0\n    for i in range(len(y_true_total)):\n        if y_true_total[i] == y_pred_total[i]:\n            overlap += 1\n    print(\"actual:\", true_pretty)\n    print(\"predicted:\", pred_pretty)\n    print(\"overlap:\", overlap)\n</pre> def array2str(arr):     return ''.join(str(a) for a in arr)  def evaluate_model(model, x, y_true):     true_pretty = [array2str(RNN2.score_to_num(yy)) for yy in y_true]     y_pred = model.predict(x)     pred_pretty = [array2str(yy) for yy in y_pred]     y_true_total = ''.join(true_pretty)     y_pred_total = ''.join(pred_pretty)     overlap = 0     for i in range(len(y_true_total)):         if y_true_total[i] == y_pred_total[i]:             overlap += 1     print(\"actual:\", true_pretty)     print(\"predicted:\", pred_pretty)     print(\"overlap:\", overlap) In\u00a0[216]: Copied! <pre>rnn2 = RNN2(learning_rate=0.0001, n_nodes=100)\n\nevaluate_model(rnn2, x, y)\n</pre> rnn2 = RNN2(learning_rate=0.0001, n_nodes=100)  evaluate_model(rnn2, x, y) <pre>actual: ['0022210110', '2002002022', '1201102100', '1101201202', '0011102210', '2001012012', '1100100012', '2010122011', '2210201220', '0002112100']\npredicted: ['1120020111', '0111110101', '1012111110', '1122021011', '1111212111', '0111110111', '1122111210', '0111210111', '0111011101', '1120112211']\noverlap: 30\n</pre> In\u00a0[217]: Copied! <pre>rnn2.train(5000, x, y, True)\n</pre> rnn2.train(5000, x, y, True) <pre>loss at epoch 0: 202.7836139814427\nloss at epoch 100: 110.27131099270905\nloss at epoch 200: 83.10060656394155\nloss at epoch 300: 68.02587099271048\nloss at epoch 400: 65.38807540419106\nloss at epoch 500: 58.830339777760116\nloss at epoch 600: 54.22732667386771\nloss at epoch 700: 50.55093415573766\nloss at epoch 800: 47.74560321491536\nloss at epoch 900: 44.73411202441217\nloss at epoch 1000: 42.674532978222715\nloss at epoch 1100: 40.22851081431978\nloss at epoch 1200: 37.45028109530502\nloss at epoch 1300: 36.12487867914503\nloss at epoch 1400: 35.06549335818424\nloss at epoch 1500: 33.513962830639684\nloss at epoch 1600: 32.5512323935453\nloss at epoch 1700: 31.362170861993704\nloss at epoch 1800: 30.204606414217032\nloss at epoch 1900: 28.721560859253696\nloss at epoch 2000: 26.970357477770808\nloss at epoch 2100: 24.717921731469385\nloss at epoch 2200: 22.49272184873606\nloss at epoch 2300: 26.95544520791458\nloss at epoch 2400: 22.36715164290292\nloss at epoch 2500: 22.927457101045125\nloss at epoch 2600: 20.08979899166029\nloss at epoch 2700: 19.196828047924182\nloss at epoch 2800: 18.896647133355465\nloss at epoch 2900: 16.375881668641835\nloss at epoch 3000: 16.72261866189776\nloss at epoch 3100: 13.838755733258779\nloss at epoch 3200: 13.076008068052444\nloss at epoch 3300: 12.151538393305366\nloss at epoch 3400: 11.311482642558467\nloss at epoch 3500: 10.327645010343307\nloss at epoch 3600: 10.523296159596141\nloss at epoch 3700: 10.245959941644236\nloss at epoch 3800: 9.942483406298678\nloss at epoch 3900: 9.955048248908344\nloss at epoch 4000: 10.352242849777243\nloss at epoch 4100: 10.464099192444507\nloss at epoch 4200: 10.364028014593218\nloss at epoch 4300: 10.251650538093257\nloss at epoch 4400: 10.447283274950694\nloss at epoch 4500: 11.262703641141721\nloss at epoch 4600: 10.727188040921888\nloss at epoch 4700: 9.973869429813469\nloss at epoch 4800: 9.390145566791826\nloss at epoch 4900: 8.742774743879831\n</pre> <p>After training:</p> In\u00a0[219]: Copied! <pre>evaluate_model(rnn2, x, y)\n</pre> evaluate_model(rnn2, x, y) <pre>actual: ['0022210110', '2002002022', '1201102100', '1101201202', '0011102210', '2001012012', '1100100012', '2010122011', '2210201220', '0002112100']\npredicted: ['0022210100', '2002002022', '1201102100', '1101201202', '0011102210', '2001012012', '1100100012', '2010122011', '2210201020', '0002112100']\noverlap: 98\n</pre> <p>Pretty good! This concludes this note happily.</p>"},{"location":"tech/ml/recurrent_neural_net/#recurrent-neural-net-from-scratch","title":"Recurrent Neural Net From Scratch\u00b6","text":""},{"location":"tech/ml/recurrent_neural_net/#feed-forward","title":"Feed forward\u00b6","text":"<p>Rephrasing the diagram, we have the feed forward process as, for each time step $t$,</p> <ul> <li>$s_{h,t} = x_t W + a_{h,t-1}U + b_h$</li> <li>$a_{h,t} = \\sigma_h(s_{h,t})$</li> <li>$s_{o,t} = a_{h,t}V +b_o$</li> <li>$y_t = \\sigma_o(s_{o,t})$</li> </ul> <p>h means hidden layer, o means output layer, s means weighted sum, a means activated value. $\\sigma$ means the activation function. For the activation functions, we use softmax at the output layer and tanh at the hidden layer.</p> <p>$$\\large tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$</p> <p>It has the convenient property that</p> <p>$$ \\large (tanh(x))' = 1-(tanh(x))^2  $$</p>"},{"location":"tech/ml/recurrent_neural_net/#backpropagation-through-time","title":"Backpropagation Through Time\u00b6","text":"<ul> <li>As there's dependency bewteen the previous and next steps, backpropagation in RNN goes through the entire sequence, called Backpropagation Through Time (BPTT) where we propagate the gradient not only through layers, but also through the entire sequence of data. This means we sum up the lost of the predictions of the sequece.</li> </ul> <p>Overall loss is</p> <p>$$ L_{total} = \\sum_1^t L_t $$</p> <p>where</p> <p>$$ L_t = -y_t log(\\hat{y_t}) $$</p> <ul> <li>Backpropagation from the output layer to the hidden layer is similar as vanila neural network since we don't add anything new between these two layers</li> <li>Backpropagation from the hidden layer will go to both the input layer and the hidden layer of the previous data point.</li> </ul>"},{"location":"tech/ml/recurrent_neural_net/#weights-from-the-output-layer-to-the-hidden-layer","title":"Weights from the output layer to the hidden layer\u00b6","text":"<p>From the math of basic neural net, we know</p>"},{"location":"tech/ml/recurrent_neural_net/#weights-from-the-hidden-to-the-input-layer-and-previous-data-point","title":"Weights from the hidden to the input layer and previous data point\u00b6","text":""},{"location":"tech/ml/recurrent_neural_net/#problem-to-solve","title":"Problem to solve\u00b6","text":"<p>We'll use RNN to solve a toy problem of predicting the next number in mod 3. 1 is next to 0, 2 is next to 1, 0 is next to 2.</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/","title":"Implement a basic neural network from scratch","text":"<p>A lot of the basics of this note is built upon the perceptron algorithm. Read that note first. Apology upfront for all the cumbersum notations below. Those will be useful in the next notebook on RNN, so I'll keep it consistent here.</p> <p>This is a terribly drawn but still valid fully connected neural net with 3 input features (x1, x2, x3), 1 hidden layer (node1, node2), and 1-dimensional output (y). Not shown in this picture is a bias term, to prevent trivial prediction at <code>x=[0, 0, 0]</code> regardless of weights. We could introduced the bias term by a constant node with value=1, but I decided to separate it out for easier reading later.</p> <p>We say it's fully connected because each node is connected with all nodes in the next layer. This can be seen as a generic representation. A sparse network is simply one with most weights=0.</p> <p>We notate the weights as: <code>w[i,j]</code> is the weight between node_i and input x_j</p> <p></p> <p>Before getting into details, we claim that neural network can help us with cognitive tasks by telling us about whether an object is. I wanted to build my own with some funny objects, but decided that's too much work. So we'll use the number dataset that's readily available.</p> In\u00a0[167]: Copied! <pre>import math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n</pre> import math import matplotlib.pyplot as plt import numpy as np import pandas as pd In\u00a0[20]: Copied! <pre>from keras.datasets import mnist\n\n(x_train_raw, y_train),(x_test_raw, y_test) = mnist.load_data()\nx_train_raw.shape, y_train.shape, x_test_raw.shape, y_test.shape\n</pre> from keras.datasets import mnist  (x_train_raw, y_train),(x_test_raw, y_test) = mnist.load_data() x_train_raw.shape, y_train.shape, x_test_raw.shape, y_test.shape <pre>2025-09-12 23:55:26.990868: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-09-12 23:55:27.063891: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757735727.085664  886704 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757735727.092652  886704 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-09-12 23:55:27.172324: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n</pre> Out[20]: <pre>((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))</pre> <p>I'll compress the images to simplify calculations later</p> In\u00a0[523]: Copied! <pre>def compress(img, fold=2):\n    n = img.shape[0]\n    com = [img[i] for i in range(n) if i % fold == 0]\n    com = [[c[i] for i in range(len(c)) if i % fold == 0] for c in com]\n    com = com[1:-2]\n    com = [c[1:-2] for c in com]\n    return np.array(com)\n\nx_train = np.array([compress(x, 2) for x in x_train_raw])\nx_test = np.array([compress(x, 2) for x in x_test_raw])\n</pre> def compress(img, fold=2):     n = img.shape[0]     com = [img[i] for i in range(n) if i % fold == 0]     com = [[c[i] for i in range(len(c)) if i % fold == 0] for c in com]     com = com[1:-2]     com = [c[1:-2] for c in com]     return np.array(com)  x_train = np.array([compress(x, 2) for x in x_train_raw]) x_test = np.array([compress(x, 2) for x in x_test_raw]) In\u00a0[24]: Copied! <pre>print(\"before compression\")\nimgs = x_train_raw[[i for i in range(16)]]\nfig, axes = plt.subplots(4,4)\nfor i in range(16):\n    axes[int(i/4)][i%4].imshow(imgs[i])\n</pre> print(\"before compression\") imgs = x_train_raw[[i for i in range(16)]] fig, axes = plt.subplots(4,4) for i in range(16):     axes[int(i/4)][i%4].imshow(imgs[i]) <pre>before compression\n</pre> In\u00a0[31]: Copied! <pre>print(\"after compression\")\nimgs = x_train[[i for i in range(16)]]\nfig, axes = plt.subplots(4,4)\nfor i in range(16):\n    axes[int(i/4)][i%4].imshow(imgs[i])\nprint(imgs.shape)\n</pre> print(\"after compression\") imgs = x_train[[i for i in range(16)]] fig, axes = plt.subplots(4,4) for i in range(16):     axes[int(i/4)][i%4].imshow(imgs[i]) print(imgs.shape) <pre>after compression\n(16, 11, 11)\n</pre> <p>OK, I can live with 2 fold compression</p> <p>In the matrix form, the formulation is easily extendable to more complex networks. The same principal/math applies. To do the number classification, we obviously need to go beyond just 3 input features. In fact, the image is a 11x11 grid (after compression), meaning 121 input features. So instead of $X^i$ representing $[x_1, x_2, x_3]$, we have it represent $[x_1, x_2, ..., x_{121}]$. The same goes with w.</p> In\u00a0[399]: Copied! <pre>class NaiveNeuralNet:\n    def __init__(self):\n        self.w = np.random.randn(20,121)\n        self.v = np.random.randn(10,20)\n        self.b = np.random.randn(20)\n        self.d = np.random.randn(10)\n\n    @staticmethod\n    def softmax(x):\n        x = np.array(x, dtype=np.float128)\n        # to avoid overflow due to super large x, we subtract the max\n        xx = x - np.max(x)\n        exp = np.exp(xx)\n        return exp / exp.sum(axis=1, keepdims=True)\n\n    @staticmethod\n    def sigmoid(x):\n        x = np.array(x, dtype=np.float128)\n        return 1 / (1 + np.exp(-x))\n\n    def predict_scores(self, x):\n        # feed forward. Here x is in the same format as the training samples\n        num_samples = x.shape[0]\n        # flatten the samples into 121 features.\n        x_flat = np.array([a.flatten() for a in x])\n        # Hidden layer\n        # add a bias term for each sample.\n        bias_1 = np.array([self.b for i in range(num_samples)])\n        sum_1 = x_flat @ self.w.T + bias_1\n        activated_1 = self.sigmoid(sum_1)\n        # Output layer\n        bias_2 = np.array([self.d for i in range(num_samples)])\n        sum_2 = activated_1 @ self.v.T + bias_2\n        activated_2 = self.softmax(sum_2)\n        return activated_2\n\n    def predict(self, x):\n        pred = self.predict_scores(x)\n        indices = [np.argmax(p) for p in pred]\n        return np.array(indices)\n</pre> class NaiveNeuralNet:     def __init__(self):         self.w = np.random.randn(20,121)         self.v = np.random.randn(10,20)         self.b = np.random.randn(20)         self.d = np.random.randn(10)      @staticmethod     def softmax(x):         x = np.array(x, dtype=np.float128)         # to avoid overflow due to super large x, we subtract the max         xx = x - np.max(x)         exp = np.exp(xx)         return exp / exp.sum(axis=1, keepdims=True)      @staticmethod     def sigmoid(x):         x = np.array(x, dtype=np.float128)         return 1 / (1 + np.exp(-x))      def predict_scores(self, x):         # feed forward. Here x is in the same format as the training samples         num_samples = x.shape[0]         # flatten the samples into 121 features.         x_flat = np.array([a.flatten() for a in x])         # Hidden layer         # add a bias term for each sample.         bias_1 = np.array([self.b for i in range(num_samples)])         sum_1 = x_flat @ self.w.T + bias_1         activated_1 = self.sigmoid(sum_1)         # Output layer         bias_2 = np.array([self.d for i in range(num_samples)])         sum_2 = activated_1 @ self.v.T + bias_2         activated_2 = self.softmax(sum_2)         return activated_2      def predict(self, x):         pred = self.predict_scores(x)         indices = [np.argmax(p) for p in pred]         return np.array(indices) In\u00a0[400]: Copied! <pre>net = NaiveNeuralNet()\nnet.predict_scores(imgs[:3])\n</pre> net = NaiveNeuralNet() net.predict_scores(imgs[:3]) Out[400]: <pre>array([[9.35205551e-01, 1.67069978e-03, 3.01476927e-03, 1.65483827e-02,\n        5.51737888e-04, 2.71009969e-05, 3.91528375e-02, 7.73713606e-05,\n        2.21130439e-04, 3.53041919e-03],\n       [2.31454335e-01, 2.78708098e-04, 1.07199313e-03, 6.77965641e-01,\n        2.51910835e-02, 3.69573108e-05, 4.92609756e-02, 4.87169969e-03,\n        2.35934251e-03, 7.50926412e-03],\n       [3.77629835e-01, 2.57955233e-04, 2.71079986e-03, 5.18559401e-01,\n        1.70052707e-02, 3.04029674e-05, 5.29631525e-03, 5.85975271e-02,\n        1.87688003e-02, 1.14369200e-03]], dtype=float128)</pre> In\u00a0[401]: Copied! <pre>net = NaiveNeuralNet()\nnet.predict(imgs)\n</pre> net = NaiveNeuralNet() net.predict(imgs) Out[401]: <pre>array([8, 8, 8, 5, 8, 5, 8, 8, 8, 8, 8, 8, 1, 8, 8, 8])</pre> <p>Of course, the predictions are all over the places! A prediction from randomly generated parameters is expected to be very random. Therefore we need to adjust the parameters via model training.</p> In\u00a0[9]: Copied! <pre>def entropy(y_true, y_pred):\n    s = 0\n    for a, b in zip(y_true, y_pred):\n        s -= a * math.log(b)\n    return s\n\ny_true = [0, 1, 0, 0]\ny_pred = [0.001, 0.0001, 0.0002, 0.001]  # predicts nothing\nprint(entropy(y_true, y_pred))\n\ny_true = [0, 1, 0, 0]\ny_pred = [0.001, 0.999, 0.0002, 0.001]  # fairly good\nprint(entropy(y_true, y_pred))\n\ny_true = [0, 1, 0, 0]\ny_pred = [0.001, 0.501, 0.5002, 0.001]  # predicts maybe ok\nprint(entropy(y_true, y_pred))\n\ny_true = [0, 1, 0, 0]\ny_pred = [0.001, 0.801, 0.5002, 0.001]  # predicts fairly ok\nprint(entropy(y_true, y_pred))\n\ny_true = [0, 1, 0, 0]\ny_pred = [0.001, 0.201, 0.9002, 0.001]  # predicts fairly bad\nprint(entropy(y_true, y_pred))\n</pre> def entropy(y_true, y_pred):     s = 0     for a, b in zip(y_true, y_pred):         s -= a * math.log(b)     return s  y_true = [0, 1, 0, 0] y_pred = [0.001, 0.0001, 0.0002, 0.001]  # predicts nothing print(entropy(y_true, y_pred))  y_true = [0, 1, 0, 0] y_pred = [0.001, 0.999, 0.0002, 0.001]  # fairly good print(entropy(y_true, y_pred))  y_true = [0, 1, 0, 0] y_pred = [0.001, 0.501, 0.5002, 0.001]  # predicts maybe ok print(entropy(y_true, y_pred))  y_true = [0, 1, 0, 0] y_pred = [0.001, 0.801, 0.5002, 0.001]  # predicts fairly ok print(entropy(y_true, y_pred))  y_true = [0, 1, 0, 0] y_pred = [0.001, 0.201, 0.9002, 0.001]  # predicts fairly bad print(entropy(y_true, y_pred)) <pre>9.210340371976182\n0.0010005003335835344\n0.6911491778972723\n0.22189433191377778\n1.6044503709230613\n</pre> <p>close to 0 for correct predictions and large for bad predictions. It goes to infinity as the predicted score goes to 0, due to the ln() term, as $\\lim\\limits_{x\\to 0}\\ln(x)=-\\infty$</p> <p>The overall cost function is the average of all losses</p> In\u00a0[15]: Copied! <pre>def cost(y_pred, y_true):\n    e = 1e-10  # to avoid actually having ln(0)\n    y_pred = np.clip(y_pred, e, 1.0)\n    loss = -np.sum(y_true * np.log(y_pred))  # dot product then sum\n    return loss / y_pred.shape[0]\n\ny_pred = [[0, 0.1, 0], [0.5, 0.2, 0.3]]\ny_true = [[1, 0, 0], [1, 0, 0]]\nprint(\"bad prediction:\", cost(np.array(y_pred), np.array(y_true)))\n\ny_pred = [[0.9, 0.1, 0], [0.7, 0.2, 0.3]]\ny_true = [[1, 0, 0], [1, 0, 0]]\nprint(\"good one:\", cost(np.array(y_pred), np.array(y_true)))\n</pre> def cost(y_pred, y_true):     e = 1e-10  # to avoid actually having ln(0)     y_pred = np.clip(y_pred, e, 1.0)     loss = -np.sum(y_true * np.log(y_pred))  # dot product then sum     return loss / y_pred.shape[0]  y_pred = [[0, 0.1, 0], [0.5, 0.2, 0.3]] y_true = [[1, 0, 0], [1, 0, 0]] print(\"bad prediction:\", cost(np.array(y_pred), np.array(y_true)))  y_pred = [[0.9, 0.1, 0], [0.7, 0.2, 0.3]] y_true = [[1, 0, 0], [1, 0, 0]] print(\"good one:\", cost(np.array(y_pred), np.array(y_true))) <pre>bad prediction: 11.8594990552502\ngood one: 0.23101772979827936\n</pre> <p>Based on the chain rule, we can breakdown the partial derivatives for w and v into LARGE chains like so:</p> <p>$$ \\large \\frac{\\partial L}{\\partial v_{jk}} = \\frac{\\partial L}{\\partial s_{ok}}\\frac{\\partial s_{ok}}{\\partial v_{jk}} $$</p> <p>$$ \\large \\frac{\\partial L}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial s_{ok}} \\frac{\\partial s_{ok}}{\\partial a_{hj}} \\frac{\\partial a_{hj}}{\\partial s_{hj}} \\frac{\\partial s_{hj}}{\\partial w_{ij}} $$</p> <p>Where $s_o, s_h$ are the weighted sum at the outer and hidden layers, respectively. $a_h$ is the activated value at the hidden layer.</p> <p>Going layer by layer back, we start with derivative of cost function</p> <p>For simplicity, let $\\sigma(s_i)$ be the softmax at class i (a.k.a output node i). Recall that $s_i$ is the weighted sum plus bias, over all edges from the hidden layer to the output layer node i.</p> <p>Therefore, going from the lost function to the hidden-&gt;output layer weights, we have</p> <p>$$\\large \\frac{\\partial L}{\\partial v_{ik}} = \\frac{\\partial L}{\\partial s_i} \\frac{\\partial s_i}{\\partial v_{ik}}= (\\hat{y_i}-y_i) x_k $$</p> <p>For the bias term, we replace $x_k$ with 1.</p> <p>We already have $\\large \\frac{\\partial L}{\\partial s_{ok}} = \\hat{y_k}-y_k$</p> <p>Also, $\\large \\frac{\\partial s_{ok}}{\\partial a_{hj}} = \\frac{\\partial (a_h V_k + d)}{\\partial a_{hj}} = v_{jk}$ because only $v_{jk}$ will be multiplied with $a_{hj}$ out of all these terms.</p> <p>Next we solve $\\large \\frac{\\partial a_{hj}}{\\partial s_{hj}}$, which is the derivative of sigmoid, as $a_{hj} = \\sigma(s_{hj})$</p> <p>Let $u = s_{hj}$, we have $$\\large \\begin{align} \\frac{\\partial \\sigma(u)}{\\partial u} &amp;= (-\\frac{1}{1+e^{-u}})' \\\\ &amp;= -\\frac{1}{(1+e^{-u})^2}(e^{-u})' \\\\ &amp;= -\\frac{1}{(1+e^{-u})^2}(-e^{-u}) \\\\ &amp;= \\frac{e^{-u}}{(1+e_{-u})^2} \\\\ &amp;= \\frac{1}{1+e^{-u}}\\frac{e^{-u}}{1+e^{-u}} \\\\ &amp;= \\frac{1}{1+e^{-u}}(1-\\frac{1}{1+e^{-u}}) \\\\ &amp;= \\sigma(u)(1-\\sigma(u)) \\end{align}  $$</p> <p>The last item in the chain, $\\large \\frac{\\partial s_{hj}}{\\partial w_{ij}}=\\frac{\\partial (XW_j+b)}{\\partial w_{ij}} = x_{i}$ since only $x_i$ will be multiplied with $w_{ij}$</p> <p>Combining all together, we get</p> <p>$$\\large \\begin{align} \\frac{\\partial L}{\\partial w_{ij}} &amp;= (\\hat{y_k} -y_k) \\cdot v_{jk} \\cdot (\\sigma_{sigmoid}(s_{hj})(1-\\sigma_{sigmoid}(s_{hj})) x_i \\\\ &amp;= (\\hat{y_k} -y_k) \\cdot v_{jk} \\cdot a_{hj} \\cdot (1-a_{hj}) \\cdot x_i \\end{align} $$</p> <p>Again, for the bias term, replace $x_i$ with 1.</p> <p>With all the tools above, let's expand upon the native neural net to make one that can backpropagate</p> In\u00a0[545]: Copied! <pre>class SmartNeuralNet(NaiveNeuralNet):\n    def __init__(self, learning_rate=0.1, n_nodes=20):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.w = np.random.randn(n_nodes,121)\n        self.v = np.random.randn(10,n_nodes)\n        self.b = np.random.randn(n_nodes)\n\n    @staticmethod\n    def cost(y_pred, y_true):\n        e = 1e-10  # to avoid actually having ln(0)\n        y_pred = np.clip(y_pred, e, 1.0)\n        loss = -np.sum(y_true * np.log(y_pred))  # dot product then sum\n        return loss / y_pred.shape[0]\n\n    def backpropagate(self, x, y):\n        num_samples = x.shape[0]\n        bias_hidden = np.array([self.b for i in range(num_samples)])\n        bias_out = np.array([self.d for i in range(num_samples)])\n        \n        # output to hidden layer\n        s_hidden = x @ self.w.T + bias_hidden\n        # print(z_hidden.shape, x.shape, self.w.T.shape, bias_hidden.shape)\n        a_hidden = self.sigmoid(s_hidden)\n        # print(a_hidden.shape)\n\n        s_out = a_hidden @ self.v.T + bias_out\n        a_out = self.softmax(s_out)\n        # print(y.shape, a_out.shape, a_hidden.shape)\n        delta_v = (-y + a_out).T @ a_hidden\n        delta_bias_out = (-y + a_out)\n        assert delta_v.shape == self.v.shape\n\n        # hidden to input layer\n        # (-y - sigmoid(z_out)) * v\n        delta_w_step_1 = (-y + a_out) @ self.v\n        # print(delta_w_step_1.shape, self.v.shape)\n        # * a_hidden * (1-a_hidden) @ x\n        delta_w_step_2 = delta_w_step_1 * a_hidden * (1 - a_hidden)\n        delta_w = delta_w_step_2.T @ x\n        delta_bias_hidden = delta_w_step_2\n\n        # update weights w, v, biases. Do this at the end since all calculations\n        # above involve w and v and need to be done first. Subtract delta because\n        # we want to move aganist the direction where delta is going. We want to\n        # reduce the gap\n        #print(delta_v.sum())\n        self.w -= self.learning_rate * delta_w\n        self.v -= self.learning_rate * delta_v\n        # for bias we need to do a sum over all the samples \n        self.b -= self.learning_rate * delta_bias_hidden.sum(axis=0)\n        self.d -= self.learning_rate * delta_bias_out.sum(axis=0)\n\n    @staticmethod\n    def label_to_classes(y):\n        # turn a single digit label to multiclass labels\n        labels = np.zeros((len(y), 10), dtype=int)\n        for i, yy in enumerate(y):\n            labels[i, yy] = 1\n        return labels\n\n    def fit(self, x, y, epochs=200, verbose=True):\n        x_flat = np.array([a.flatten() for a in x])\n        for i in range(epochs):\n            # print out current cost\n            if verbose and ((i % 200 == 0) or (i &lt; 20 and i % 5 == 0) or (i &lt; 100 and i % 20 == 0)):\n                y_curr = self.predict_scores(x)\n                cost = self.cost(y_curr, y)\n                y_pred = self.predict(tx)\n                indices = [np.argmax(p) for p in y]\n                correct = (y_pred == indices).sum()\n                print(f\"cost at epoch {i}: {cost}. Predicted {correct} samples correctly, or {round(100.0*correct/x.shape[0],2)} percent\")\n            # backprop to update weights\n            self.backpropagate(x_flat, y)\n</pre> class SmartNeuralNet(NaiveNeuralNet):     def __init__(self, learning_rate=0.1, n_nodes=20):         super().__init__()         self.learning_rate = learning_rate         self.w = np.random.randn(n_nodes,121)         self.v = np.random.randn(10,n_nodes)         self.b = np.random.randn(n_nodes)      @staticmethod     def cost(y_pred, y_true):         e = 1e-10  # to avoid actually having ln(0)         y_pred = np.clip(y_pred, e, 1.0)         loss = -np.sum(y_true * np.log(y_pred))  # dot product then sum         return loss / y_pred.shape[0]      def backpropagate(self, x, y):         num_samples = x.shape[0]         bias_hidden = np.array([self.b for i in range(num_samples)])         bias_out = np.array([self.d for i in range(num_samples)])                  # output to hidden layer         s_hidden = x @ self.w.T + bias_hidden         # print(z_hidden.shape, x.shape, self.w.T.shape, bias_hidden.shape)         a_hidden = self.sigmoid(s_hidden)         # print(a_hidden.shape)          s_out = a_hidden @ self.v.T + bias_out         a_out = self.softmax(s_out)         # print(y.shape, a_out.shape, a_hidden.shape)         delta_v = (-y + a_out).T @ a_hidden         delta_bias_out = (-y + a_out)         assert delta_v.shape == self.v.shape          # hidden to input layer         # (-y - sigmoid(z_out)) * v         delta_w_step_1 = (-y + a_out) @ self.v         # print(delta_w_step_1.shape, self.v.shape)         # * a_hidden * (1-a_hidden) @ x         delta_w_step_2 = delta_w_step_1 * a_hidden * (1 - a_hidden)         delta_w = delta_w_step_2.T @ x         delta_bias_hidden = delta_w_step_2          # update weights w, v, biases. Do this at the end since all calculations         # above involve w and v and need to be done first. Subtract delta because         # we want to move aganist the direction where delta is going. We want to         # reduce the gap         #print(delta_v.sum())         self.w -= self.learning_rate * delta_w         self.v -= self.learning_rate * delta_v         # for bias we need to do a sum over all the samples          self.b -= self.learning_rate * delta_bias_hidden.sum(axis=0)         self.d -= self.learning_rate * delta_bias_out.sum(axis=0)      @staticmethod     def label_to_classes(y):         # turn a single digit label to multiclass labels         labels = np.zeros((len(y), 10), dtype=int)         for i, yy in enumerate(y):             labels[i, yy] = 1         return labels      def fit(self, x, y, epochs=200, verbose=True):         x_flat = np.array([a.flatten() for a in x])         for i in range(epochs):             # print out current cost             if verbose and ((i % 200 == 0) or (i &lt; 20 and i % 5 == 0) or (i &lt; 100 and i % 20 == 0)):                 y_curr = self.predict_scores(x)                 cost = self.cost(y_curr, y)                 y_pred = self.predict(tx)                 indices = [np.argmax(p) for p in y]                 correct = (y_pred == indices).sum()                 print(f\"cost at epoch {i}: {cost}. Predicted {correct} samples correctly, or {round(100.0*correct/x.shape[0],2)} percent\")             # backprop to update weights             self.backpropagate(x_flat, y) In\u00a0[529]: Copied! <pre>tx = x_train[:1000]\nty_raw = y_train[:1000]\ntx = [a.flatten() for a in tx]\ntx = np.array(tx)\nty = nn.label_to_classes(ty_raw)\n</pre> tx = x_train[:1000] ty_raw = y_train[:1000] tx = [a.flatten() for a in tx] tx = np.array(tx) ty = nn.label_to_classes(ty_raw) In\u00a0[520]: Copied! <pre>nn = SmartNeuralNet(learning_rate=0.00001, n_nodes=100)\nnn.fit(tx, ty, epochs=500, verbose=True)\n</pre> nn = SmartNeuralNet(learning_rate=0.00001, n_nodes=100) nn.fit(tx, ty, epochs=500, verbose=True) <pre>cost at epoch 0: 13.198930496483507. Predicted 92 samples correctly\ncost at epoch 5: 12.167644632532499. Predicted 101 samples correctly\ncost at epoch 10: 11.417701696942665. Predicted 101 samples correctly\ncost at epoch 15: 10.83024405785545. Predicted 103 samples correctly\ncost at epoch 20: 10.320146829744136. Predicted 107 samples correctly\ncost at epoch 40: 8.940267426342567. Predicted 130 samples correctly\ncost at epoch 60: 8.07253134511341. Predicted 134 samples correctly\ncost at epoch 80: 7.489950654295265. Predicted 141 samples correctly\ncost at epoch 200: 5.982117800522912. Predicted 157 samples correctly\ncost at epoch 400: 5.081140574251207. Predicted 179 samples correctly\n</pre> In\u00a0[532]: Copied! <pre>txx = x_test[:1000]\ntyy_raw = y_test[:1000]\ntxx = [a.flatten() for a in txx]\ntxx = np.array(txx)\ntyy = nn.label_to_classes(tyy_raw)\n\ny_predd = nn.predict(txx)\ncor = (y_predd == tyy_raw).sum()\nprint(f\"Predicted {cor} samples from the test set, or {round(100.0*cor/txx.shape[0],2)} percent\")\n</pre> txx = x_test[:1000] tyy_raw = y_test[:1000] txx = [a.flatten() for a in txx] txx = np.array(txx) tyy = nn.label_to_classes(tyy_raw)  y_predd = nn.predict(txx) cor = (y_predd == tyy_raw).sum() print(f\"Predicted {cor} samples from the test set, or {round(100.0*cor/txx.shape[0],2)} percent\") <pre>Predicted 152 samples from the test set, or 15.2 percent\n</pre> <p>Try the above for a few more times. Maybe for different parameters.</p> In\u00a0[540]: Copied! <pre>nn = SmartNeuralNet(learning_rate=0.0001, n_nodes=100)\nnn.fit(tx, ty, epochs=500, verbose=True)\n\ny_predd = nn.predict(txx)\ncor = (y_predd == tyy_raw).sum()\nprint(f\"Predicted {cor} samples from the test set, or {round(100.0*cor/txx.shape[0],2)} percent\")\n</pre> nn = SmartNeuralNet(learning_rate=0.0001, n_nodes=100) nn.fit(tx, ty, epochs=500, verbose=True)  y_predd = nn.predict(txx) cor = (y_predd == tyy_raw).sum() print(f\"Predicted {cor} samples from the test set, or {round(100.0*cor/txx.shape[0],2)} percent\") <pre>cost at epoch 0: 10.0488708348618. Predicted 99 samples correctly, or 9.9 percent\ncost at epoch 5: 6.886981741943602. Predicted 108 samples correctly, or 10.8 percent\ncost at epoch 10: 5.830659017113628. Predicted 119 samples correctly, or 11.9 percent\ncost at epoch 15: 5.314125473940891. Predicted 134 samples correctly, or 13.4 percent\ncost at epoch 20: 4.991783899894754. Predicted 155 samples correctly, or 15.5 percent\ncost at epoch 40: 4.215592478986276. Predicted 202 samples correctly, or 20.2 percent\ncost at epoch 60: 3.745509546814985. Predicted 243 samples correctly, or 24.3 percent\ncost at epoch 80: 3.3774466478225027. Predicted 289 samples correctly, or 28.9 percent\ncost at epoch 200: 2.1660509565047277. Predicted 480 samples correctly, or 48.0 percent\ncost at epoch 400: 1.4274599810089086. Predicted 617 samples correctly, or 61.7 percent\nPredicted 496 samples from the test set, or 49.6 percent\n</pre> In\u00a0[543]: Copied! <pre># some spot checking to make sure it's not an error\nprint(y_predd[:20])\nprint(tyy_raw[:20])\n</pre> # some spot checking to make sure it's not an error print(y_predd[:20]) print(tyy_raw[:20]) <pre>[7 3 1 7 9 1 4 4 4 9 0 2 9 0 1 4 9 7 5 4]\n[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#implement-a-basic-neural-network-from-scratch","title":"Implement a basic neural network from scratch\u00b6","text":"<ul> <li>Prerequisite: basic concepts of derivatives. Partial derivative, chain rule, quotient rule etc.</li> </ul>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#formulation","title":"Formulation\u00b6","text":"<p>Here's the formula for the simplified network at the very top, given the feature values of a single data point (x1, x2). The actual representation might vary in different tutorials but the idea is the same. You multiply each feature value with its weight towards nodes. The value at each node is the sum of value*weight of all features connected to it. Finally, a bias term <code>b</code> is added before the activation function.</p> <p>For activation, we use the good old sigmoid function to go from input layer to the hidden layer, aka first layer. As a reminder, the sigmoid function is defined as</p> <p>$$\\sigma(s_i) = \\frac{1}{1 + e^{-s_i}}$$</p> <p>where $s_i$ is the weighted sum at hidden layer node i. However, we'll do it differently at the output layer. As we want to predict the probabilities of the output being 0-9 respectively, it makes sense to have them add up to 1. The softmax function forces the sum of all probabilites to be 1, so we'll use it for the output layer</p> <p>$$\\sigma(s_i) = \\frac{e^{s_i}}{\\sum_{j=1}^{j=m}(e^{-s_j})}$$</p> <p>where m is the number of nodes in the output layer and $s_i$ is the weighted sum at output node i. Note that softmax at node_i depends on the weighted sum at other nodes.</p> <p>Writing all items out, we have</p> <p>Hidden layer:</p> <p>$$ \\begin{bmatrix} node1 \\\\ node2 \\\\ \\end{bmatrix} = \\sigma_{sigmoid}( \\begin{bmatrix}  w_{11} &amp; w_{12} &amp; w_{13} \\\\ w_{21} &amp; w_{22} &amp; w_{23} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix} ) $$</p> <p>Output layer</p> <p>$$ y =  \\sigma_{softmax}( \\begin{bmatrix} v_1 &amp; v_2 \\end{bmatrix} \\begin{bmatrix} node1 \\\\ node2 \\\\ \\end{bmatrix} + d ) $$</p> <p>$w$ and $v$ are the weights and $b, d$ are the bias terms.</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#feed-forward-from-input-to-prediction","title":"Feed forward, from input to prediction\u00b6","text":""},{"location":"tech/ml/wooden_wheels/basic_neural_net/#hidden-layer","title":"Hidden layer\u00b6","text":"<p>Suppose there are n samples, to go from those $x$ values to the hidden layer, we include all samples and layout the entire thing in a matrix form. Here</p> <ul> <li>$N^i$ means the vector $[node_1, node_2]$ calculated from the ith sample.</li> <li>$X^i$ means $[x_1, x_2, x_3]$ of the ith sample</li> <li>$W$ means the entire weight vector, transposed to whichever shape it needs to be for matrbix match, either 2x3 or 3x2</li> <li>Similarly, $B$ is the bias term vector.</li> </ul> <p>$$ \\begin{bmatrix}  N^1 \\\\ N^2 \\\\ ... \\\\ N^n \\end{bmatrix} = \\sigma( \\begin{bmatrix}  X^1 \\\\ X^2 \\\\ ... \\\\ X^n \\end{bmatrix} W + \\begin{bmatrix} B \\\\ B \\\\ ... \\\\ B \\\\ \\end{bmatrix} ) $$</p> <p>which is really:</p> <p>$$ \\begin{bmatrix}  node_{11} &amp; node_{12} \\\\ node_{21} &amp; node_{22} \\\\ ... \\\\ node_{n1} &amp; node_{n2} \\end{bmatrix} = \\sigma( \\begin{bmatrix}  x_{11} &amp; x_{12} &amp; x_{12} \\\\ x_{21} &amp; x_{22} &amp; x_{23} \\\\ ... \\\\ x_{n1} &amp; x_{n2} &amp; x_{n3} \\end{bmatrix} \\begin{bmatrix} w_{11} &amp; w_{21} \\\\ w_{12} &amp; w_{22} \\\\ w_{13} &amp; w_{23} \\end{bmatrix} + \\begin{bmatrix} b_1 &amp; b_2 \\\\ b_1 &amp; b_2 \\\\ ... \\\\ b_1 &amp; b_2 \\\\ \\end{bmatrix} ) $$</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#output-layer","title":"Output layer\u00b6","text":"<p>Similarly, to get to the output, we have</p> <p>$$ \\hat{Y} = \\begin{bmatrix} \\hat{y_1} \\\\ \\hat{y_2} \\\\ ... \\\\ \\hat{y_n} \\end{bmatrix} = \\sigma( \\begin{bmatrix}  node_{11} &amp; node_{12} \\\\ node_{21} &amp; node_{22} \\\\ ... \\\\ node_{n1} &amp; node_{n2} \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} + \\begin{bmatrix} d \\\\ d \\\\ ..\\\\ d \\end{bmatrix} ) = \\sigma(Nodes\\times V + D) $$</p> <p>($\\sigma$ represents either sigmoid or softmax)</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#lets-go-even-further-and-do-some-predictions","title":"Let's go even further and do some predictions!\u00b6","text":"<p>To tackle the number prediction problem, we use a larger network such that</p> <ul> <li>input layer has 121 features</li> <li>1st layer has 20 nodes</li> <li>output layer has 10 nodes, representing 0-9.</li> </ul> <p>Since we don't know what the weights and biases are, we will just use some random values and parameterized the functions. Summarizing all the calculations above, we have a basic naive predictor:</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#how-to-find-best-set-of-parameters","title":"How to find best set of parameters?\u00b6","text":"<p>Unlike linear regression, there's no algebraic solution to find optimal coefficients. We need to iteratively \"guess\" a set of weights, calculate the errors, and infer from the errors how we should modify the weights towards a better direction (lower errors).</p> <p>N x (update weights -&gt; calculate error -&gt; somehow find out how to update weights based on the error)</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#loss-function-and-cost","title":"Loss function and cost\u00b6","text":"<p>The function that calculates such errors is called the loss function. The sum of the loss over all the samples is called the cost. The practice of using errors at output layer to update all the weights is called back propagation, since we are going backwards. How do we use errors? Would be nice if someone can tell me something like \"if you change $w_1$ by 0.2, then you can reduce the errors by 0.3\". Keep in mind that while this tells us the direction of the next move, it doesn't tell us how much to move before we reach a minimum.</p> <p>To get such direction, we want to know the derivative of loss function with respect to each weight and bias. The prerequisite is that the loss function must be differentiable w.r.t all weights and biases.</p> <p>For a classification problem of n classes, let's use cross entropy as the loss:</p> <p>$$ L = -(y_{1}\\ln\\hat{y_1} + y_{2}\\ln\\hat{y_2}...+y_{n}\\hat{y_n}) $$ where vector $y$ is the truth and vector $\\hat{y}$ is the prediction. The predictioins come from softmax at the corresponding node, so we can also write the loss as</p> <p>$$L = -\\sum y_i \\ln \\sigma (s_i)$$</p> <p>where $s_i$ is the weighted sum plus bias at the output layer.</p> <p>To get an intuition of this function, we look at some examples</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#backpropagation","title":"Backpropagation\u00b6","text":"<p>With the cost calculated, we are ready to backpropagate! The idea is to trace the error all the way back to see to which direction we should move the weights. As we attribute the errors to each weight, a lot of partial derivatives are involved. Here's a simple demo of a single path from input to output layer. We want to take derivatives along such paths.</p> <p></p> <p>Take a deep breath and then take the derivative of cost over the weights w</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#some-math-magic-if-you-are-interesetd-else-jump-to-the-code-implementation","title":"Some math magic if you are interesetd, else jump to the code implementation\u00b6","text":"<p>To do some prep work for implementation, we need to do derivatives. We rely on the following rules:</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#chain-rule","title":"Chain rule\u00b6","text":"<p>$$\\frac{dy}{du} = \\frac{dy}{dx} \\frac{dx}{du}$$</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#quotient-rule","title":"Quotient rule\u00b6","text":"<p>if $h(x) = \\frac{f(x)}{g(x)}$, then $$h'(x) = \\frac{f'(x)g(x)-g'(x)f(x)}{g(x)^2}$$</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#useful-derivatives","title":"Useful derivatives\u00b6","text":"<p>$$ (e^x)' = e^x $$</p> <p>$$ (\\ln x)' = \\frac{1}{x} $$ From now on, I'll use $\\frac{dy}{dx}$ style notation and $f'$ style interchangeably for my convenience. Apology for potential confusions.</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#deriavtive-of-cross-entropy-loss","title":"Deriavtive of cross entropy loss\u00b6","text":"<p>With some chain rule trick, we get $$ \\large \\begin{align} \\frac{\\partial L}{\\partial s_i} &amp;= \\frac{\\partial}{\\partial s_i} (-\\sum y_k \\ln \\sigma (s_k)) \\\\ &amp;= -\\sum y_k \\frac{\\partial}{\\partial s_i} \\ln \\sigma(s_k) \\\\ &amp;= -\\sum y_k \\frac{\\partial \\ln \\sigma (s_k)}{\\partial \\sigma (s_k)} \\frac{\\partial \\sigma(s_k)}{\\partial s_i} \\\\ &amp;= -\\sum y_k \\frac{1}{\\sigma(s_k)} \\frac{\\partial (\\sigma(s_k))}{\\partial s_i} \\end{align} $$</p> <p>We already have $\\sigma(s_k)$ calculated during feed forward. We next derive $\\Large \\frac{\\partial (\\sigma(s_k))}{\\partial s_i}$</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#derivative-of-softmax","title":"Derivative of softmax\u00b6","text":"<p>The partial derivative of softmax at class i w.r.t. $s_j$ is:</p> <p>$$  \\large  \\frac{\\partial(\\sigma(s_i))}{\\partial(s_j)} = \\dfrac{\\partial \\frac{e^{s_i}}{\\sum_{1}^{m}(e^{s_k})}}{\\partial(s_j)} $$</p> <p>There are two cases, $i==j$ or $i\\neq j$</p> <p>if $i==j$, then $\\Large \\frac{\\partial}{\\partial s_i}(\\sum e^{s_k})$ reduces to $\\Large \\frac{\\partial}{\\partial s_i} e^{s_i}$, because $s_i$ and $s_k$s are independent</p> <p>$$ \\large \\begin{align} \\dfrac{\\partial \\frac{e^{s_i}}{\\sum_{1}^{m}(e^{s_k})}}{\\partial(s_i)} &amp;= \\frac{(e^{s_i})'\\sum(e^{s_k})-(\\sum e^{s_k})'e^{s_i}}{(\\sum e^{s_k})^2} \\\\ &amp;= \\frac{e^{s_i}\\sum e^{s_k} - e^{s_i}e^{s_i}}{(\\sum e^{s_k})^2} \\\\ &amp;= \\frac{e^{s_i} (\\sum e^{s_k} - e^{s_i})}{\\sum e^{s_k} \\sum e^{s_k}} \\\\ &amp;= \\frac{e^{s_i}}{\\sum e^{s_k}} (1 - \\frac{e^{s_i}}{\\sum e^{s_k}}) = \\sigma(s_i)(1-\\sigma(s_i)) \\end{align} $$</p> <p>This is nice, we can express the derivative of softmax in terms of the softmax itself</p> <p>if $i\\neq j$, we have $\\Large \\frac{\\partial}{\\partial e_j} e^{s_i}=0$, and $\\Large \\frac{\\partial}{\\partial s_j}(\\sum e^{s_k}) = \\frac{\\partial}{\\partial s_j} e^{s_j}$, therefore</p> <p>$$ \\large \\begin{align} \\dfrac{\\partial(\\frac{e^{s_i}}{\\sum_{1}^{m}(e^{s_k})})}{\\partial(s_j)} &amp;= \\frac{(\\frac{\\partial}{\\partial s_j}e^{s_i})\\sum(e^{s_k})-(\\frac{\\partial}{\\partial s_j}\\sum e^{s_k})e^{s_i}}{(\\sum e^{s_k})^2} \\\\ &amp;= \\frac{0-e^{s_j}e^{s_i}}{\\sum e^{s_k}\\sum e^{s_k}} \\\\ &amp;= -\\sigma(s_i)\\sigma(s_j) \\end{align} $$</p> <p>This is nice too. Now we have both cases covered for the derivative of softmax.</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#derivative-of-the-last-layer","title":"Derivative of the last layer\u00b6","text":"<p>Pulling all together, we have</p> <p>$$ \\large \\begin{align} \\frac{\\partial L}{\\partial s_i} &amp;= -y_i\\frac{1}{\\sigma(s_i)}(\\sigma(s_i)(1-\\sigma(s_i)) \\\\ &amp;- \\sum_{k\\neq i}y_k\\frac{1}{\\sigma(s_k)}(-\\sigma(s_i)\\sigma(s_k)) \\\\ &amp;= -y_i(1-\\sigma(s_i)) - \\sum_{k\\neq i} -y_k\\sigma(s_i) \\\\ &amp;= -y_i + \\sigma(s_i)(y_i + \\sum_{k\\neq i} y_k) \\end{align} $$</p> <p>Obviously, $y_i + \\sum_{k\\neq i} y_k$ equals 1 as there's only one object in the image. For only one class can the truth label be 1. So the whole thing simplifies to</p> <p>$$ \\large \\frac{\\partial L}{\\partial s_i} = -y_i + \\sigma(s_i) $$</p> <p>which is exactly $\\hat{y_i} - y_i$. Magic!</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#derivativer-over-weights-between-hidden-and-outupt-layers","title":"Derivativer over weights between hidden and outupt layers\u00b6","text":"<p>The last piece is the derivative of softmax over the weights, since weights are what need to be adjusted.</p> <p>$$ \\large \\frac{\\partial s_i}{\\partial v_{ik}} = \\frac{\\partial (v_{i1}x_1 + v_{i2}x_2 ... + v_{ik}x_k + ...)}{\\partial v_{ik}} = x_k $$</p> <p>Where $\\large v_{ik}$s are the weights of edges connecting the hidden layer nodes with the output layer node i, and x here is the node from the hidden layer, Not the input layer! I know I should have better notations</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#derivative-over-weights-between-input-and-hidden-layers","title":"Derivative over weights between input and hidden layers\u00b6","text":"<p>Updates to weights $w$ is similar. Recall that earlier we have</p> <p></p> <p>$$ \\large \\frac{\\partial L}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial s_{ok}} \\frac{\\partial s_{ok}}{\\partial a_{hj}} \\frac{\\partial a_{hj}}{\\partial s_{hj}} \\frac{\\partial s_{hj}}{\\partial w_{ij}} $$</p> <p>For the weight between input i and hidden layer j, from the loss function attributed to output node k</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#neural-network","title":"Neural network\u00b6","text":"<p>It took me some trial and errors to get the dimensions right for vectorization. I'll also add the option to modify the size of the hidden layer</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#train-and-test-modes","title":"Train and test modes\u00b6","text":"<p>Make a new model. Take a subset of training samples to train (fit) it and monitor the cost over epochs. To assess the effectiveness of this algorithm, we also look at how many samples we predict correctly.</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#performance-on-the-test-set","title":"Performance on the test set\u00b6","text":"<p>Recall that we saved a subset as the test set. Let's see how it performs on the test set. We hope it's not too much worse than the train set. It's almost guaranteed that it would be worse than the train set, because the model iterrates over and over on the train set whereas it hasn't \"seen\" the test set yet.</p>"},{"location":"tech/ml/wooden_wheels/basic_neural_net/#open-questions","title":"Open questions\u00b6","text":"<p>Try above a few times and pay attention to a few things:</p> <ul> <li>What's the initial quality of prediction, before any back propagation? How about when training is done?</li> <li>How does the quality of prediction correlate with the cost?</li> <li>How does cost go down as we do more epochs? How many epochs did it take to become flat?</li> <li>Does cost always reduce or sometimes increarse as well?</li> <li>How varied are the results each time you start fresh with a new network</li> <li>What happens if you adjust the number of nodes, number of epochs, and/or learning rate?</li> <li>What happens if you use more/fewer training samples?</li> </ul> <p>Further topics for research</p> <ul> <li>Regularization</li> <li>Overfitting</li> <li>Factors impacting the performance of neural networks</li> </ul>"},{"location":"tech/ml/wooden_wheels/genetic_algorithm/","title":"Genetic algorithm","text":"<p>There's already a good explanation here: https://towardsdatascience.com/introduction-to-genetic-algorithms-including-example-code-e396e98d8bf3</p> <p>but I'll use a different example to demonstrate how to do optimization by selecting the most fit individuals and allowing them to produce progency of potentially higher fitness at each generation.</p> <p>Problem statement: given a shuriken (ninja dart) represented as segments, I want to write a method that draws another shuriken as close to this shuriken as possible. Another way to say it is that I want to minimize the distance between the new and old shuriken.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nfrom numpy import random\nimport pylab as pl\nfrom matplotlib import collections  as mc\n</pre> import numpy as np from numpy import random import pylab as pl from matplotlib import collections  as mc In\u00a0[2]: Copied! <pre># A method to return coordinates for the segments of a shuriken\n\ndef make_shuriken(x, y):\n     return [[(x, y - 1.5), (x + 0.5, y - 0.5)],\n             [(x - 0.5, y - 0.5), (x, y - 1.5)],\n             [(x + 0.5, y - 0.5), (x + 1.5, y)],\n             [(x + 1.5, y), (x + 0.5, y + 0.5)],\n             [(x + 0.5, y + 0.5), (x, y + 1.5)],\n             [(x, y + 1.5), (x - 0.5, y + 0.5)],\n             [(x - 0.5, y + 0.5), (x - 1.5, y)],\n             [(x - 1.5, y), (x - 0.5, y - 0.5)]\n            ]\n</pre> # A method to return coordinates for the segments of a shuriken  def make_shuriken(x, y):      return [[(x, y - 1.5), (x + 0.5, y - 0.5)],              [(x - 0.5, y - 0.5), (x, y - 1.5)],              [(x + 0.5, y - 0.5), (x + 1.5, y)],              [(x + 1.5, y), (x + 0.5, y + 0.5)],              [(x + 0.5, y + 0.5), (x, y + 1.5)],              [(x, y + 1.5), (x - 0.5, y + 0.5)],              [(x - 0.5, y + 0.5), (x - 1.5, y)],              [(x - 1.5, y), (x - 0.5, y - 0.5)]             ] In\u00a0[3]: Copied! <pre># plot shurikens\nfrom itertools import cycle\ncycol = cycle('bgrcmk')\n\ndef plot_shurikens(shurikens, colors=None):\n    fig, ax = pl.subplots()\n    if not colors:\n        colors = [random.rand(3,) for _ in shurikens]\n    for shuriken, color in zip(shurikens, colors):\n        lc = mc.LineCollection(shuriken, linewidths=2, color=color)\n        ax.add_collection(lc)\n    ax.plot()\n    pl.xlim((-5, 5))\n    pl.ylim((-5, 5))\n    pl.gca().set_aspect('equal', adjustable='box')\n</pre> # plot shurikens from itertools import cycle cycol = cycle('bgrcmk')  def plot_shurikens(shurikens, colors=None):     fig, ax = pl.subplots()     if not colors:         colors = [random.rand(3,) for _ in shurikens]     for shuriken, color in zip(shurikens, colors):         lc = mc.LineCollection(shuriken, linewidths=2, color=color)         ax.add_collection(lc)     ax.plot()     pl.xlim((-5, 5))     pl.ylim((-5, 5))     pl.gca().set_aspect('equal', adjustable='box') In\u00a0[4]: Copied! <pre>shuriken1 = make_shuriken(1, 1)\nshuriken2 = make_shuriken(2, 2)\nplot_shurikens([shuriken1, shuriken2], colors=['blue', 'red'])\n</pre> shuriken1 = make_shuriken(1, 1) shuriken2 = make_shuriken(2, 2) plot_shurikens([shuriken1, shuriken2], colors=['blue', 'red']) <p>Workflow of genetic algorithm:</p> <pre><code>1. Generate population randomly\n&lt;--------&gt;\n&lt;+++-----&gt;\n&lt;---+----&gt;\n&lt;++++++++&gt;\n...\n\n2. Calculate fitness for each chromosome in the population\n&lt;--------&gt; 193\n&lt;+++-----&gt; 12\n&lt;---+----&gt; 34\n&lt;++++++++&gt; 392\n...\n\n3. Pick top individuals\n&lt;--------&gt; 193\n&lt;++++++++&gt; 392\n\n4. crossover to get two children. This generates the next population\n&lt;---+++++&gt;\n&lt;+++-----&gt;\n...\n\n5. Calculate fitness again\n&lt;---+++++&gt; 422\n&lt;+++-----&gt; 235\n\nGo back to 2 and repeat\n</code></pre> <p>You might notice that there's a reduction in population in step 4, but above graph is a simplified representation. In implementation, we take multiple combinations of the best fit parents to produce many children until the population size is the same as previous generation.</p> In\u00a0[5]: Copied! <pre>from numpy import random\n\ndef make_chromosome():\n    x, y = -4 + 8 * random.random(), -4 + 8 * random.random()\n    return [[(x, y - 2 * random.random()), (x + 2 * random.random(), y - 2 * random.random())],\n             [(x - 2 * random.random(), y - 0.5), (x, y - 2 * random.random())],\n             [(x + 2 * random.random(), y - 0.5), (x + 2 * random.random(), y)],\n             [(x + 2 * random.random(), y), (x + 0.5, y + 2 * random.random())],\n             [(x + 2 * random.random(), y + 0.5), (x, y + 2 * random.random())],\n             [(x, y + 2 * random.random()), (x - 0.5, y + 2 * random.random())],\n             [(x - 2 * random.random(), y + 0.5), (x - 2 * random.random(), y)],\n             [(x - 2 * random.random(), y), (x - 0.5, y - 2 * random.random())]\n            ]\n\ndef make_population(size=100):\n    return [make_chromosome() for _ in range(size)]\n\nplot_shurikens(make_population(size=4),\n           ['blue', 'red', 'orange', 'green'])\n</pre> from numpy import random  def make_chromosome():     x, y = -4 + 8 * random.random(), -4 + 8 * random.random()     return [[(x, y - 2 * random.random()), (x + 2 * random.random(), y - 2 * random.random())],              [(x - 2 * random.random(), y - 0.5), (x, y - 2 * random.random())],              [(x + 2 * random.random(), y - 0.5), (x + 2 * random.random(), y)],              [(x + 2 * random.random(), y), (x + 0.5, y + 2 * random.random())],              [(x + 2 * random.random(), y + 0.5), (x, y + 2 * random.random())],              [(x, y + 2 * random.random()), (x - 0.5, y + 2 * random.random())],              [(x - 2 * random.random(), y + 0.5), (x - 2 * random.random(), y)],              [(x - 2 * random.random(), y), (x - 0.5, y - 2 * random.random())]             ]  def make_population(size=100):     return [make_chromosome() for _ in range(size)]  plot_shurikens(make_population(size=4),            ['blue', 'red', 'orange', 'green']) In\u00a0[6]: Copied! <pre>from scipy.spatial.distance import cdist\nfrom copy import deepcopy\n\nshuriken = make_shuriken(1, 1)\n\ndef distance(p1, p2):\n    # distance between two points\n    return ((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2) ** 0.5\n\ndef fitness(chromosome):\n    dist = 0\n    list1 = deepcopy(shuriken)\n    list2 = deepcopy(chromosome)\n    list1.sort(key=lambda x: x[0])\n    list2.sort(key=lambda x: x[0])\n    for i in range(8):\n        dist += distance(list1[i][0], list2[i][0])\n        dist += distance(list1[i][1], list2[i][1])\n    return dist\nfitness(shuriken2)\n</pre> from scipy.spatial.distance import cdist from copy import deepcopy  shuriken = make_shuriken(1, 1)  def distance(p1, p2):     # distance between two points     return ((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2) ** 0.5  def fitness(chromosome):     dist = 0     list1 = deepcopy(shuriken)     list2 = deepcopy(chromosome)     list1.sort(key=lambda x: x[0])     list2.sort(key=lambda x: x[0])     for i in range(8):         dist += distance(list1[i][0], list2[i][0])         dist += distance(list1[i][1], list2[i][1])     return dist fitness(shuriken2) Out[6]: <pre>22.62741699796953</pre> In\u00a0[7]: Copied! <pre>population = make_population(10)\npopulation_fitness = [(chromosome, fitness(chromosome)) for chromosome in population]\nprint([f[1] for f in population_fitness])\nplot_shurikens(population + [shuriken])\n</pre> population = make_population(10) population_fitness = [(chromosome, fitness(chromosome)) for chromosome in population] print([f[1] for f in population_fitness]) plot_shurikens(population + [shuriken]) <pre>[40.666787609293785, 45.670299596809585, 82.28526328078193, 54.44839135656313, 59.7893497391738, 48.942168138652235, 37.291500819971255, 25.956759623441698, 27.60023418787563, 22.21189442824818]\n</pre> In\u00a0[8]: Copied! <pre>population_fitness.sort(key=lambda x: x[1])\nbest = [p[0] for p in population_fitness][:5]\nplot_shurikens(best + [shuriken])\n</pre> population_fitness.sort(key=lambda x: x[1]) best = [p[0] for p in population_fitness][:5] plot_shurikens(best + [shuriken]) In\u00a0[9]: Copied! <pre>def crossover(ch1, ch2):\n    \"\"\"\n    For each gene, pick randomly whether it is from dad or mom\n    \"\"\"\n    new_chromosome = []\n    for i in range(8):\n        if random.random() &lt; 0.5:\n            new_chromosome.append(ch1[i])\n        else:\n            new_chromosome.append(ch2[i])\n    return new_chromosome\n</pre> def crossover(ch1, ch2):     \"\"\"     For each gene, pick randomly whether it is from dad or mom     \"\"\"     new_chromosome = []     for i in range(8):         if random.random() &lt; 0.5:             new_chromosome.append(ch1[i])         else:             new_chromosome.append(ch2[i])     return new_chromosome In\u00a0[10]: Copied! <pre>new_population = []\nfor _ in range(10):\n    # pick two parents randomly\n    idx1, idx2 = random.choice(len(best), 2, replace=False)\n    parent1, parent2 = best[idx1], best[idx2]\n    new_population.append(crossover(parent1, parent2))\n</pre> new_population = [] for _ in range(10):     # pick two parents randomly     idx1, idx2 = random.choice(len(best), 2, replace=False)     parent1, parent2 = best[idx1], best[idx2]     new_population.append(crossover(parent1, parent2)) <p>Not perfect, but the new generation is not as spreaded over as the first population</p> In\u00a0[11]: Copied! <pre>plot_shurikens(new_population + [shuriken])\n</pre> plot_shurikens(new_population + [shuriken]) <p>Let's then evolve over a few generations</p> In\u00a0[12]: Copied! <pre>n_iter = 10\n\ncurrent_genration = best\n\nfor i in range(n_iter):\n    population_fitness = [(chrom, fitness(chrom)) for chrom in current_genration]\n    population_fitness.sort(key=lambda x: x[1])\n    best = [p[0] for p in population_fitness][:5]\n\n    new_population = []\n    for _ in range(10):\n        # pick two parents randomly\n        idx1, idx2 = random.choice(len(best), 2, replace=False)\n        parent1, parent2 = best[idx1], best[idx2]\n        new_population.append(crossover(parent1, parent2))\n    current_genration = new_population\n</pre> n_iter = 10  current_genration = best  for i in range(n_iter):     population_fitness = [(chrom, fitness(chrom)) for chrom in current_genration]     population_fitness.sort(key=lambda x: x[1])     best = [p[0] for p in population_fitness][:5]      new_population = []     for _ in range(10):         # pick two parents randomly         idx1, idx2 = random.choice(len(best), 2, replace=False)         parent1, parent2 = best[idx1], best[idx2]         new_population.append(crossover(parent1, parent2))     current_genration = new_population <p>The population converges to one solution, i.e. local optimal. Several factors affect how far it is from the actual optimal:</p> <ol> <li>How good is the original population?</li> <li>Number of iterations</li> <li>How genetic contents are passed from one to the next generation. In this example, only a simple crossover operation is performed</li> </ol> <p>and many many more</p> In\u00a0[13]: Copied! <pre>plot_shurikens(current_genration + [shuriken])\n</pre> plot_shurikens(current_genration + [shuriken]) In\u00a0[14]: Copied! <pre>def evolve(initial_population, n_iter):\n    population_size = len(initial_population)\n    current_generation = initial_population\n    for i in range(n_iter):\n        population_fitness = [(chrom, fitness(chrom)) for chrom in current_generation]\n        population_fitness.sort(key=lambda x: x[1])\n        best = [p[0] for p in population_fitness][:int(population_size / 4)]\n\n        new_population = []\n        for _ in range(population_size):\n            # pick two parents randomly\n            idx1, idx2 = random.choice(len(best), 2, replace=False)\n            parent1, parent2 = best[idx1], best[idx2]\n            new_population.append(crossover(parent1, parent2))\n        current_generation = new_population\n    return current_generation\n    \ndef optimize(population_size, n_iter):\n    initial_population = make_population(population_size)\n    last_generation = evolve(initial_population, n_iter)\n    population_fitness = [(chrom, fitness(chrom)) for chrom in last_generation]\n    population_fitness.sort(key=lambda x: x[1])\n    return population_fitness[0]\n</pre> def evolve(initial_population, n_iter):     population_size = len(initial_population)     current_generation = initial_population     for i in range(n_iter):         population_fitness = [(chrom, fitness(chrom)) for chrom in current_generation]         population_fitness.sort(key=lambda x: x[1])         best = [p[0] for p in population_fitness][:int(population_size / 4)]          new_population = []         for _ in range(population_size):             # pick two parents randomly             idx1, idx2 = random.choice(len(best), 2, replace=False)             parent1, parent2 = best[idx1], best[idx2]             new_population.append(crossover(parent1, parent2))         current_generation = new_population     return current_generation      def optimize(population_size, n_iter):     initial_population = make_population(population_size)     last_generation = evolve(initial_population, n_iter)     population_fitness = [(chrom, fitness(chrom)) for chrom in last_generation]     population_fitness.sort(key=lambda x: x[1])     return population_fitness[0] In\u00a0[15]: Copied! <pre>def test(pop_size, n_iter):\n    new_shuriken = optimize(pop_size, n_iter)\n    plot_shurikens([new_shuriken[0], shuriken], colors=['blue', 'red'])\n</pre> def test(pop_size, n_iter):     new_shuriken = optimize(pop_size, n_iter)     plot_shurikens([new_shuriken[0], shuriken], colors=['blue', 'red']) In\u00a0[16]: Copied! <pre>test(10, 10)\n</pre> test(10, 10) In\u00a0[17]: Copied! <pre>test(100, 10)\n</pre> test(100, 10) In\u00a0[18]: Copied! <pre>test(100, 50)\n</pre> test(100, 50) In\u00a0[19]: Copied! <pre>test(500, 50)\n</pre> test(500, 50) In\u00a0[20]: Copied! <pre>test(1000, 10)\n</pre> test(1000, 10) In\u00a0[21]: Copied! <pre>test(1000, 30)\n</pre> test(1000, 30) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tech/ml/wooden_wheels/genetic_algorithm/#yet-another-implementation-of-genetic-algorithm","title":"Yet another implementation of genetic algorithm\u00b6","text":""},{"location":"tech/ml/wooden_wheels/genetic_algorithm/#setting-up","title":"Setting up\u00b6","text":""},{"location":"tech/ml/wooden_wheels/genetic_algorithm/#now-write-a-function-to-produce-a-random-chromosome","title":"Now, write a function to produce a random chromosome\u00b6","text":"<p>Here instead of making a shuriken, just draw 8 segments randomly. They don't even need to be connected to each other. Also write a method to create a population of chromosomes.</p>"},{"location":"tech/ml/wooden_wheels/genetic_algorithm/#next-define-fitness-function","title":"Next, define fitness function\u00b6","text":"<p>as distance between the original shuriken and the input segments. For simplicity, simply sum up euclidean distance between the points.</p>"},{"location":"tech/ml/wooden_wheels/genetic_algorithm/#ready-first-population","title":"READY, first population\u00b6","text":""},{"location":"tech/ml/wooden_wheels/genetic_algorithm/#then-we-take-the-top","title":"Then we take the top.\u00b6","text":"<p>Here, best fit = minimal fitness function value for the set up of this problem</p>"},{"location":"tech/ml/wooden_wheels/genetic_algorithm/#generate-next-population","title":"Generate next population\u00b6","text":"<p>by crossing over the top parents</p>"},{"location":"tech/ml/wooden_wheels/genetic_algorithm/#bonus","title":"Bonus\u00b6","text":"<p>Let's put everything together and do some clean up, to make it easier to test a few scenarios</p> <p>We write a function <code>evolve</code> that does a few iterations of evolution and a function <code>optimize</code> that allows you to specify population size and the number of generations.</p> <p>Lastly, a method <code>test</code> to run optimization and then plot the optimization result along with the original shuriken.</p>"},{"location":"tech/ml/wooden_wheels/genetic_algorithm/#then-run-different-combinations-of-population-size-and-number-of-generations","title":"Then run different combinations of population size and number of generations\u00b6","text":"<p>Note that since this method is stochastic, if you run this notebook again, it's almost impossible to get the same results. Also there's no guarantee that larger population and number of iterations will produce better results. They may or may not. In practice, it takes a lot of experimentation to find the most suitable implementation for the problem you are interested in.</p>"},{"location":"tech/ml/wooden_wheels/knn/","title":"K Nearest Neighbors From Scratch","text":"In\u00a0[\u00a0]: Copied!"},{"location":"tech/ml/wooden_wheels/knn/#k-nearest-neighbors-from-scratch","title":"K Nearest Neighbors From Scratch\u00b6","text":"<p>KNN is a conceptually simple but effective algorithm that is commonly used in classification tasks. With existing knowledge about how data points of different classes are located in different clusters, we make predictions on the classes of new data points. Basic requisite for KNN to work well:</p> <ul> <li>Different classes indeed form well-isolated clusters</li> <li>We have a good distance function such that d(p1, p2) is small when p1 and p2 are from the same class and big when p1 and p2 are from different classes.</li> </ul>"},{"location":"tech/ml/wooden_wheels/linear_regression/","title":"Implement a linear regression model by minimizing sum of squared errors","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"darkgrid\")\n</pre> import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt sns.set(style=\"darkgrid\") In\u00a0[2]: Copied! <pre># Generate sample data points\n# let y = 3x + 4\nx = np.random.randint(low=0, high=100, size=100)\ndf = pd.DataFrame({'x': x})\ndf['base_y'] = df.x.apply(lambda x: 3 * x + 4)\ndf['mod_y'] = df.base_y.apply(lambda y: y + 10 * np.random.normal())\n</pre> # Generate sample data points # let y = 3x + 4 x = np.random.randint(low=0, high=100, size=100) df = pd.DataFrame({'x': x}) df['base_y'] = df.x.apply(lambda x: 3 * x + 4) df['mod_y'] = df.base_y.apply(lambda y: y + 10 * np.random.normal()) In\u00a0[3]: Copied! <pre>sns.scatterplot(data=df, x='x', y='mod_y')\n</pre> sns.scatterplot(data=df, x='x', y='mod_y') Out[3]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x11ccd6710&gt;</pre> In\u00a0[4]: Copied! <pre>class LinearRegressor:\n    def __init__(self):\n        self.slope = None\n        self.intercept = None\n    def fit(self, x, y):\n        x_mean = np.mean(x)\n        y_mean = np.mean(y)\n        n = len(x)\n        a_numerator = sum([x_i*y_i for x_i, y_i in zip(x, y)]) - n * x_mean * y_mean\n        a_denominator = sum([x_i**2 for x_i in x]) - n * (x_mean ** 2)\n        self.slope = 1.0 * a_numerator / a_denominator\n        self.intercept = y_mean - self.slope * x_mean\n    def predict(self, x):\n        if self.slope is None:\n            raise ValueError(\"Please fit first\")\n        if isinstance(x, float):\n            return self.slope * x + self.intercept\n        elif isinstance(x, pd.DataFrame):\n            return x.apply(lambda val: val * self.slope + self.intercept)\n        else:\n            return [val * self.slope + self.intercept for val in x]\n    def summary(self):\n        return self.slope, self.intercept\n</pre> class LinearRegressor:     def __init__(self):         self.slope = None         self.intercept = None     def fit(self, x, y):         x_mean = np.mean(x)         y_mean = np.mean(y)         n = len(x)         a_numerator = sum([x_i*y_i for x_i, y_i in zip(x, y)]) - n * x_mean * y_mean         a_denominator = sum([x_i**2 for x_i in x]) - n * (x_mean ** 2)         self.slope = 1.0 * a_numerator / a_denominator         self.intercept = y_mean - self.slope * x_mean     def predict(self, x):         if self.slope is None:             raise ValueError(\"Please fit first\")         if isinstance(x, float):             return self.slope * x + self.intercept         elif isinstance(x, pd.DataFrame):             return x.apply(lambda val: val * self.slope + self.intercept)         else:             return [val * self.slope + self.intercept for val in x]     def summary(self):         return self.slope, self.intercept In\u00a0[5]: Copied! <pre>model = LinearRegressor()\nmodel.fit(df.x, df.mod_y)\ndf['predicted'] = model.predict(df.x)\n</pre> model = LinearRegressor() model.fit(df.x, df.mod_y) df['predicted'] = model.predict(df.x) In\u00a0[6]: Copied! <pre>sns.lineplot(data=df, x='x', y='predicted', color='r')\nsns.scatterplot(data=df, x='x', y='mod_y', alpha=0.8)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\n</pre> sns.lineplot(data=df, x='x', y='predicted', color='r') sns.scatterplot(data=df, x='x', y='mod_y', alpha=0.8) plt.xlabel(\"x\") plt.ylabel(\"y\") plt.show() In\u00a0[7]: Copied! <pre>from mpl_toolkits.mplot3d import axes3d, Axes3D\n\nfig = plt.figure(figsize=(7, 7))\nax = fig.add_subplot(1, 1, 1, projection='3d')\n\n# y = 4 * x1 + 3 * x2\nx1 = np.random.randint(low=0, high=100, size=100)\nx2 = np.random.randint(low=0, high=100, size=100)\ny = [4 * x1_i + 3 * x2_i for x1_i, x2_i in zip(x1, x2)]\ny = [val + 50 * np.random.normal() for val in y]\nax.scatter(x1, x2, y, s=50, alpha=1, edgecolors='w')\n\nax.set_xlabel('X1')\nax.set_ylabel('X2')\nax.set_zlabel('Y')\nplt.show()\n</pre> from mpl_toolkits.mplot3d import axes3d, Axes3D  fig = plt.figure(figsize=(7, 7)) ax = fig.add_subplot(1, 1, 1, projection='3d')  # y = 4 * x1 + 3 * x2 x1 = np.random.randint(low=0, high=100, size=100) x2 = np.random.randint(low=0, high=100, size=100) y = [4 * x1_i + 3 * x2_i for x1_i, x2_i in zip(x1, x2)] y = [val + 50 * np.random.normal() for val in y] ax.scatter(x1, x2, y, s=50, alpha=1, edgecolors='w')  ax.set_xlabel('X1') ax.set_ylabel('X2') ax.set_zlabel('Y') plt.show() <p>Suppose the space is $n + 1$ dimensions and there are $m &gt;= n + 1$ sampless. Each target variable $y$ has the form $y = w_0 + w_1x_1 + w_2x_2 + ... w_nx_n$. Error for each sample is</p> <p>$e^2 = (y - w_0 - w_1x_1 - w_2x_2 - ... w_nx_n)^2$, giving the following overal loss function,</p> <p>$\\Sigma_1^m e_i^2 = \\Sigma_1^m (y_i - w_0 - w_1x_{i1} - w_2x_{i2} - ... -w_nx_{in})^2$</p> <p>Following the same approach as single-variable linear regresion, take derivative w.r.t. each weight and set to 0. We get $n + 1$ equations</p> <p>$$\\Sigma (y_i - w_0 - w_1x_{i1} - w_2x_{i2} ... - w_nx_{in}) = 0$$ $$\\Sigma x_{i1}(y_i - w_0 - w_1x_{i1} - w_2x_{i2} ... - w_nx_{in}) = 0$$ $$...$$ $$\\Sigma x_{in}(y_i - w_0 - w_1x_{i1} - w_2x_{i2} ... - w_nx_{in}) = 0$$</p> <p>A more intuitive arrangement is</p> <p>$$\\Sigma y_i = \\Sigma (w_0 + w_1x_{i1} + w_2x_{i2} ... + w_nx_{in})$$ $$\\Sigma y_i x_{i1} = \\Sigma x_{i1}(w_0 + w_1x_{i1} + w_2x_{i2} ... + w_nx_{in})$$ $$...$$ $$\\Sigma y_i x_{in} = \\Sigma x_{in} (w_0 + w_1x_{i1} + w_2x_{i2} ... + w_nx_{in})$$</p> <p>Putting to matrix form, left hand side is</p> <p>$$ \\begin{bmatrix}  1 &amp; 1 &amp; 1 &amp; ... &amp; 1 \\\\ x_{11} &amp; x_{21} &amp; x_{31} &amp; ... &amp; x_{m1} \\\\ ... \\\\ x_{1n} &amp; x_{2n} &amp; ... &amp; ... &amp; x_{mn} \\end{bmatrix} \\begin{bmatrix}  y_1 \\\\ y_2 \\\\ .\\\\ .\\\\ y_m \\end{bmatrix}  = X^T Y $$</p> <p>Right hand side is</p> <p>$$ \\begin{bmatrix}  \\Sigma 1 &amp; \\Sigma x_{i1} &amp; \\Sigma x_{i2} &amp; ... &amp; \\Sigma x_{in} \\\\ \\Sigma x_{i1} &amp; \\Sigma x_{i1}x_{i1} &amp; \\Sigma x_{i1}x_{i2} &amp; ... &amp; \\Sigma x_{i1}x_{in} \\\\ ... \\\\ \\Sigma x_{in} &amp; \\Sigma x_{in}x_{i1} &amp; ... &amp; ... &amp; \\Sigma x_{in}x_{in} \\end{bmatrix} \\begin{bmatrix}  w_0 \\\\ w_1 \\\\ .\\\\ .\\\\ w_n \\end{bmatrix} =  \\begin{bmatrix}  1 &amp; 1 &amp; 1 &amp; ... &amp; 1 \\\\ x_{11} &amp; x_{21} &amp; x_{31} &amp; ... &amp; x_{m1} \\\\ ... \\\\ x_{1n} &amp; x_{2n} &amp; ... &amp; ... &amp; x_{mn} \\end{bmatrix} \\begin{bmatrix}  1 &amp; x_{11} &amp; x_{12} &amp; ... &amp; x_{1n} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; ... &amp; x_{2n} \\\\ ... \\\\ 1 &amp; x_{m1} &amp; ... &amp; ... &amp; x_{mn} \\end{bmatrix} \\begin{bmatrix}  w_0 \\\\ w_1 \\\\ .\\\\ .\\\\ w_n \\end{bmatrix} = X^T X w $$</p> <p>left = right means $X^T Y = X^T X w$, or $$w = (X^T X)^{-1} X^T Y$$</p> <p>What if $X^T X$ is not invertible? Then there must be redundancy in the variables and there can be multiple solutions. Use professional libraries in such case.</p> In\u00a0[8]: Copied! <pre>class MultvariantRegressor:\n    def __init__(self):\n        self.weights = None\n\n    def fit(self, x, y):\n        if isinstance(x, list):\n            X = np.vstack(x).transpose()\n        else:\n            X = x\n        Y = np.array(y).transpose()\n        inversed = np.linalg.inv(np.matmul(X.transpose(), X))\n        self.weights = np.matmul(np.matmul(inversed, X.transpose()), Y)\n    def predict(self, x):\n        if isinstance(x, list):\n            X = np.vstack(x).transpose()\n        else:\n            X = x\n        return np.matmul(X, self.weights)\n    def summary(self):\n        return self.weights\n</pre> class MultvariantRegressor:     def __init__(self):         self.weights = None      def fit(self, x, y):         if isinstance(x, list):             X = np.vstack(x).transpose()         else:             X = x         Y = np.array(y).transpose()         inversed = np.linalg.inv(np.matmul(X.transpose(), X))         self.weights = np.matmul(np.matmul(inversed, X.transpose()), Y)     def predict(self, x):         if isinstance(x, list):             X = np.vstack(x).transpose()         else:             X = x         return np.matmul(X, self.weights)     def summary(self):         return self.weights In\u00a0[9]: Copied! <pre>model = MultvariantRegressor()\nmodel.fit([x1, x2], y)\npred = model.predict([x1, x2])\n</pre> model = MultvariantRegressor() model.fit([x1, x2], y) pred = model.predict([x1, x2]) In\u00a0[10]: Copied! <pre>fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(1, 1, 1, projection='3d')\n\nax.scatter(x1, x2, y, s=50, alpha=1, edgecolors='w', color='b', label='actual')\nax.scatter(x1, x2, pred, s=50, alpha=0.8, edgecolors='w', color='r', label='predicted')\n\ndef fun(val1, val2):\n    return model.weights[0] * val1 + model.weights[1] * val2\n\nx1_surface = x2_surface = np.arange(0, 120, 20)\nx1_surface_pt, x2_surface_pt = np.meshgrid(x1_surface, x2_surface)\ny_surface = np.array(fun(np.ravel(x1_surface_pt), np.ravel(x2_surface_pt)))\nZ = y_surface.reshape(x1_surface_pt.shape)\n\nax.plot_surface(x1_surface_pt, x2_surface_pt, Z, color='y', alpha=0.2,\n                shade=False)\n\nax.set_xlabel('X1')\nax.set_ylabel('X2')\nax.set_zlabel('Y')\nplt.legend(fontsize=15)\nplt.show()\n</pre> fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(1, 1, 1, projection='3d')  ax.scatter(x1, x2, y, s=50, alpha=1, edgecolors='w', color='b', label='actual') ax.scatter(x1, x2, pred, s=50, alpha=0.8, edgecolors='w', color='r', label='predicted')  def fun(val1, val2):     return model.weights[0] * val1 + model.weights[1] * val2  x1_surface = x2_surface = np.arange(0, 120, 20) x1_surface_pt, x2_surface_pt = np.meshgrid(x1_surface, x2_surface) y_surface = np.array(fun(np.ravel(x1_surface_pt), np.ravel(x2_surface_pt))) Z = y_surface.reshape(x1_surface_pt.shape)  ax.plot_surface(x1_surface_pt, x2_surface_pt, Z, color='y', alpha=0.2,                 shade=False)  ax.set_xlabel('X1') ax.set_ylabel('X2') ax.set_zlabel('Y') plt.legend(fontsize=15) plt.show() <p>Pretty good!</p>"},{"location":"tech/ml/wooden_wheels/linear_regression/#implement-a-linear-regression-model-by-minimizing-sum-of-squared-errors","title":"Implement a linear regression model by minimizing sum of squared errors\u00b6","text":"<p>with friendly explanations</p>"},{"location":"tech/ml/wooden_wheels/linear_regression/#single-variable","title":"Single variable\u00b6","text":"<p>Let's start simple</p>"},{"location":"tech/ml/wooden_wheels/linear_regression/#solution","title":"Solution\u00b6","text":"<p>Suppose the real line is $y = ax + b$, then the sum of squared error is</p> <p>$(ax_1+b-y_1)^2 + (ax_2+b-y_2)^2... + (ax_n+b-y_n)^2$</p> <p>To minimize error w.r.t. $b$, take derivative w.r.t. it and set to 0:</p> <p>$2(ax_1+b-y_1) + 2(ax_2+b-y_2)... + 2(ax_n+b-y_n) = 0$, simplified to</p> <p>$$a(\\Sigma_1^nx_i) + bn - \\Sigma_1^ny_i = an\\bar{x} + bn - n\\bar{y} = 0$$ $$b = \\bar{y} - a\\bar{x}$$</p> <p>To minimize the error with respect to $a$, take derivative w.r.t $a$ and set to 0:</p> <p>$2x_1(ax_1+b-y_1) + 2x_2(ax_2+b-y_2) + ... 2x_n(ax_n + b - y_n) = 0$, simplified to</p> <p>$$a\\Sigma x_i^2 + b\\Sigma x_i - \\Sigma x_iy_i = 0$$</p> <p>Substituting solution for $y$, we get</p> <p>$$a\\Sigma x_i^2 + \\Sigma x_i(\\bar{y} - a\\bar{x}) - \\Sigma x_iy_i = 0$$ $$a(\\Sigma x_i^2 - n\\bar{x}^2) = \\Sigma x_iy_i - n\\bar{y}\\bar{x}$$ $$a = \\frac{\\Sigma x_iy_i - n\\bar{y}\\bar{x}}{\\Sigma x_i^2 - n\\bar{x}^2}$$</p> <p>This means we don't need to \"fit\", we can just calculate!</p>"},{"location":"tech/ml/wooden_wheels/linear_regression/#multivariant-linear-regression","title":"Multivariant linear regression\u00b6","text":"<p>What if we have more than one dimensions in x?</p>"},{"location":"tech/ml/wooden_wheels/perceptron_algorithm/","title":"Perceptron from scratch","text":"<pre><code>make a super naive network like this:\n\nx1---w1\n       \\\nx2---w2--weighted sum--activation function--&gt;y\n       /\n1----w3(bias)\n</code></pre> <p>where we calculate a weighted sum of the input and then use a step function to determine y like so: y = 1 if sum &gt; 0 else 0. The input has a bias term defined by 1*w3 to provide flexibility. Without the bias term, f([0,0]) will always be 0 regardless of the weights, which might not be desired.</p> In\u00a0[310]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\nx_samples = np.random.randint(0, 10, size=(50, 2))\ny_samples = np.random.randint(0, 2, size=50)\n\ndf = pd.DataFrame(x_samples)\ndf.columns = ['x1', 'x2']\ndf['y'] = y_samples\nprint(f\"positive samples: {y_samples.sum()}\")\ndf.head()\n</pre> import numpy as np import pandas as pd from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay import matplotlib.pyplot as plt  x_samples = np.random.randint(0, 10, size=(50, 2)) y_samples = np.random.randint(0, 2, size=50)  df = pd.DataFrame(x_samples) df.columns = ['x1', 'x2'] df['y'] = y_samples print(f\"positive samples: {y_samples.sum()}\") df.head() <pre>positive samples: 22\n</pre> Out[310]: x1 x2 y 0 3 9 0 1 6 1 0 2 1 4 1 3 4 7 0 4 4 3 0 In\u00a0[311]: Copied! <pre>class NaivePerceptron:\n    def __init__(self):\n        self.w = np.random.randn(3)\n\n    def activate(self, weighted_sum):\n        return 1 if weighted_sum &gt; 0 else 0\n\n    def weighted_sum(self, x):\n        with_bias = np.c_[x, np.ones((x.shape[0]))]\n        return np.matmul(with_bias, self.w.transpose())\n\n    def predict_single(self, x):\n        return self.activate(np.dot(x + [1], self.w))\n\n    def predict(self, x):\n        weighted_sums = self.weighted_sum(x)\n        return [self.activate(s) for s in weighted_sums]\n</pre> class NaivePerceptron:     def __init__(self):         self.w = np.random.randn(3)      def activate(self, weighted_sum):         return 1 if weighted_sum &gt; 0 else 0      def weighted_sum(self, x):         with_bias = np.c_[x, np.ones((x.shape[0]))]         return np.matmul(with_bias, self.w.transpose())      def predict_single(self, x):         return self.activate(np.dot(x + [1], self.w))      def predict(self, x):         weighted_sums = self.weighted_sum(x)         return [self.activate(s) for s in weighted_sums] In\u00a0[312]: Copied! <pre>p = NaivePerceptron()\np.predict_single([1,2])\n</pre> p = NaivePerceptron() p.predict_single([1,2]) Out[312]: <pre>1</pre> <p>In this naive version, the weights are generated randomly, so there's no reason to believe it can do any meaningful prediction or find any pattern. We can show ineffectiveness of the naive prception by running the algorithm N times and observe the poor confusion matrices</p> In\u00a0[313]: Copied! <pre>fig, ax = plt.subplots(3, 3, figsize=(10,10))\n\nfor i in range(9):\n    p = NaivePerceptron()\n    predicted = p.predict(x_samples)\n    df['predicted'] = predicted\n    stats = confusion_matrix(y_samples, predicted)\n    \n    plot = ConfusionMatrixDisplay(confusion_matrix=stats)\n    plot.plot(ax=ax[int(i/3)][i%3])\nplt.show()\n</pre> fig, ax = plt.subplots(3, 3, figsize=(10,10))  for i in range(9):     p = NaivePerceptron()     predicted = p.predict(x_samples)     df['predicted'] = predicted     stats = confusion_matrix(y_samples, predicted)          plot = ConfusionMatrixDisplay(confusion_matrix=stats)     plot.plot(ax=ax[int(i/3)][i%3]) plt.show() In\u00a0[314]: Copied! <pre>class SmartPerceptron(NaivePerceptron):\n    def __init__(self, learning_rate=0.1):\n        self.learning_rate = learning_rate\n        super().__init__()\n\n    def fit(self, x, y, epochs, verbose=False):\n        for i in range(epochs):\n            total_err = 0\n            for x_i, y_i in zip(x, y):\n                total_err += abs(self.fit_single(x_i, y_i))\n            if verbose:\n                print(f\"error for epoch {i+1}: {total_err}\")\n\n    def fit_single(self, x, y):\n        pred = self.predict_single(x.tolist())\n        err = y - pred\n        x_with_bias = np.array(x.tolist() + [1])\n        self.w += self.learning_rate * err * x_with_bias\n        return err\n</pre> class SmartPerceptron(NaivePerceptron):     def __init__(self, learning_rate=0.1):         self.learning_rate = learning_rate         super().__init__()      def fit(self, x, y, epochs, verbose=False):         for i in range(epochs):             total_err = 0             for x_i, y_i in zip(x, y):                 total_err += abs(self.fit_single(x_i, y_i))             if verbose:                 print(f\"error for epoch {i+1}: {total_err}\")      def fit_single(self, x, y):         pred = self.predict_single(x.tolist())         err = y - pred         x_with_bias = np.array(x.tolist() + [1])         self.w += self.learning_rate * err * x_with_bias         return err <p>In the fit_single method, we move w by <code>self.w += self.learning_rate * err * x_with_bias</code>. Inuitively, the direction in which we adjust the weight is determined by whether we over- or under-estimate y. The magnitude of adjustment depends on the learning rate and the value of x. Why do we need to scale it by x? Think of it this way: if we are below the activation threshold even when x is large, then we need to increase w a lot to move the needle.</p> <p>Check the error at each epoch. We expect it to roughly reduce at each epoch.</p> In\u00a0[315]: Copied! <pre>p = SmartPerceptron()\np.fit(x_samples, y_samples, 10, verbose=True)\npred = p.predict(x_samples)\n</pre> p = SmartPerceptron() p.fit(x_samples, y_samples, 10, verbose=True) pred = p.predict(x_samples) <pre>error for epoch 1: 24\nerror for epoch 2: 20\nerror for epoch 3: 21\nerror for epoch 4: 21\nerror for epoch 5: 25\nerror for epoch 6: 21\nerror for epoch 7: 21\nerror for epoch 8: 26\nerror for epoch 9: 23\nerror for epoch 10: 26\n</pre> <p>Now check how the smart perceptron performs. For each perceptron, we look at the final results after trianing for 20 epoches.</p> In\u00a0[317]: Copied! <pre>fig, ax = plt.subplots(3, 3, figsize=(10,10))\n\nfor i in range(9):\n    p = SmartPerceptron()\n    p.fit(x_samples, y_samples, 5)\n    predicted = p.predict(x_samples)\n    df['predicted'] = predicted\n    stats = confusion_matrix(y_samples, predicted)\n    \n    plot = ConfusionMatrixDisplay(confusion_matrix=stats)\n    plot.plot(ax=ax[int(i/3)][i%3])\nplt.show()\n</pre> fig, ax = plt.subplots(3, 3, figsize=(10,10))  for i in range(9):     p = SmartPerceptron()     p.fit(x_samples, y_samples, 5)     predicted = p.predict(x_samples)     df['predicted'] = predicted     stats = confusion_matrix(y_samples, predicted)          plot = ConfusionMatrixDisplay(confusion_matrix=stats)     plot.plot(ax=ax[int(i/3)][i%3]) plt.show() <p>Interestingly, all converge to a naive approach of just assigning the same label for all samples! This is due to the randomness of the samples in the first place and the fact that this is a super small network. It's already doing the best it could.</p> <p>What if we artificially generate a set of samples that actually have a clear and simple class boundary? Let's generate some samples under the rules by the perceptron algorithm, based on w and activation function. Execute below until we get a relatively unbiased set of samples, e.g. not all samples are positive.</p> In\u00a0[338]: Copied! <pre>x_samples_smart = np.random.randint(0, 10, size=(50, 2))\n\nw = np.random.randn(3)\n\nwith_bias = np.c_[x_samples_smart, np.ones((x_samples_smart.shape[0]))]\nweighted_sum = np.matmul(with_bias, w.transpose())\n\ndef activate(s):\n    return 1 if s &gt; 0 else 0\n\ny_samples_smart = [activate(s) for s in weighted_sum]\nprint(f\"positive samples: {np.array(y_samples_smart).sum()}\")\n</pre> x_samples_smart = np.random.randint(0, 10, size=(50, 2))  w = np.random.randn(3)  with_bias = np.c_[x_samples_smart, np.ones((x_samples_smart.shape[0]))] weighted_sum = np.matmul(with_bias, w.transpose())  def activate(s):     return 1 if s &gt; 0 else 0  y_samples_smart = [activate(s) for s in weighted_sum] print(f\"positive samples: {np.array(y_samples_smart).sum()}\") <pre>positive samples: 32\n</pre> <p>As before, see what happens with untrained perceptron. It does pretty bad!</p> In\u00a0[339]: Copied! <pre>fig, ax = plt.subplots(3, 3, figsize=(10,10))\n\nfor i in range(9):\n    p = NaivePerceptron()\n    predicted = p.predict(x_samples_smart)\n    df['predicted'] = predicted\n    stats = confusion_matrix(y_samples_smart, predicted)\n    plot = ConfusionMatrixDisplay(confusion_matrix=stats)\n    plot.plot(ax=ax[int(i/3)][i%3])\nplt.show()\n</pre> fig, ax = plt.subplots(3, 3, figsize=(10,10))  for i in range(9):     p = NaivePerceptron()     predicted = p.predict(x_samples_smart)     df['predicted'] = predicted     stats = confusion_matrix(y_samples_smart, predicted)     plot = ConfusionMatrixDisplay(confusion_matrix=stats)     plot.plot(ax=ax[int(i/3)][i%3]) plt.show() <p>With training, it does fairly well with 30 epochs.</p> In\u00a0[340]: Copied! <pre>fig, ax = plt.subplots(3, 3, figsize=(10,10))\n\nfor i in range(9):\n    p = SmartPerceptron()\n    p.fit(x_samples_smart, y_samples_smart, 30)\n    predicted = p.predict(x_samples_smart)\n    df['predicted'] = predicted\n    stats = confusion_matrix(y_samples_smart, predicted)\n    \n    plot = ConfusionMatrixDisplay(confusion_matrix=stats)\n    plot.plot(ax=ax[int(i/3)][i%3])\nplt.show()\n</pre> fig, ax = plt.subplots(3, 3, figsize=(10,10))  for i in range(9):     p = SmartPerceptron()     p.fit(x_samples_smart, y_samples_smart, 30)     predicted = p.predict(x_samples_smart)     df['predicted'] = predicted     stats = confusion_matrix(y_samples_smart, predicted)          plot = ConfusionMatrixDisplay(confusion_matrix=stats)     plot.plot(ax=ax[int(i/3)][i%3]) plt.show() <p>This concludes perceptron, the foundational unit of neural network</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tech/ml/wooden_wheels/perceptron_algorithm/#perceptron-from-scratch","title":"Perceptron from scratch\u00b6","text":"<p>Perceptron is the simplest form of artificial neural network used for binary classifcation. In this notebook we'll implement one from scratch. This can help us understand the foundations of neural network.</p>"},{"location":"tech/ml/wooden_wheels/perceptron_algorithm/#problem-to-be-solved","title":"Problem to be solved\u00b6","text":"<p>Create an artificial data set where we want to predict the binary classification y from samples x generated randomly. (Note that this is a random problem by itself so we don't expected a perfect fit).</p>"},{"location":"tech/ml/wooden_wheels/perceptron_algorithm/#implement-a-naive-perceptron","title":"Implement a naive perceptron\u00b6","text":"<p>weighted sum is simply $\\Sigma_{i} x_i w_i$, the dot product of input x and weight vector w. When taking an array of inputs, we can use marix multiplication to generate weighted sums for all samples.</p>"},{"location":"tech/ml/wooden_wheels/perceptron_algorithm/#add-training-methods-to-the-perceptron","title":"Add training methods to the perceptron\u00b6","text":"<p>To improve the perceptron, we want it to adjust its weights while going through training samples one by one. We need to add a learning rate to determine how fast we go with gradient descent.</p> <p>To do gradient descent, we first calculate the difference between predicted and expected classification. This can only be 0, -1, or 1 and essentially means a direction rather than a magnitude. The magnitude is determined by x itself, tuned by the learning rate.</p> <p>To gain an intuition, suppose y=1 and prediction=0, this means the weighted sum is below the activation threshold. w_i needs to be bigger whenever x_i is positive and smaller whenever x_i is negative. So if we set the error term as y_true - y_pred, then we want to move towards the direction of this term.</p>"},{"location":"tech/ml/wooden_wheels/perceptron_algorithm/#try-a-different-set-of-samples","title":"Try a different set of samples\u00b6","text":""},{"location":"tech/ml/wooden_wheels/random_forest/","title":"Implement random forest from scratch, start from decision treee","text":"In\u00a0[1]: Copied! <pre>import math\nimport numpy as np\nimport pandas as pd\n\ndef entropy(Y):\n    \"\"\"\n    sum of p0 log2(p0) + p1 log2(p1)\n    \"\"\"\n    total_ent = 0\n    size = len(Y)\n    p = sum([y == 1 for y in Y]) / size\n    if p == 0 or p == 1:\n        return 0\n    else:\n        return -p * math.log(p, 2) - (1 - p) * math.log(1 - p, 2)\n</pre> import math import numpy as np import pandas as pd  def entropy(Y):     \"\"\"     sum of p0 log2(p0) + p1 log2(p1)     \"\"\"     total_ent = 0     size = len(Y)     p = sum([y == 1 for y in Y]) / size     if p == 0 or p == 1:         return 0     else:         return -p * math.log(p, 2) - (1 - p) * math.log(1 - p, 2) <p>Sanity check entropy calculation. It should be like a bell</p> In\u00a0[3]: Copied! <pre>import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nsamples = [[0] * x + [1] * (100 - x) for x in range(100)]\nproportions = [sum(s) / 100 for s in samples]\nentropies = [entropy(s) for s in samples]\nplt.plot()\nplt.scatter(proportions, entropies)\nplt.xlabel(\"Proportion of class 1\", fontsize=15)\nplt.ylabel(\"Entropy\", fontsize=15)\nplt.show()\n</pre> import matplotlib.pyplot as plt import seaborn as sns sns.set_style('darkgrid')  samples = [[0] * x + [1] * (100 - x) for x in range(100)] proportions = [sum(s) / 100 for s in samples] entropies = [entropy(s) for s in samples] plt.plot() plt.scatter(proportions, entropies) plt.xlabel(\"Proportion of class 1\", fontsize=15) plt.ylabel(\"Entropy\", fontsize=15) plt.show() <p>An example of how we can use entropy reduction to guide spliting</p> In\u00a0[4]: Copied! <pre>before = [0] * 10 + [1] * 10\nprint(\"Entropy before splitting:\", entropy(before))\n\nsplit1 = [0] * 8 + [1] *2\nsplit2 = [0] * 2 + [1] * 8\nentropy_good = entropy(split1) * len(split1) / len(before) \\\n    + entropy(split2) * len(split2) / len(before)\nprint(\"Entropy after a good split\", entropy_good)\n\nsplit1 = [0] * 6 + [1] *4\nsplit2 = [0] * 4 + [1] * 6\nentropy_bad = entropy(split1) * len(split1) / len(before) \\\n    + entropy(split2) * len(split2) / len(before)\nprint(\"Entropy after a bad split\", entropy_bad)\n</pre> before = [0] * 10 + [1] * 10 print(\"Entropy before splitting:\", entropy(before))  split1 = [0] * 8 + [1] *2 split2 = [0] * 2 + [1] * 8 entropy_good = entropy(split1) * len(split1) / len(before) \\     + entropy(split2) * len(split2) / len(before) print(\"Entropy after a good split\", entropy_good)  split1 = [0] * 6 + [1] *4 split2 = [0] * 4 + [1] * 6 entropy_bad = entropy(split1) * len(split1) / len(before) \\     + entropy(split2) * len(split2) / len(before) print(\"Entropy after a bad split\", entropy_bad) <pre>Entropy before splitting: 1.0\nEntropy after a good split 0.7219280948873623\nEntropy after a bad split 0.9709505944546686\n</pre> <p>For any given feature, we want to find the split where entropy reduction is maximized. Iterate through all possible splitting point to do so. To find the best feature to split first, do an exaustive search for all splitting points of all features.</p> <p>First implement a method to find a optimal splitting point for one feature</p> In\u00a0[5]: Copied! <pre>def split_entropy(values, labels):\n    zipped = list(zip(values, labels))\n    zipped.sort(key=lambda x: x[0])\n    best_split, best_entr = values[0], entropy(labels)\n    size = len(labels)\n    for point in zipped:\n        # split samples based on split value\n        small_points = [p[1] for p in zipped if p[0] &lt;= point[0]]\n        large_points = [p[1] for p in zipped if p[0] &gt; point[0]]\n        if not small_points or not large_points:\n            continue\n        # calculate new entropy\n        sum_entropy = entropy(small_points) * len(small_points) / size\\\n            + entropy(large_points) * len(large_points) / size\n        # update best entropy if necessary\n        if sum_entropy &lt; best_entr:\n            best_split, best_entr = point[0], sum_entropy\n    return best_split, best_entr\n\n# Example\nsplit_entropy([1, 2, 3, 4], [0, 0, 0, 0])\n</pre> def split_entropy(values, labels):     zipped = list(zip(values, labels))     zipped.sort(key=lambda x: x[0])     best_split, best_entr = values[0], entropy(labels)     size = len(labels)     for point in zipped:         # split samples based on split value         small_points = [p[1] for p in zipped if p[0] &lt;= point[0]]         large_points = [p[1] for p in zipped if p[0] &gt; point[0]]         if not small_points or not large_points:             continue         # calculate new entropy         sum_entropy = entropy(small_points) * len(small_points) / size\\             + entropy(large_points) * len(large_points) / size         # update best entropy if necessary         if sum_entropy &lt; best_entr:             best_split, best_entr = point[0], sum_entropy     return best_split, best_entr  # Example split_entropy([1, 2, 3, 4], [0, 0, 0, 0]) Out[5]: <pre>(1, 0)</pre> In\u00a0[6]: Copied! <pre>class Node:\n    def __init__(self):\n        self.feature = None\n        self.split = None\n        # left/right is either a node or a value. If it's a value then this node\n        # is a leaf\n        self.left = None\n        self.right = None\n    def move(self, val):\n        \"\"\"Move to left or right by comparing sample value to the splitting point\"\"\"\n        if val &lt;= self.split:\n            return self.left\n        else:\n            return self.right\n    def height(self):\n        \"\"\"Get the height of the tree rooted at this node\"\"\"\n        if isinstance(self.left, float):\n            return 1\n        else:\n            return 1 + max([self.left.height(), self.right.height()])\n</pre> class Node:     def __init__(self):         self.feature = None         self.split = None         # left/right is either a node or a value. If it's a value then this node         # is a leaf         self.left = None         self.right = None     def move(self, val):         \"\"\"Move to left or right by comparing sample value to the splitting point\"\"\"         if val &lt;= self.split:             return self.left         else:             return self.right     def height(self):         \"\"\"Get the height of the tree rooted at this node\"\"\"         if isinstance(self.left, float):             return 1         else:             return 1 + max([self.left.height(), self.right.height()]) In\u00a0[7]: Copied! <pre>class DecisionTree:\n    def __init__(self):\n        self.root = None\n\n    def fit(self, df, features, target):\n        self.root = self._make_node(df, features, target)\n\n    def predict(self, df):\n        # transverse the tree for each data point\n        pred = df.apply(lambda row: self._transverse(self.root, row), axis=1)\n        return pred\n\n    def _best_feature(self, df, features, target):\n        \"\"\"\n        return the best feature given a set of feature values and a target.\n        Store everything in a dataframe and let user specify which feature\n        columns to use and which columns corresponds to the binary target\n        \"\"\"\n        best_split, best_entro, best_feature = None, None, None\n        # Try all features. At each iteration update the best option.\n        for feature in features:\n            this_split, this_entro = split_entropy(df[feature].tolist(), df[target])\n            if best_split is None or best_entro &gt; this_entro:\n                best_split, best_entro, best_feature = this_split, this_entro, feature\n        return best_split, best_feature\n\n    def _make_node(self, df, features, target):\n        node = Node()\n        # Use the best splitting value of the best feature out of all.\n        node.split, node.feature = self._best_feature(df, features, target)\n        # Divide the samples into two subsets and use them to continue\n        # subtree construction\n        small_points = df[df[node.feature] &lt;= node.split]\n        large_points = df[df[node.feature] &gt; node.split]\n        # if No more split, return most frequent target value\n        if small_points.shape[0] == 0 or large_points.shape[0] == 0:\n            node.left = float(df[target].mode().iloc[0])\n            node.right = float(df[target].mode().iloc[0])\n        else:\n            node.left = self._make_node(small_points, features, target)\n            node.right = self._make_node(large_points, features, target)\n        return node\n\n    def _walk_node(self, node, row):\n        # Go to the next decision point or leaf\n        return node.move(row[node.feature])\n\n    def _transverse(self, node, row):\n        # Transverse nodes until a target (leaf) value is reached\n        next_location = self._walk_node(node, row)\n        while not isinstance(next_location, float):\n            next_location = self._walk_node(next_location, row)\n        return next_location\n\n    def depth(self):\n        return self.root.height()\n</pre> class DecisionTree:     def __init__(self):         self.root = None      def fit(self, df, features, target):         self.root = self._make_node(df, features, target)      def predict(self, df):         # transverse the tree for each data point         pred = df.apply(lambda row: self._transverse(self.root, row), axis=1)         return pred      def _best_feature(self, df, features, target):         \"\"\"         return the best feature given a set of feature values and a target.         Store everything in a dataframe and let user specify which feature         columns to use and which columns corresponds to the binary target         \"\"\"         best_split, best_entro, best_feature = None, None, None         # Try all features. At each iteration update the best option.         for feature in features:             this_split, this_entro = split_entropy(df[feature].tolist(), df[target])             if best_split is None or best_entro &gt; this_entro:                 best_split, best_entro, best_feature = this_split, this_entro, feature         return best_split, best_feature      def _make_node(self, df, features, target):         node = Node()         # Use the best splitting value of the best feature out of all.         node.split, node.feature = self._best_feature(df, features, target)         # Divide the samples into two subsets and use them to continue         # subtree construction         small_points = df[df[node.feature] &lt;= node.split]         large_points = df[df[node.feature] &gt; node.split]         # if No more split, return most frequent target value         if small_points.shape[0] == 0 or large_points.shape[0] == 0:             node.left = float(df[target].mode().iloc[0])             node.right = float(df[target].mode().iloc[0])         else:             node.left = self._make_node(small_points, features, target)             node.right = self._make_node(large_points, features, target)         return node      def _walk_node(self, node, row):         # Go to the next decision point or leaf         return node.move(row[node.feature])      def _transverse(self, node, row):         # Transverse nodes until a target (leaf) value is reached         next_location = self._walk_node(node, row)         while not isinstance(next_location, float):             next_location = self._walk_node(next_location, row)         return next_location      def depth(self):         return self.root.height() In\u00a0[8]: Copied! <pre>from sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import accuracy_score\n\ndata = load_breast_cancer()\ndf = pd.DataFrame(data['data'])\ncolumns = [c for c in df.columns]\ndf['target'] = data['target']\n# Count values to make sure accuracy makes sense (balanced class)\ndf['target'].value_counts()\n</pre> from sklearn.datasets import load_breast_cancer from sklearn.metrics import accuracy_score  data = load_breast_cancer() df = pd.DataFrame(data['data']) columns = [c for c in df.columns] df['target'] = data['target'] # Count values to make sure accuracy makes sense (balanced class) df['target'].value_counts() Out[8]: <pre>1    357\n0    212\nName: target, dtype: int64</pre> <p>Fit the tree with the entire sample. This should create a HUGE tree that is 100% overfit and gives 100% accuracy, since the tree keeps making nodes until all samples are settled into homogeneous leaves.</p> In\u00a0[9]: Copied! <pre>tree = DecisionTree()\ntree.fit(df, features=columns, target='target')\n</pre> tree = DecisionTree() tree.fit(df, features=columns, target='target') In\u00a0[10]: Copied! <pre>pred = tree.predict(df)\nacc = accuracy_score(y_pred=pred, y_true=df['target'])\nprint(\"Accuracy is\", acc)\nprint(\"Tree depth is\", tree.depth())\n</pre> pred = tree.predict(df) acc = accuracy_score(y_pred=pred, y_true=df['target']) print(\"Accuracy is\", acc) print(\"Tree depth is\", tree.depth()) <pre>Accuracy is 1.0\nTree depth is 11\n</pre> <p>Fit the tree with a subset of samples so that we can set aside test data. Compare performance on test vs training data to confirm overfitting.</p> In\u00a0[11]: Copied! <pre>tree = DecisionTree()\ntrain = df.head(200)\ntest = df.tail(300)\ntree.fit(train, features=columns, target='target')\npred = tree.predict(test)\nacc = accuracy_score(y_pred=pred, y_true=test['target'])\nprint(\"Accuracy is\", acc)\nprint(\"Tree depth is\", tree.depth())\n</pre> tree = DecisionTree() train = df.head(200) test = df.tail(300) tree.fit(train, features=columns, target='target') pred = tree.predict(test) acc = accuracy_score(y_pred=pred, y_true=test['target']) print(\"Accuracy is\", acc) print(\"Tree depth is\", tree.depth()) <pre>Accuracy is 0.83\nTree depth is 10\n</pre> In\u00a0[12]: Copied! <pre>class SimpleForest:\n    def __init__(self, n_trees=100):\n        self.trees = [DecisionTree() for _ in range(n_trees)]\n    \n    def fit(self, df, features, target):\n        for tree in self.trees:\n            subset = df.sample(frac=0.66)\n            sub_features = np.random.choice(features, int(2 * len(features) / 3))\n            tree.fit(subset, sub_features, target)\n    \n    def predict(self, df):\n        pred_list = [tree.predict(df) for tree in self.trees]\n        combined = pd.concat(pred_list, axis=1)\n        return combined.mode(axis=1).iloc[:, 0].tolist()\n</pre> class SimpleForest:     def __init__(self, n_trees=100):         self.trees = [DecisionTree() for _ in range(n_trees)]          def fit(self, df, features, target):         for tree in self.trees:             subset = df.sample(frac=0.66)             sub_features = np.random.choice(features, int(2 * len(features) / 3))             tree.fit(subset, sub_features, target)          def predict(self, df):         pred_list = [tree.predict(df) for tree in self.trees]         combined = pd.concat(pred_list, axis=1)         return combined.mode(axis=1).iloc[:, 0].tolist() <p>The following will take a few minutes due to inefficient implementation</p> In\u00a0[13]: Copied! <pre>forest = SimpleForest()\nforest.fit(train, features=columns, target='target')\n</pre> forest = SimpleForest() forest.fit(train, features=columns, target='target') In\u00a0[14]: Copied! <pre>pred = forest.predict(test)\n</pre> pred = forest.predict(test) In\u00a0[15]: Copied! <pre>pred = forest.predict(train)\nacc = accuracy_score(y_pred=pred, y_true=train['target'])\n\npred_tree = tree.predict(train)\ntree_acc = accuracy_score(y_pred=pred_tree, y_true=train['target'])\n\nprint(\"Training accuracy of random forest:\", acc)\nprint(\"Training accuracy of decision tree:\", tree_acc)\n\npred = forest.predict(test)\nacc = accuracy_score(y_pred=pred, y_true=test['target'])\n\npred_tree = tree.predict(test)\ntree_acc = accuracy_score(y_pred=pred_tree, y_true=test['target'])\n\nprint(\"Test accuracy of random forest:\", acc)\nprint(\"Test accuracy of decision tree:\", tree_acc)\n</pre> pred = forest.predict(train) acc = accuracy_score(y_pred=pred, y_true=train['target'])  pred_tree = tree.predict(train) tree_acc = accuracy_score(y_pred=pred_tree, y_true=train['target'])  print(\"Training accuracy of random forest:\", acc) print(\"Training accuracy of decision tree:\", tree_acc)  pred = forest.predict(test) acc = accuracy_score(y_pred=pred, y_true=test['target'])  pred_tree = tree.predict(test) tree_acc = accuracy_score(y_pred=pred_tree, y_true=test['target'])  print(\"Test accuracy of random forest:\", acc) print(\"Test accuracy of decision tree:\", tree_acc) <pre>Training accuracy of random forest: 1.0\nTraining accuracy of decision tree: 1.0\nTest accuracy of random forest: 0.93\nTest accuracy of decision tree: 0.83\n</pre> <p>This data set is simple enough that random forest manages to get 100% accuracy on training samples and is still robust against unseen test samples. In most cases, it does not achieve 100% training accuracy, but that's not a problem and probably a good thing.</p> <p>Optimizations of decision tree can be used to random forest as well. Besides, random forest has additional optimization such as</p> <ul> <li>Number of trees</li> <li>Number of features to be sampled at each tree</li> <li>How sampling is done for fitting each tree</li> <li>Voting method at prediction time</li> <li>Adding different weights to trees</li> <li>More...</li> </ul>"},{"location":"tech/ml/wooden_wheels/random_forest/#implement-random-forest-from-scratch-start-from-decision-treee","title":"Implement random forest from scratch, start from decision treee\u00b6","text":""},{"location":"tech/ml/wooden_wheels/random_forest/#decision-tree","title":"Decision tree\u00b6","text":"<p>Decision tree is a machine learning method that makes sequential binary choices at each step based on one feature value per step, until a final value is assigned to the target variable.</p> <p>A decision tree on whether to date or not</p> <pre><code>        weather\n        /     \\\n      cold   hot\n      /         \\\n    location   time\n    /  \\        / \\\n   in  out  early  late\n  /      \\    /     \\\n Yes     No  No    Yes\n</code></pre> <p>It's called decision tree because one needs to transverse through a tree to make a final decision. Machine model comes in when figuring out how the tree should be constructed, i.e. what features to use and what values are used to split at each features.</p>"},{"location":"tech/ml/wooden_wheels/random_forest/#entropy-where-to-split","title":"Entropy &amp; Where to split?\u00b6","text":"<p>At each step, ideally we split a heterogeneous pool into two homogeneous pools. To get close to that, we aim to reduce total entropy. This measure is also called information gain. We pick the value such that information gain is maximized when split optimally.</p> <p>Assuming binary classification, entropy is $$Entropy = -p_0\\log_2 p_0 - p_1\\log_2 p_1$$ Where $p_0, p_1$ is the proprotion of samples in the two classes respectively.</p>"},{"location":"tech/ml/wooden_wheels/random_forest/#tree-nodes","title":"Tree Nodes\u00b6","text":"<p>We use nodes to represent each decision point. The decision to branch left or right is determined by the feature value of the sample. For each node, we need to know which feature it's associated with and what value to use as the splitting point. For analytic purpose later, also make a <code>height</code> method to get height of the subtree rooted at the node.</p>"},{"location":"tech/ml/wooden_wheels/random_forest/#decision-tree","title":"Decision Tree\u00b6","text":"<p>A basic ID3 decision tree model is no more than a wrapper class that holds a tree. During training phase, it deterministically constructs nodes one by one, picking the best splitting value of the best feature at each iteration. This process is greedy and exaustive. To make a prediction, it transverse through the entire tree based on samples' values at each feature, until a leaf is reached.</p>"},{"location":"tech/ml/wooden_wheels/random_forest/#plant-some-trees","title":"Plant some trees\u00b6","text":"<p>Use above simple tree to fit breast cancer data, a binary classification problem</p>"},{"location":"tech/ml/wooden_wheels/random_forest/#overcome-overfitting","title":"Overcome overfitting\u00b6","text":"<p>There are many ways to avoid overfitting a tree, such as</p> <ul> <li>Limit tree depth</li> <li>Stop node construction once there are fewer than X number of samples in either split</li> <li>Prune the tree later by removing leaves that do not contribute to prediction much</li> <li>Use a subset of features</li> <li>Fit on a subset of samples</li> <li>Set a lower bound of information gain to continue node construction</li> <li>Many more!</li> </ul> <p>Those are all trade-offs between bias and variation to avoid fitting on errors in the samples. Here's where a technique called <code>random forest</code> comes in. The general idea is that by creating a forest of trees and all of them to make prediction, then we mitigate overfitting. This is an ensemble method, where multiple estimators collectively make a decision on the outcome.</p>"},{"location":"tech/ml/wooden_wheels/random_forest/#a-simple-random-forest-model","title":"A Simple Random Forest Model\u00b6","text":"<p>To demonstrate that random forest enhances the power of decision tree, let's implement a classifier with 100 trees. Instead of fitting all samples with all features, each tree is fed with 66% of traning samples and 2/3 of the features. See what happens</p>"},{"location":"tech/ml/wooden_wheels/recurrent_neural_net/","title":"Recurrent Neural Net From Scratch","text":"<p>I will skip the reasoning behind why we need RNN. There's plenty on the internet. I'll focus on learning the technical side, and hope that the answers to \"why\"s will come naturally. This is the same principal as other notebooks in my \"from scratch\" series. This is heavily based on the basic neural net, so read that one first.</p> <p>Recall that in a neural network, we have:</p> <p></p> <p>For recurrent neural network, we carry over info from the previous data point in the sequence, by incorporating the values of the hidden layer, with its own weight $u$.</p> <p></p> <p>$$ \\large \\frac{\\partial L_t}{\\partial V} = (\\hat{y_t} - y_t) \\cdot a_{h,t} $$</p> <p>Replace $a_h$ with 1 for the bias term</p> <p>From above setup, we can see that $s_{h,t}$ is an important term as it's a function of $W, U, a_{h,t-1}$. In backpropagation, the gradient will flow from $s_{h,t}$ to those components like the following, based on the formula in the feedforward section above:</p> <p>$$\\large \\begin{align} \\frac{\\partial L_t}{\\partial W} &amp;= \\frac{\\partial L_t}{\\partial s_{h,t}} \\cdot x_t \\\\ \\frac{\\partial L_t}{\\partial U} &amp;= \\frac{\\partial L_t}{\\partial s_{h,t}} \\cdot a_{h,t-1} \\\\ \\frac{\\partial L_t}{\\partial b_h} &amp;= \\frac{\\partial L_t}{\\partial s_{h,t}}  \\end{align} $$</p> <p>To get $\\Large \\frac{\\partial L_t}{\\partial s_{h,t}}$, we have</p> <p>$$\\large \\begin{align} \\frac{\\partial L_t}{\\partial s_{h,t}} &amp; = \\frac{\\partial L_t}{\\partial a_{h,t}}\\cdot \\frac{\\partial a_{h,t}}{\\partial s_{h,t}} \\\\ &amp; = \\frac{\\partial L_t}{\\partial a_{h,t}}\\cdot \\frac{\\partial \\sigma_h(s_{h,t})}{\\partial s_{h,t}} \\\\ &amp; = \\frac{\\partial L_t}{\\partial a_{h,t}}\\cdot (1-(tanh(s_{h,t}))^2)  \\end{align} $$</p> <p>All we have left to derive is $\\Large \\frac{\\partial L_t}{\\partial a_{h,t}}$. As $\\large a_{h,t}$ contributes to $L_{t+1}$, the gradient from $L_{t+1}$ also goes to $\\large a_{h,t}$. Therefore</p> <p>$$\\large \\begin{align} \\frac{\\partial L_t}{\\partial a_{h,t}} &amp;= \\frac{\\partial L_t}{\\partial s_{o,t}} \\cdot \\frac{\\partial s_{o,t}}{\\partial a_{h,t}} + \\frac{\\partial L_{t+1}}{\\partial s_{h,t+1}} \\cdot \\frac{\\partial s_{h,t+1}}{\\partial a_{h,t}} \\\\ &amp;= (\\hat{y_t} - y_t)\\cdot V + \\frac{\\partial L_{t+1}}{\\partial s_{h,t+1}} \\cdot U \\end{align} $$</p> In\u00a0[1]: Copied! <pre>import numpy as np\n\n# length of the sequence is the length of the problem. Keep it not too big for calculation, but not too small to be boring.\nlength = 50\n\nx_num = [np.random.randint(0, 3) for i in range(length)]\ny_num = [(x+1) % 3 for x in x_num]\nx_str = ''.join([str(i) for i in x_num])\ny_str = ''.join([str(i) for i in y_num])\nprint('input: ', x_str)\nprint('output:', y_str)\n</pre> import numpy as np  # length of the sequence is the length of the problem. Keep it not too big for calculation, but not too small to be boring. length = 50  x_num = [np.random.randint(0, 3) for i in range(length)] y_num = [(x+1) % 3 for x in x_num] x_str = ''.join([str(i) for i in x_num]) y_str = ''.join([str(i) for i in y_num]) print('input: ', x_str) print('output:', y_str) <pre>input:  11220020221211221211112201122011202120001210122112\noutput: 22001101002022002022220012200122010201112021200220\n</pre> <p>Repeat to make a dataset</p> In\u00a0[2]: Copied! <pre>def num_to_label(y, vocab=3):\n    # turn a single digit label to multiclass labels\n    labels = np.zeros((len(y), vocab), dtype=int)\n    for i, yy in enumerate(y):\n        labels[i, yy] = 1\n    return labels\n\ndef make_pairs(length=10, vocab=3):\n    x_num = [np.random.randint(0, vocab) for i in range(length)]\n    y_num = [(x+1) % vocab for x in x_num]\n    # One hot encode the input and output\n    x = num_to_label(x_num)\n    y = num_to_label(y_num)\n    return x, y\n</pre> def num_to_label(y, vocab=3):     # turn a single digit label to multiclass labels     labels = np.zeros((len(y), vocab), dtype=int)     for i, yy in enumerate(y):         labels[i, yy] = 1     return labels  def make_pairs(length=10, vocab=3):     x_num = [np.random.randint(0, vocab) for i in range(length)]     y_num = [(x+1) % vocab for x in x_num]     # One hot encode the input and output     x = num_to_label(x_num)     y = num_to_label(y_num)     return x, y In\u00a0[3]: Copied! <pre>size = 10\nx, y = [], []\nfor i in range(size):\n    xx, yy = make_pairs()\n    x.append(xx)\n    y.append(yy)\n</pre> size = 10 x, y = [], [] for i in range(size):     xx, yy = make_pairs()     x.append(xx)     y.append(yy) <p>Different from basic neural net, RNN does feed forward and backpropagation for the entire sequence, not at each step independently. First let's layout the components we need. This should give a rough breakdown to help writing the code.</p> In\u00a0[4]: Copied! <pre>class RNN():\n    def __init__(self):\n        # initialize a random set of weigths and biases\n        pass\n\n    def predict(self):\n        # this is the feed forward process\n        pass\n\n    def backpropagation(self):\n        # one round of backpropagation through the entire sequence\n        pass\n\n    def train(self, epochs):\n        pass\n\n    def total_loss(self):\n        pass\n\n    def loss(self, t):\n        return \n\n    def tanh(x):\n        pass\n\n    def softmax(x):\n        pass\n</pre> class RNN():     def __init__(self):         # initialize a random set of weigths and biases         pass      def predict(self):         # this is the feed forward process         pass      def backpropagation(self):         # one round of backpropagation through the entire sequence         pass      def train(self, epochs):         pass      def total_loss(self):         pass      def loss(self, t):         return       def tanh(x):         pass      def softmax(x):         pass <p>Then fill in the blanks. We'll first enable feed forward functions. In the init function, we not only need to initialize the parameters, but also reserve vectors to memorize all the calculations, as we'll rely on them in both feed forward and BTT. As we see above, terms like $\\Large \\frac{\\partial L_t}{\\partial s_{h,t}}$ are used multiple times and in different epochs.</p> In\u00a0[31]: Copied! <pre>import numpy as np\n\nclass RNN1():\n    def __init__(self, n_nodes=10, vocab_size=3):\n        # initialize a random set of weigths and biases\n        self.n_nodes = n_nodes\n        self.w = np.random.randn(n_nodes, vocab_size)\n        self.u = np.random.randn(n_nodes, n_nodes)\n        self.v = np.random.randn(vocab_size,n_nodes)\n        self.b_h = np.random.randn(n_nodes)\n        self.b_o = np.random.randn(vocab_size)\n        self.s_h = []\n        self.a_h = []\n        self.s_o = []\n        self.y_hat = []\n\n    def predict(self, x):\n        return [self.predict_single(xx) for xx in x]\n\n    def predict_scores(self, x):\n        return [self.feedforward(xx) for xx in x]\n    \n    def predict_single(self, x):\n        scores = self.feedforward(x)\n        return self.score_to_num(scores)\n\n    def reset_mem(self):\n        self.s_h = []\n        self.a_h = []\n        self.s_o = []\n        self.y_hat = []\n    \n    def feedforward(self, x):\n        self.reset_mem()\n        # this is the feed forward process\n        for t in range(len(x)):\n            # this follows straight from the formula\n            a_ht_prev = np.zeros(self.n_nodes) if t == 0 else self.a_h[t-1]\n            s_ht = x[t] @ self.w.T + a_ht_prev @ self.u + self.b_h\n            a_ht = self.tanh(s_ht)\n            s_ot = a_ht @ self.v.T + self.b_o\n            y_t = self.softmax(s_ot)\n\n            # memorize all above\n            self.s_h.append(s_ht)\n            self.a_h.append(a_ht)\n            self.s_o.append(s_ot)\n            self.y_hat.append(y_t)\n        return self.y_hat\n\n    def total_loss(self, y, y_hat):\n        return sum([self.loss(t, y, y_hat) for t in range(len(y))])\n\n    @staticmethod\n    def loss(t, y, y_hat):\n        if (y[t] == y_hat[t]).all():\n            return 0\n        return np.sum(-y[t]*np.log(y_hat[t]))\n\n    @staticmethod\n    def tanh(x):\n        return np.tanh(x)\n\n    @staticmethod\n    def softmax(x):\n        x = np.array(x, dtype=np.float128)\n        # to avoid overflow due to super large x, we subtract the max\n        xx = x - np.max(x)\n        exp = np.exp(xx)\n        return exp / exp.sum(keepdims=True)\n\n    @staticmethod\n    def score_to_num(scores):\n        indices = [np.argmax(p) for p in scores]\n        return np.array(indices)\n</pre> import numpy as np  class RNN1():     def __init__(self, n_nodes=10, vocab_size=3):         # initialize a random set of weigths and biases         self.n_nodes = n_nodes         self.w = np.random.randn(n_nodes, vocab_size)         self.u = np.random.randn(n_nodes, n_nodes)         self.v = np.random.randn(vocab_size,n_nodes)         self.b_h = np.random.randn(n_nodes)         self.b_o = np.random.randn(vocab_size)         self.s_h = []         self.a_h = []         self.s_o = []         self.y_hat = []      def predict(self, x):         return [self.predict_single(xx) for xx in x]      def predict_scores(self, x):         return [self.feedforward(xx) for xx in x]          def predict_single(self, x):         scores = self.feedforward(x)         return self.score_to_num(scores)      def reset_mem(self):         self.s_h = []         self.a_h = []         self.s_o = []         self.y_hat = []          def feedforward(self, x):         self.reset_mem()         # this is the feed forward process         for t in range(len(x)):             # this follows straight from the formula             a_ht_prev = np.zeros(self.n_nodes) if t == 0 else self.a_h[t-1]             s_ht = x[t] @ self.w.T + a_ht_prev @ self.u + self.b_h             a_ht = self.tanh(s_ht)             s_ot = a_ht @ self.v.T + self.b_o             y_t = self.softmax(s_ot)              # memorize all above             self.s_h.append(s_ht)             self.a_h.append(a_ht)             self.s_o.append(s_ot)             self.y_hat.append(y_t)         return self.y_hat      def total_loss(self, y, y_hat):         return sum([self.loss(t, y, y_hat) for t in range(len(y))])      @staticmethod     def loss(t, y, y_hat):         if (y[t] == y_hat[t]).all():             return 0         return np.sum(-y[t]*np.log(y_hat[t]))      @staticmethod     def tanh(x):         return np.tanh(x)      @staticmethod     def softmax(x):         x = np.array(x, dtype=np.float128)         # to avoid overflow due to super large x, we subtract the max         xx = x - np.max(x)         exp = np.exp(xx)         return exp / exp.sum(keepdims=True)      @staticmethod     def score_to_num(scores):         indices = [np.argmax(p) for p in scores]         return np.array(indices) <p>Run some random numbers to make sure matrix dimensions are good. We have written functions to</p> <ul> <li>Make predictinos by feed forward, using randomly generated parameters</li> <li>Calculate loss</li> </ul> In\u00a0[32]: Copied! <pre>rnn1 = RNN1()\npred_scores = rnn1.predict_scores(x)\npred = rnn1.predict(x)\nloss = rnn1.total_loss(y, pred_scores)\n</pre> rnn1 = RNN1() pred_scores = rnn1.predict_scores(x) pred = rnn1.predict(x) loss = rnn1.total_loss(y, pred_scores) In\u00a0[13]: Copied! <pre>pred, loss, \n</pre> pred, loss,  Out[13]: <pre>([array([1, 0, 0, 0, 1, 0, 0, 1, 1, 0]),\n  array([2, 0, 0, 0, 1, 0, 0, 1, 2, 0]),\n  array([1, 0, 2, 1, 0, 0, 2, 0, 0, 0]),\n  array([1, 0, 2, 1, 0, 0, 0, 0, 1, 0]),\n  array([1, 0, 0, 0, 1, 2, 2, 2, 0, 0]),\n  array([2, 0, 0, 0, 0, 1, 0, 0, 1, 1]),\n  array([1, 0, 2, 2, 0, 0, 1, 0, 1, 2]),\n  array([2, 0, 0, 0, 1, 0, 0, 1, 1, 0]),\n  array([2, 0, 0, 0, 0, 0, 1, 0, 0, 2]),\n  array([1, 0, 0, 0, 1, 0, 0, 1, 1, 0])],\n np.longdouble('164.06705644911090597'))</pre> In\u00a0[10]: Copied! <pre># validate correctness of the loss calculation\nloss0 = rnn1.total_loss(y[0], y[0])\nloss1 = rnn1.total_loss(pred_scores[0], pred_scores[0])\nassert loss0 == 0\nassert loss1 == 0\n</pre> # validate correctness of the loss calculation loss0 = rnn1.total_loss(y[0], y[0]) loss1 = rnn1.total_loss(pred_scores[0], pred_scores[0]) assert loss0 == 0 assert loss1 == 0 <p>Move ahead to BPTT!</p> In\u00a0[191]: Copied! <pre>class RNN2(RNN1):\n    def __init__(self, learning_rate=0.0001, n_nodes=10, vocab_size=3):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.delta_l_over_s_h = []\n\n    def backpropagation(self, x, y, y_hat):\n        # BPTT for a single sequence of sample. This function\n        # returns the deltas of W, V, U, b_h, and b_o\n        T = len(y)\n        du = np.zeros(self.u.shape)\n        dv = np.zeros(self.v.shape)\n        dw = np.zeros(self.w.shape)\n        dbh = np.zeros(self.b_h.shape)\n        dbo = np.zeros(self.b_o.shape)\n        # btt goes backwards, from time t to time 0\n        ds_ht_plus_1 = np.zeros(self.n_nodes)\n        for t in reversed(range(T)):\n            # one round of backpropagation at time t\n            # output layer to hidden layer.\n            # dLt/dbo = (y_hat_t - y_t)\n            # dLt/dV = (y_hat_t - y_t) * a_ht = dLt/dbo * a_ht\n            dbo_t = (y_hat[t] - y[t].T)\n            dv_t = dbo_t[:, np.newaxis] @ self.a_h[t][:, np.newaxis].T\n            dbo += dbo_t\n            dv += dv_t\n\n            # hidden layer to input layer\n            da_ht = (y_hat[t] - y[t]) @ self.v + ds_ht_plus_1 @ self.u\n            ds_ht = da_ht * (1-self.a_h[t] ** 2)\n            ds_ht_plus_1 = ds_ht\n            dw_t = ds_ht[:, np.newaxis] @ x[t][:, np.newaxis].T\n\n            if t-1 &gt;= 0:\n                a_ht_1 = self.a_h[t-1]\n            else:\n                a_ht_1 = np.zeros(self.n_nodes)\n            du_t = ds_ht @ a_ht_1\n\n            dbh_t = ds_ht\n\n            dw += dw_t\n            du += du_t\n            dbh += dbh_t\n            \n        return dw, dv, du, dbh, dbo\n\n    def train(self, epochs, x, y, verbose=False):\n        for i in range(epochs):\n            loss = 0\n            for j in range(len(x)):\n                # feed forward\n                y_hat = self.feedforward(x[j])\n                # current loss\n                loss += self.total_loss(y[j], y_hat)\n                # calculate deltas\n                dW, dV, dU, dbh, dbo = self.backpropagation(x[j], y[j], y_hat)\n                # update weights accordingly\n                self.u -= self.learning_rate * dU\n                self.w -= self.learning_rate * dW\n                self.v -= self.learning_rate * dV\n                self.b_h -= self.learning_rate * dbh\n                self.b_o -= self.learning_rate * dbo\n            if verbose and i % 100 == 0:\n                print(f\"loss at epoch {i}: {loss}\")\n</pre> class RNN2(RNN1):     def __init__(self, learning_rate=0.0001, n_nodes=10, vocab_size=3):         super().__init__()         self.learning_rate = learning_rate         self.delta_l_over_s_h = []      def backpropagation(self, x, y, y_hat):         # BPTT for a single sequence of sample. This function         # returns the deltas of W, V, U, b_h, and b_o         T = len(y)         du = np.zeros(self.u.shape)         dv = np.zeros(self.v.shape)         dw = np.zeros(self.w.shape)         dbh = np.zeros(self.b_h.shape)         dbo = np.zeros(self.b_o.shape)         # btt goes backwards, from time t to time 0         ds_ht_plus_1 = np.zeros(self.n_nodes)         for t in reversed(range(T)):             # one round of backpropagation at time t             # output layer to hidden layer.             # dLt/dbo = (y_hat_t - y_t)             # dLt/dV = (y_hat_t - y_t) * a_ht = dLt/dbo * a_ht             dbo_t = (y_hat[t] - y[t].T)             dv_t = dbo_t[:, np.newaxis] @ self.a_h[t][:, np.newaxis].T             dbo += dbo_t             dv += dv_t              # hidden layer to input layer             da_ht = (y_hat[t] - y[t]) @ self.v + ds_ht_plus_1 @ self.u             ds_ht = da_ht * (1-self.a_h[t] ** 2)             ds_ht_plus_1 = ds_ht             dw_t = ds_ht[:, np.newaxis] @ x[t][:, np.newaxis].T              if t-1 &gt;= 0:                 a_ht_1 = self.a_h[t-1]             else:                 a_ht_1 = np.zeros(self.n_nodes)             du_t = ds_ht @ a_ht_1              dbh_t = ds_ht              dw += dw_t             du += du_t             dbh += dbh_t                      return dw, dv, du, dbh, dbo      def train(self, epochs, x, y, verbose=False):         for i in range(epochs):             loss = 0             for j in range(len(x)):                 # feed forward                 y_hat = self.feedforward(x[j])                 # current loss                 loss += self.total_loss(y[j], y_hat)                 # calculate deltas                 dW, dV, dU, dbh, dbo = self.backpropagation(x[j], y[j], y_hat)                 # update weights accordingly                 self.u -= self.learning_rate * dU                 self.w -= self.learning_rate * dW                 self.v -= self.learning_rate * dV                 self.b_h -= self.learning_rate * dbh                 self.b_o -= self.learning_rate * dbo             if verbose and i % 100 == 0:                 print(f\"loss at epoch {i}: {loss}\") <p>Some helper function to visualize the quality of the model before and after training</p> In\u00a0[214]: Copied! <pre>def array2str(arr):\n    return ''.join(str(a) for a in arr)\n\ndef evaluate_model(model, x, y_true):\n    true_pretty = [array2str(RNN2.score_to_num(yy)) for yy in y_true]\n    y_pred = model.predict(x)\n    pred_pretty = [array2str(yy) for yy in y_pred]\n    y_true_total = ''.join(true_pretty)\n    y_pred_total = ''.join(pred_pretty)\n    overlap = 0\n    for i in range(len(y_true_total)):\n        if y_true_total[i] == y_pred_total[i]:\n            overlap += 1\n    print(\"actual:\", true_pretty)\n    print(\"predicted:\", pred_pretty)\n    print(\"overlap:\", overlap)\n</pre> def array2str(arr):     return ''.join(str(a) for a in arr)  def evaluate_model(model, x, y_true):     true_pretty = [array2str(RNN2.score_to_num(yy)) for yy in y_true]     y_pred = model.predict(x)     pred_pretty = [array2str(yy) for yy in y_pred]     y_true_total = ''.join(true_pretty)     y_pred_total = ''.join(pred_pretty)     overlap = 0     for i in range(len(y_true_total)):         if y_true_total[i] == y_pred_total[i]:             overlap += 1     print(\"actual:\", true_pretty)     print(\"predicted:\", pred_pretty)     print(\"overlap:\", overlap) In\u00a0[216]: Copied! <pre>rnn2 = RNN2(learning_rate=0.0001, n_nodes=100)\n\nevaluate_model(rnn2, x, y)\n</pre> rnn2 = RNN2(learning_rate=0.0001, n_nodes=100)  evaluate_model(rnn2, x, y) <pre>actual: ['0022210110', '2002002022', '1201102100', '1101201202', '0011102210', '2001012012', '1100100012', '2010122011', '2210201220', '0002112100']\npredicted: ['1120020111', '0111110101', '1012111110', '1122021011', '1111212111', '0111110111', '1122111210', '0111210111', '0111011101', '1120112211']\noverlap: 30\n</pre> In\u00a0[217]: Copied! <pre>rnn2.train(5000, x, y, True)\n</pre> rnn2.train(5000, x, y, True) <pre>loss at epoch 0: 202.7836139814427\nloss at epoch 100: 110.27131099270905\nloss at epoch 200: 83.10060656394155\nloss at epoch 300: 68.02587099271048\nloss at epoch 400: 65.38807540419106\nloss at epoch 500: 58.830339777760116\nloss at epoch 600: 54.22732667386771\nloss at epoch 700: 50.55093415573766\nloss at epoch 800: 47.74560321491536\nloss at epoch 900: 44.73411202441217\nloss at epoch 1000: 42.674532978222715\nloss at epoch 1100: 40.22851081431978\nloss at epoch 1200: 37.45028109530502\nloss at epoch 1300: 36.12487867914503\nloss at epoch 1400: 35.06549335818424\nloss at epoch 1500: 33.513962830639684\nloss at epoch 1600: 32.5512323935453\nloss at epoch 1700: 31.362170861993704\nloss at epoch 1800: 30.204606414217032\nloss at epoch 1900: 28.721560859253696\nloss at epoch 2000: 26.970357477770808\nloss at epoch 2100: 24.717921731469385\nloss at epoch 2200: 22.49272184873606\nloss at epoch 2300: 26.95544520791458\nloss at epoch 2400: 22.36715164290292\nloss at epoch 2500: 22.927457101045125\nloss at epoch 2600: 20.08979899166029\nloss at epoch 2700: 19.196828047924182\nloss at epoch 2800: 18.896647133355465\nloss at epoch 2900: 16.375881668641835\nloss at epoch 3000: 16.72261866189776\nloss at epoch 3100: 13.838755733258779\nloss at epoch 3200: 13.076008068052444\nloss at epoch 3300: 12.151538393305366\nloss at epoch 3400: 11.311482642558467\nloss at epoch 3500: 10.327645010343307\nloss at epoch 3600: 10.523296159596141\nloss at epoch 3700: 10.245959941644236\nloss at epoch 3800: 9.942483406298678\nloss at epoch 3900: 9.955048248908344\nloss at epoch 4000: 10.352242849777243\nloss at epoch 4100: 10.464099192444507\nloss at epoch 4200: 10.364028014593218\nloss at epoch 4300: 10.251650538093257\nloss at epoch 4400: 10.447283274950694\nloss at epoch 4500: 11.262703641141721\nloss at epoch 4600: 10.727188040921888\nloss at epoch 4700: 9.973869429813469\nloss at epoch 4800: 9.390145566791826\nloss at epoch 4900: 8.742774743879831\n</pre> <p>After training:</p> In\u00a0[219]: Copied! <pre>evaluate_model(rnn2, x, y)\n</pre> evaluate_model(rnn2, x, y) <pre>actual: ['0022210110', '2002002022', '1201102100', '1101201202', '0011102210', '2001012012', '1100100012', '2010122011', '2210201220', '0002112100']\npredicted: ['0022210100', '2002002022', '1201102100', '1101201202', '0011102210', '2001012012', '1100100012', '2010122011', '2210201020', '0002112100']\noverlap: 98\n</pre> <p>Pretty good! This concludes this note happily.</p>"},{"location":"tech/ml/wooden_wheels/recurrent_neural_net/#recurrent-neural-net-from-scratch","title":"Recurrent Neural Net From Scratch\u00b6","text":""},{"location":"tech/ml/wooden_wheels/recurrent_neural_net/#feed-forward","title":"Feed forward\u00b6","text":"<p>Rephrasing the diagram, we have the feed forward process as, for each time step $t$,</p> <ul> <li>$s_{h,t} = x_t W + a_{h,t-1}U + b_h$</li> <li>$a_{h,t} = \\sigma_h(s_{h,t})$</li> <li>$s_{o,t} = a_{h,t}V +b_o$</li> <li>$y_t = \\sigma_o(s_{o,t})$</li> </ul> <p>h means hidden layer, o means output layer, s means weighted sum, a means activated value. $\\sigma$ means the activation function. For the activation functions, we use softmax at the output layer and tanh at the hidden layer.</p> <p>$$\\large tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$</p> <p>It has the convenient property that</p> <p>$$ \\large (tanh(x))' = 1-(tanh(x))^2  $$</p>"},{"location":"tech/ml/wooden_wheels/recurrent_neural_net/#backpropagation-through-time","title":"Backpropagation Through Time\u00b6","text":"<ul> <li>As there's dependency bewteen the previous and next steps, backpropagation in RNN goes through the entire sequence, called Backpropagation Through Time (BPTT) where we propagate the gradient not only through layers, but also through the entire sequence of data. This means we sum up the lost of the predictions of the sequece.</li> </ul> <p>Overall loss is</p> <p>$$ L_{total} = \\sum_1^t L_t $$</p> <p>where</p> <p>$$ L_t = -y_t log(\\hat{y_t}) $$</p> <ul> <li>Backpropagation from the output layer to the hidden layer is similar as vanila neural network since we don't add anything new between these two layers</li> <li>Backpropagation from the hidden layer will go to both the input layer and the hidden layer of the previous data point.</li> </ul>"},{"location":"tech/ml/wooden_wheels/recurrent_neural_net/#weights-from-the-output-layer-to-the-hidden-layer","title":"Weights from the output layer to the hidden layer\u00b6","text":"<p>From the math of basic neural net, we know</p>"},{"location":"tech/ml/wooden_wheels/recurrent_neural_net/#weights-from-the-hidden-to-the-input-layer-and-previous-data-point","title":"Weights from the hidden to the input layer and previous data point\u00b6","text":""},{"location":"tech/ml/wooden_wheels/recurrent_neural_net/#problem-to-solve","title":"Problem to solve\u00b6","text":"<p>We'll use RNN to solve a toy problem of predicting the next number in mod 3. 1 is next to 0, 2 is next to 1, 0 is next to 2.</p>"},{"location":"writing/2010/","title":"2010","text":""},{"location":"writing/2010/#jan-2-2010","title":"Jan 2, 2010","text":"<p>\u5355\u88e4\u5355\u8863+\u5916\u5957\u8fc7\u51ac\u7684\u4f1f\u5927\u7406\u60f3\u7834\u706d\uff0c\u65b0\u5e74\u524d\u53bbnavy pier\u770b\u70df\u82b1\uff0c\u96f6\u4e0b\u5341\u51e0\u7684\u6e29 \u5ea6\u4e0b\u5446\u5dee\u4e0d\u591a\u4e00\u5c0f\u65f6\uff0c\u5373\u4f7f\u6709\u9971\u6696\u8863\u88e4\uff0c\u53d6\u6696\u8fd8\u662f\u57fa\u672c\u9760\u6296\uff0c\u8d76\u8def\u7684\u65f6\u5019\u5c0f\u8dd1\uff0c \u4e0d\u8d76\u8def\u7684\u65f6\u5019\u8ddf\u7740\u97f3\u4e50\u8e66\u3002\u624b\u811a\u6ca1\u6709\u77e5\u89c9\u5f88\u4e45\uff0c\u4f46\u6ca1\u611f\u5192\u3002\u70df\u82b1\u6548\u679c\u4e0d\u600e\u7684\uff0c\u6c14 \u6c1b\u5012\u662f\u4e0d\u9519\uff0c\u6e2f\u53e3\u6324\u5f97\u6ee1\u6ee1\u7684\uff0c\u4eba\u7fa4\u65f6\u4e0d\u65f6\u6b22\u547c\u4e00\u4e0b\u3002</p> <p>\u653e\u5047\u5446\u5bb6\u91cc\u603b\u662f\u8d8a\u5446\u8d8a\u61d2\uff0c\u8fde\u6e38\u620f\u90fd\u61d2\u7684\u73a9\uff0c\u60f3\u53eb\u670b\u53cb\u6765\u53c8\u61d2\u5f97\u6253\u7535\u8bdd\uff0c\u5982\u679c\u4e0d \u662f\u56e0\u4e3a\u51c6\u5907\u4e0b\u5348\u7684\u997a\u5b50\u706b\u9505party\uff0c\u4f30\u8ba1\u53c8\u6d6a\u8d39\u4e00\u5929\u3002</p>"},{"location":"writing/2010/#jan-15-2010","title":"Jan 15, 2010","text":"<p>\u68a6\u5230\u623f\u95f4\u91cc\u51e0\u5929\u6ca1\u5403\u7684\u68a8\u574f\u6389\u4e86\uff0c\u9192\u6765\u5403\uff0c\u679c\u7136\u574f\u4e86\u3002\u6253\u592a\u9f13\u8fbe\u4eba\u6253\u534a\u4e2a\u5c0f\u65f6\uff0c \u5dee\u4e0d\u591a\u4e94\u70b9\u534a\uff0c\u51fa\u95e8\u3002</p> <p>\u8fd9\u5468\u662f\u672c\u79d1\u751f\u65e9\u953b\u70bc\u5468\uff0c\u516d\u4e03\u70b9\u7684\u6837\u5b50\u5728\u4f53\u80b2\u9986\u96c6\u4f53\u505a\u5404\u79cd\u8fd0\u52a8\uff0c\u4e5f\u662f\u5404\u4f53\u80b2\u793e \u56e2\u62db\u4eba\u7684\u673a\u4f1a\u3002\u4eca\u5929\u6211\u4eec\u8dc6\u62f3\u9053\u793e\u56e2\u51fa\u52a8\u3002\u5148\u96c6\u4f53\u745c\u73c8\uff0c\u505a\u62c9\u4f38\uff0c\u6211\u4eec\u5728\u4e00\u65c1\u770b\u3002 \u58ee\u89c2\u5f97\u50cf\u8df3\u5927\u795e\uff0cn\u591a\u4eba\u505a\u745c\u73c8\uff0c\u8db4\u5728\u5730\u4e0a\u53cc\u624b\u5411\u540e\u4e0a\u65b9\u5e73\u4e3e\uff0c\u8ba9\u6211\u60f3\u8d77\u5728\u65e7\u91d1 \u5c71\u770b\u7684\u6d77\u72ee\u3002\u7136\u540e\u5e26\u9886\u4f17\u4eba\u505a\u4e86\u51e0\u4e2a\u8dc6\u62f3\u9053\u57fa\u7840\u52a8\u4f5c\uff0c\u5c31\u53bb\u5403\u514d\u8d39\u65e9\u9910\u4e86\u3002</p> <p>\u505a\u5b8c\u6574\u5929\u7684\u5b9e\u9a8c\u540e\uff0c\u8dd1\u6b65\uff0c\u4e0a\u8dc6\u62f3\u9053\uff0c\u518d\u8d76\u53bb\u6821\u7535\u5f71\u9662\u770bx\u7247\u7cfb\u5217\u3002 resurrection of eve\uff0c\u8272\u5f69\u504f\u7ea2\uff0c\u8eab\u4f53\u878d\u8fdb\u80cc\u666f\u8272\u3002\u59bb\u5b50eve\u8f66\u7978\u6bc1\u5bb9\u518d\u6574\u5bb9\u540e\uff0c \u88ab\u4e08\u592b\u534a\u80c1\u8feb\u53bbnp\u805a\u4f1a\uff0c\u5374\u5728\u591a\u6b21np\u4e2d\u53d1\u6398\u51fa\u4e86x\u7684\u771f\u7406\u3002\u6280\u672f\u91cd\u590d\u6027\u5927\uff0c\u573a\u666f \u53d8\u5316\u5012\u591a\uff0cnp\u7684\u5730\u76d8\u6bcf\u6b21\u90fd\u6709\u4e2a\u4e3b\u9898\u88c5\u9970\u3002\u8fd9\u90e8\u548c\u4e0a\u6b21\u7684alice in wonderland \u91cc\u7684\u4e3b\u7ebf\u90fd\u662f\u6dd1\u5973\u5728\u91cd\u590d\u52b3\u52a8\u4e2d\u53d8\u719f\u5973\uff0c\u6211\u4e8e\u662f\u4ee5\u504f\u6982\u5168\u7684\u60f3\u5973\u7684\u5bf9av\u6ca1\u6709\u7537\u7684 \u70ed\u8877\uff0c\u662f\u4e0d\u662f\u56e0\u4e3aav\u5185\u5bb9\u4fa7\u91cd\u5bf9\u5973\u4eba\u7684\u5f00\u53d1\uff0c\u800c\u4e0d\u662f\u5bf9\u7537\u4eba\u7684\u5f00\u53d1\u3002\u4e00\u5806\u4eba\u6b63\u4e8c \u516b\u7ecf\u7684\u5750\u5b66\u6821\u5f71\u9662\u770bav\uff0c\u548c\u8c10\u65e0\u6bd4\uff0c\u62b5\u9500\u6389\u4e86\u5927\u5c4f\u5e55\u770bav\u7684\u8272\u60c5\u6548\u679c\u3002</p> <p>\u6f2b\u957f\u7684\u4e00\u5929\uff0c\u5012\u4e0b\u6b7b\u7761\u3002</p>"},{"location":"writing/2010/#jan-16-2010","title":"Jan 16, 2010","text":"<p>\u7ecf\u8fc7\u5199review\u7684\u714e\u71ac\u518d\u770bpaper\u5c31\u662f\u4e0d\u4e00\u6837\uff0cintroduction\u90e8\u5206\u4e00\u770b\u5230\u5f15\u7528\u4f5c\u8005 \u540d\uff0c\u5c31\u6a21\u7cca\u8bb0\u8d77\u6587\u7ae0\u91cc\u7684\u6570\u636e\uff0c\u65e0\u6bd4\u4eb2\u5207\u3002</p> <p>\u6700\u8fd1\u4e00\u76f4\u91cd\u6e29\u592a\u9f13\u8fbe\u4ebaDS\u4e0a\u7684\u4e24\u4f5c\u3002\u4e03\u5c9b\u7684\u8fc7\u5173\u6a21\u5f0f\u53ef\u73a9\u6027\u5f3a\uff0c\u65e0\u5948\u97f3\u4e50\u57fa\u672c\u6ca1 \u6709\u5bf9\u80c3\u53e3\u7684\uff0c\u97f3\u4e50\u8282\u7684\u97f3\u4e50\u4e0d\u9519\uff0c\u4f46\u6ca1\u6709\u8fc7\u5173\u5f62\u5f0f\u662f\u9057\u61be\u3002</p> <p>\u4e70\u4e86\u7535\u5b50\u9f13\uff0c\u7ec4\u88c5\u5982\u540c\u73a9\u6a21\u578b\uff0c\u5f88\u597d\u73a9\u3002\u5fd8\u8bb0\u4e70\u8033\u673a\u8f6c\u6362\u63d2\u5934\uff0c\u4e8e\u662f\u5b88\u7740\u6ca1\u6709\u58f0 \u97f3\u8f93\u51fa\u7684\u9f13\u5e72\u77aa\u773c\uff0c\u6068\u4e0d\u5f97\u80fd\u628a\u58f0\u97f3\u624b\u52a8\u6316\u51fa\u6765\u3002</p>"},{"location":"writing/2010/#jan-18-2010","title":"Jan 18, 2010","text":"<p>\u53c8\u5230\u5468\u672b\u8e6d\u996d\u65f6\uff0c\u4ee5\u6e38\u620f\u673a\u505a\u8bf1\u60d1\uff0c\u628a\u540c\u5b66\u9a97\u6765\u505a\u996d\uff0c\u5403\u5269\u7684\u5f52\u6211\u3002\u5403\u5b8c\u52a0\u4e0a\u5ba4 \u53cb\u4e24\u4eba\u6253wii\u7f51\u7403\u3002\u4e0d\u5f97\u4e0d\u8bf4\u8001\u4efb\u7684\u7f51\u7403\u5438\u5f15\u975e\u73a9\u5bb6\u5f88\u6210\u529f\uff0c\u4e0d\u9700\u8981\u5b66\u4e60\uff0c\u4f46\u53c8 \u6709\u6280\u5de7\u53ef\u4ee5\u6478\u7d22\uff0c\u56db\u4eba\u5408\u4f5c\u975e\u5e38\u597d\u73a9\uff0c\u73a9\u5bb6\u975e\u73a9\u5bb6\u90fd\u5f00\u5fc3\uff0c\u4e00\u76f4\u6253\u5230\u534a\u591c\u4e09\u70b9\u3002</p> <p>\u75af\u73a9\u540e\u7684\u7b2c\u4e8c\u5929\u72c2\u7761\uff0c\u800c\u4e14\u8fd8\u6ca1\u52a8\u529b\u505a\u4e8b\uff0c\u770b\u6765\u6062\u590d\u671f\u591f\u957f\uff0c\u8981\u91c7\u53d6\u63aa\u65bd\u3002\u4ee5\u540e \u5e94\u8be5\u73a9\u5b8c\u7761\u9192\u5c31\u8dd1\u6b65\uff0c\u91cd\u65b0\u628a\u4eba\u8c03\u52a8\u8d77\u6765\u3002</p>"},{"location":"writing/2010/#jan-20-2010","title":"Jan 20, 2010","text":"<p>\u65e9\u4e0a\u542cA\u540c\u5b66\u7684\u6f14\u8bb2\u7ec3\u4e60\uff0c\u534a\u7761\u534a\u9192\u70b9\u5934\u4e2d\u770b\u5230C\u540c\u5b66\u5bf9\u6211\u7b11\uff0c\u4e0d\u597d\u610f\u601d\u5730\u9192\u4e86\u3002</p> <p>\u4e0b\u5348\u5c0f\u9ed1\u5c4b\u6e29\u5ea6\u9ad8\u8fbe29\uff0c\u53c8\u5728\u663e\u5fae\u955c\u524d\u9762\u4e00\u7761\u5230\u5e95\uff0c\u6ce2\u5170\u59b9\u5b50\u63a8\u95e8\u770b\u5230\u54c8\u54c8\u5927\u7b11 \u5e76\u5927\u8d5e\u6211\u7761\u7684\u6c89\u7a33\u800c\u4e0d\u70b9\u5934\u3002</p> <p>\u7761\u9192\u540e\u5b9e\u9a8c\u4e5f\u6ca1\u505a\u591a\u5c11\uff0c\u800c\u4e14\u53d1\u73b0\u524d\u51e0\u5929\u7684\u6570\u636e\u66f4\u597d\u4e14\u591f\u7528\uff0c\u4eca\u5929\u5176\u5b9e\u5c31\u6ca1\u5fc5\u8981\u505a\u3002</p> <p>\u542c\u6f14\u8bb2\u3002\u8def\u4e0a\u6ce2\u5170\u59b9\u5b50\u95ee\u6211\u4eec\u8fdf\u5230\u662f\u4e0d\u662f\u4e0d\u597d\uff0c\u662f\u4e0d\u662fpizza\u5c31\u6ca1\u4e86\uff0c\u6211\u8bf4\u4e0d\u53ef \u80fd\uff0c\u4e00\u662f\u4e0d\u53ef\u80fd\u6ca1\u4eba\u8fdf\u5230\uff0c\u4e8c\u662fpizza\u6bd4\u6f14\u8bb2\u8fd8\u91cd\u8981\uff0c\u600e\u4e48\u53ef\u80fd\u6ca1\u6709\u3002\u997f\u7684\u4e0d\u884c \u6240\u4ee5\u53ea\u987e\u7740\u5403\uff0c\u6f14\u8bb2\u90fd\u6ca1\u600e\u4e48\u542c\u3002</p> <p>\u5403\u592a\u9971\u4e5f\u6ca1\u6cd5\u8dd1\u6b65\uff0c\u9876\u7740\u5706\u6eda\u6eda\u7684\u809a\u5b50\u5750\u8f66\u56de\u5bb6\uff0c\u6211\u771f\u662f\u7814\u7a76\u751f\u9662\u7684\u5927\u86c0\u866b\uff0c\u4eca \u5929\u9664\u4e86\u5403\u548c\u7761\u5c31\u6ca1\u4efb\u4f55\u6210\u679c\u4e86\u3002</p>"},{"location":"writing/2010/#jan-26-2010","title":"Jan 26, 2010","text":"<p>\u665a\u4e0a\u5728\u56fe\u4e66\u9986\u7b49\u8f66\u3002\u4e00\u4e2a\u59b9\u5b50\u4ece\u56fe\u4e66\u9986\u91cc\u51fa\u6765\uff0c\u60ca\u8bb6\u7684\u95ee\uff0c\u95e8\u574f\u4e86\u5417\uff0c\u4f60\u4eec\u600e\u4e48 \u6324\u5728\u8fd9\u91cc\u4e0d\u51fa\u53bb\u3002\u6211\u4eec\u56de\u7b54\uff1a\u592a\u51b7\u4e86\uff0c\u7b49\u8f66\u5462\u3002</p> <p>\u6bcf\u5929\u5c31\u662f\u4e24\u4e2a\u72b6\u6001\u7684\u640f\u6597\uff1a\u4e0d\u591f\u9971\u548c\u4e0d\u591f\u997f\u3002\u6bcf\u987f\u996d\u7acb\u5373\u4ece\u4e0d\u591f\u9971\u8f6c\u5316\u4e3a\u4e0d\u591f\u997f\uff0c \u7136\u540e\u6d88\u8017\u963f\u6d88\u8017\uff0c\u8111\u529b\u52a0\u4f53\u529b\uff0c\u7ec8\u4e8e\u53d8\u6210\u4e0d\u591f\u9971\uff0c\u53ef\u4ee5\u5403\u4e86\uff0c\u7136\u540e\u7ee7\u7eed\u4e0b\u4e00\u4e2a\u5faa \u73af\u3002</p>"},{"location":"writing/2010/#feb-2-2010","title":"Feb 2, 2010","text":"<p>\u6536\u5230\u4e00\u5806\u97f3\u9891\u7ebf\uff0c\u4ee5\u5f88\u70c2\u7684\u97f3\u8d28\u5f55\u4e86\u4e00\u9996\uff0c\u624d\u53d1\u89c9\u6572\u67b6\u5b50\u6545\u95ee\u9898\u5927\u5927\u7684\uff0c\u8282\u594f\u4e0d \u7a33\u5f53\uff0c\u624b\u811a\u4e0d\u591f\u540c\u6b65\uff0c\u624b\u811a\u4e0d\u591f\u72ec\u7acb\uff0c\u901f\u5ea6\u4e0d\u591f\u5feb\u3002\u539f\u672c\u8fd8\u4ee5\u4e3a\u53cd\u5c04\u795e\u7ecf\u591f\u5feb\uff0c \u8282\u594f\u611f\u591f\u5f3a\uff0c\u534f\u8c03\u611f\u52c9\u5f3a\uff0c\u4e00\u5b66\u9f13\u5168\u539f\u5f62\u6bd5\u9732\u3002</p>"},{"location":"writing/2010/#feb-5-2010","title":"Feb 5, 2010","text":"<p>\u663e\u793a\u65f6\u95f4\u7684\u4e1c\u897f\u8d8a\u6765\u8d8a\u591a\uff0c\u4e00\u4e2a\u95f9\u949f\uff0c\u4e00\u4e2a\u957f\u76f8\u5947\u7279\u7684\u6302\u949f\uff0c\u4e00\u4e2a\u624b\u8868\uff0c\u4e00\u4e2a\u8dd1 \u6b65\u7528\u7684\u8868\u3002\u7a81\u7136\u6709\u4e00\u5929\u6211\u53d1\u73b0\uff0c\u5982\u679c\u6211\u6ca1\u5728\u770b\u65f6\u95f4\uff0c\u8fd9\u4e9b\u4e1c\u897f\u5c31\u662f\u767d\u8d39\u7535\u3002</p>"},{"location":"writing/2010/#feb-15-2010","title":"Feb 15, 2010","text":"<p>\u4e2d\u56fd\u5b66\u751f\u4f1a\u65b0\u5e74\u665a\u4f1a\u540e\uff0c\u53ec\u5524\u72d0\u670b\u72d7\u53cb\u6765\u5bb6\u91cc\u73a9\uff0c\u4e2d\u573a\u4f11\u606f\u65f6\u670b\u53cb\u4eec\u7eb7\u7eb7\u6253\u7535\u8bdd \u7ed9\u4eb2\u621a\u8f6e\u6d41\u62dc\u5e74\uff0c\u6211\u65e0\u4eba\u53ef\u62dc\uff0c\u4e0e\u5176\u8bf4\u5931\u843d\uff0c\u4e0d\u5982\u8bf4\u610f\u5916\uff0c\u4e3a\u5565\u4ee5\u524d\u6ca1\u53d1\u73b0\u8fd9\u4e2a\u3002</p>"},{"location":"writing/2010/#feb-18-2010","title":"Feb 18, 2010","text":"<p>\u5b66\u6821doc film\u5468\u56db\u665a\u662f\u7f8e\u56fdx\u7535\u5f71\u7cfb\u5217\uff0c\u653e\u6620a\u7247\u9ec4\u91d1\u65f6\u671f\u7684\u4e00\u4e9b\u4f5c\u54c1\uff0c\u770b\u4e866\u5468 \u4e4b\u540e\u7ec8\u4e8e\u653e\u5f03\uff0c\u5927\u5c4f\u5e55\u770ba\u7247\u7684\u65b0\u9c9c\u611f\u8fc7\u53bb\u540e\uff0c\u89c9\u5f97\u5927\u8001\u665a\u7684\u5439\u7740\u51b7\u98ce\u53bb\u770b\u8001a\u7247\uff0c \u6027\u4ef7\u6bd4\u4e0d\u9ad8\u3002</p> <p>\u60c5\u8282\u6700\u4e3a\u8be1\u5f02\u7684\u662fthe last bath\uff0c\u5b66\u751f\u62cd\u7684\u5b9e\u9a8ca\u7247\uff0c\u706f\u5149\u6548\u679c\u7729\u6655\uff0c\u58f0\u97f3\u6548\u679c \u6781\u5176\u5dee\uff0c\u770b\u5f97\u4eba\u6655\u6765\u6655\u53bb\u3002\u4e3b\u89d2\u662f\u4e2a\u6444\u5f71\u7537\uff0c\u8def\u4e0a\u642d\u8f66\u88ab\u4fe9\u7f8e\u5973\u63a5\u56de\u5bb6\u62cd\u88f8\u7167\uff0c 3p\uff0c\u7b2c\u4e8c\u5929\u8d77\u6765\u5176\u4e2d\u4e00\u4e2a\u7f8e\u5973\u5e26\u4ed6\u53bb\u6cb3\u8fb9\u6253\u91ce\u6218\uff0c\u7136\u540e\u4e00\u811a\u628a\u4ed6\u8e22\u5230\u6cb3\u91cc\uff0c\u56de\u5bb6 \u548c\u7559\u5b88\u7684\u7f8e\u5973\u62a5\u544a\u8bf4\u4e00\u5207\u6309\u8ba1\u5212\u8fdb\u884c\uff0c\u7136\u540e\u4e24\u4e2a\u7f8e\u5973\u6fc0\u60c5\u53bb\u4e86\u3002\u539f\u4ee5\u4e3a\u662f\u53cc\u98de\uff0c \u5230\u5934\u6765\u5374\u662f\u5973\u94dc\u6740\u4eba\u7247\u3002</p>"},{"location":"writing/2010/#feb-19-2010","title":"Feb 19, 2010","text":"<p>\u6628\u5929\u805a\u4f1a\u4e0a\u6709\u4eba\u8bf4\u8d77\u5b66\u6821\u5267\u793e\u6709\u7b49\u5f85\u6208\u591a\uff0c\u5e76\u5f3a\u70c8\u5efa\u8bae\u5148\u770b\u5267\u672c\uff0c\u4e8e\u662f\u4eca\u5929\u6211\u4e00 \u8fb9\u505a\u5b9e\u9a8c\u4e00\u8fb9\u770b\u5267\u672c\u3002\u665a\u4e0a\u4e00\u78b0\u5934\uff0c\u5c45\u7136\u805a\u4f1a\u7684\u4eba\u90fd\u5dee\u4e0d\u591a\u6765\u4e86\uff0c\u8fd8\u4e00\u534a\u90fd\u4e34\u65f6 \u6076\u8865\u4e86\u5267\u672c\u3002\u53f0\u8bcd\u5927\u534a\u542c\u61c2\uff0c\u6f14\u5f97\u6709\u51fa\u5f69\u7684\u5730\u65b9\uff0c\u540e\u534a\u6bb5\u8fd8\u6709\u4e0d\u5c11\u7b11\u70b9\uff0c\u53cd\u6b63\u662f \u7f8e\u56fd\u5316\u4e86\uff0c\u4e0d\u8fc7\u4e34\u8fd1\u672b\u5c3e\u7ec8\u7a76\u62b5\u4e0d\u4f4f\u56f0\u610f\u3002\u6211\u65c1\u8fb9\u7684\u59b9\u5b50\u4e5f\u4e00\u76f4\u4ee5\u534a\u7761\u72b6\u6001\u7b49\u6208 \u591a\u3002</p>"},{"location":"writing/2010/#feb-20-2010","title":"Feb 20, 2010","text":"<p>\u7f51\u7edc\u516c\u53f8\u6765\u91cd\u88c5\u7f51\u7ebf\uff0c\u6280\u672f\u5458\u5927\u53d4\u770b\u6211\u6709\u9f13\u53c8\u6709\u753b\uff0c\u95ee\u6211\u662f\u4e0d\u662f\u827a\u672f\u5bb6\uff0c\u6211\u5f88\u88ab \u5949\u627f\u7684\u5fd8\u8bb0\u8bf4\u6211\u662f\u641e\u79d1\u5b66\u7684\u4e86\u3002\u770b\u89c1\u6211\u7684\u9ad8\u8fbe\u6a21\u578b\uff0c\u4ed6\u5f88\u6fc0\u52a8\u7684\u8bf4\u4ed6\u559c\u6b22\u94c1\u7532\u4e07 \u80fd\u4fa0\uff0c\u6536\u96c6\u4e86\u6a21\u578b\uff0c\u8fd8\u4e00\u76f4\u8ffd\u65b0\u52a8\u753b\u770b\uff0c\u7136\u540e\u804a\u4e86\u4f1a\u673a\u5668\u4eba\u70ed\u8840\u52a8\u753b\u6e38\u620f\uff0c\u771f\u662f \u5404\u884c\u5404\u4e1a\u6709\u5b85\u4eba\u3002</p>"},{"location":"writing/2010/#feb-22-2010","title":"Feb 22, 2010","text":"<p>\u4e70\u4e86\u4e00\u7f50\u5b50\u7684\u7cd6\uff0c\u7ed9\u81ea\u5df1\u5efa\u7acb\u6b63\u53cd\u9988\u673a\u5236\uff0c\u5982\u679c\u767d\u5929\u52aa\u529b\u5de5\u4f5c\u5b66\u4e60\uff0c\u665a\u4e0a\u5c31\u5403\u9897 \u7cd6\uff0c\u5982\u679c\u4e00\u5468\u575a\u6301\u4e00\u4e9b\u597d\u4e60\u60ef\uff0c\u5468\u672b\u5c31\u589e\u52a0\u6e38\u620f\u65f6\u95f4\u3002</p> <p>\u5e0c\u671b\u6709\u4e00\u5929\u6211\u80fd\u6210\u4e3a\u4e00\u4e2a\u5b8c\u6574\u7684\u4eba\u3002</p>"},{"location":"writing/2010/#mar-1-2010","title":"Mar 1, 2010","text":"<p>\u6587\u7ae0\u5c31\u5dee\u6700\u540e\u4e00\u4e2a\u56fe\uff0c\u800c\u653b\u514b\u8fd9\u4e2a\u56fe\u82b1\u6389\u7684\u65f6\u95f4\u5374\u65e0\u6570\u3002\u6700\u8fd1\u663e\u5fae\u955c\u53c8\u574f\u4e86\u7b49\u4eba \u4fee\uff0c\u53ea\u597d\u770b\u6587\u7ae0\u5ea6\u65e5\u3002\u770b\u5230\u4e00\u534a\u5361\u4f4f\uff0c\u5bf9\u7740\u663e\u793a\u5668\u6253\u76f9\u534a\u5bbf\uff0c\u7a81\u7136\u60ca\u9192\u540e\u5c31\u660e\u767d \u4e86\uff0c\u5fc3\u6ee1\u610f\u8db3\u7684\u56de\u5bb6\u3002</p>"},{"location":"writing/2010/#mar-3-2010","title":"Mar 3, 2010","text":"<p>\u4e09\u56db\u70b9\u7684\u592a\u9633\uff0c\u6b63\u5bf9\u7740\u6211\u7684\u7a97\u6237\uff0c\u9633\u5149\u5148\u843d\u5728\u684c\u5b50\u4e0a\uff0c\u518d\u626b\u5230\u5b9e\u9a8c\u53f0\u3002\u6b64\u65f6\u5e72\u6d3b \u6781\u4e0d\u65b9\u4fbf\uff0c\u6211\u4fbf\u62ce\u7740\u8dd1\u978b\u4e0b\u697c\u4e86\u3002\u4e00\u5927\u65e9\u53bb\u5b9e\u9a8c\u5ba4\u7684\u6700\u5927\u597d\u5904\uff0c\u83ab\u8fc7\u4e8e\u53ef\u4ee5\u4e0b\u5348 \u5e72\u6d3b\u5e72\u5230\u4e00\u534a\u53bb\u953b\u70bc\uff0c\u8fdb\u5ea6\u8fd8\u4e0d\u53d7\u5f71\u54cd\u3002</p>"},{"location":"writing/2010/#mar-4-2010","title":"Mar 4, 2010","text":"<p>\u5fd9\u4e4e\u4e86\u4e00\u5929\uff0c\u5728\u82e5\u5e72\u5b9e\u9a8c\u5668\u6750\u4e4b\u95f4\u8dd1\u6765\u8dd1\u53bb\uff0c\u4e5f\u5c31\u4e2d\u5348\u7684seminar\u4f11\u606f\u4e86\u4f1a\uff0c\u524d \u534a\u65f6\u95f4\u5403\u996d\uff0c\u540e\u534a\u65f6\u95f4\u6253\u76f9\u3002</p> <p>seminar\u4e0a\u8eab\u8fb9\u4e09\u4e2a\u4eba\u7761\u89c9\uff0c\u6b64\u8d77\u5f7c\u4f0f\u7684\u70b9\u5934\u3002seminar\u8bb2\u663c\u591c\u8282\u5f8b\uff0c\u6b63\u597d\u662f\u4e09\u4e2a \u86cb\u767d\u6b64\u8d77\u5f7c\u4f0f\u6d3b\u52a8\u7684\u7ed3\u679c\u3002</p>"},{"location":"writing/2010/#mar-6-2010","title":"Mar 6, 2010","text":"<p>\u6628\u665a\u73a9Dragon Age Origin\uff0c\u62dc\u8bbf\u7cbe\u7075\u90e8\u843d\uff0c\u7279\u6392\u5916\uff0c\u800c\u4e14\u5176\u5b83\u5730\u65b9\u968f\u4fbf\u5f00\u5b9d\u7bb1 \u987a\u4e1c\u897f\u90fd\u6ca1\u4eba\u542d\u4e00\u58f0\uff0c\u7cbe\u7075\u90e8\u843d\u91cc\u78b0\u4e00\u4e0b\u5c31\u88ab\u9a82\u3002\u665a\u4e0a\u505a\u68a6\uff0c\u81ea\u5df1\u5e26\u961f\u4f0d\u5230\u7cbe\u7075 \u90e8\u843d\u91cc\u53bb\u4e86\uff0c\u4e0d\u8fc7\u7cbe\u7075\u957f\u8001\u68c0\u6d4b\u4eba\u7684\u65b9\u5f0f\u7279\u5947\u602a\uff0c\u7a81\u51fb\u987a\u65f6\u9488\u6216\u9006\u65f6\u9488\u6320\u4f60\u811a\u5e95 \u677f\uff0c\u7136\u540e\u4f60\u5f97\u53cd\u65b9\u5411\u7528\u811a\u7ed5\u5708\uff0c\u53cd\u5e94\u7a0d\u5fae\u6162\u70b9\u5c31\u8981\u88ab\u6cfc\u4e00\u6876\u9ecf\u7cca\u7cca\u7684\u6c34\u3002\u9192\u6765\u53d1 \u73b0\u53f3\u817f\u662f\u9ebb\u7684\u3002</p>"},{"location":"writing/2010/#mar-9-2010","title":"Mar 9, 2010","text":"<p>\u6709\u4e00\u5b66\u671f\u4e0a\u9f99\u6f2b\u8fdc\u8001\u5e08\u7684\u5206\u5b50\u8fdb\u5316\u8bfe\uff0c\u8bb2\u4ed6\u53d1\u73b0\u7684jingwei\u57fa\u56e0\u3002\u7531\u4e8e\u548c\u9152\u7cbe\u6709 \u5173\uff0c\u6211\u601d\u8003\u4e86\u534a\u5929\u8ba4\u5b9a\u662f\u91d1\u5a01\u5564\u9152\u7684jingwei\uff0c\u76f4\u5230\u770b\u4e86\u677e\u9f20\u4f1a\u7684\u91c7\u8bbf\uff0c\u624d\u77e5\u9053 \u662f\u7cbe\u536b\u3002</p>"},{"location":"writing/2010/#mar-18-2010","title":"Mar 18, 2010","text":"<p>\u524d\u5929\u8001\u677f\u8f7b\u63cf\u6de1\u5199\u7684\u8bf4\u6709\u4e2a\u4eba\u8981\u6765\u5b66\u5b9e\u9a8c\u6280\u672f\uff0c\u4f60\u5e26\u4e00\u4e0b\u4ed6\u3002\u6628\u5929\u4e00\u95ee\uff0c\u539f\u6765\u662f \u4e2a\u65b0\u8001\u677f\u3002\u665a\u4e0a\u68a6\u5230\u6765\u4e86\u4e2a\u8001\u677f\uff0c\u5e26\u51e0\u4e2a\u5b66\u751f\uff0c\u5168\u90fd\u4e0d\u5c51\u6211\u7684\u6c34\u5e73\uff0c\u4e0d\u670d\u6211\u5e26\u5b9e \u9a8c\uff0c\u9192\u6765\u82e6\u7b11\u3002\u4eca\u5929\u4eba\u6765\u4e86\uff0c\u968f\u548c\uff0c\u5b66\u751f\u6c14\uff0c\u6ca1\u67b6\u5b50\uff0c\u8d70\u8def\u8fd8\u8ddf\u5728\u6211\u540e\u9762\uff0c\u505a\u5b9e \u9a8c\u4e0d\u505c\u53c2\u8003\u6211\u7684\u610f\u89c1\uff0c\u53cd\u5012\u7279\u4e0d\u9002\u5e94\u3002</p>"},{"location":"writing/2010/#mar-29-2010","title":"Mar 29, 2010","text":"<p>\u751f\u5316\u5371\u673a5</p> <p>\u5468\u672b\u901a\u4e86\u4e24\u6b21\u5173\u3002\u7531\u6050\u6016\u6e38\u620f\u53d8\u6210\u67aa\u6218\u4e86\uff0c\u9664\u4e86\u7535\u952f\u7537\u548c\u673a\u67aa\u7537\uff0c\u6ca1\u6709\u4ec0\u4e48\u80fd\u7ed9 \u5fc3\u7406\u538b\u529b\u3002\u4eba\u7269\u523b\u753b\u751f\u786c\u3002\u4e24\u4e2a\u4eba\u4e00\u8d77\u8dd1\u8def\u600e\u4e48\u5c31\u4e0d\u804a\u70b9\u6709\u8da3\u7684\u5462\uff0c\u90fd\u7f3a\u4e4f\u5e7d\u9ed8 \u611f\u3002\u6253\u5b57\u673a\u6539\u6210\u4e86\u5b58\u76d8\u70b9\uff0c\u6000\u5ff5\u4e4b\u524d\u62d6\u7740\u6b65\u5b50\u627e\u6253\u5b57\u673a\u7684\u611f\u89c9\u3002\u5b58\u76d8\u70b9\u975e\u5e38\u9891\u7e41\uff0c \u964d\u4f4e\u4e86\u96be\u5ea6\u3002</p> <p>\u589e\u52a0\u4e86\u5e55\u540e\u8fd0\u8f93\u56e2\uff0c\u6240\u6709\u96be\u5ea6\u548c\u7ae0\u8282\u5171\u4eab\u4e00\u4e2a\u7269\u54c1\u8d44\u91d1\u5e93\uff0c\u4e8e\u662f\u53ef\u4ee5\u5237\u94b1\u5237\u7269\u54c1\uff0c \u7b49\u4e8e\u6709\u65e0\u9650\u836f\u8349\u3002\u53c8\u662f\u4e2a\u964d\u4f4e\u7d27\u5f20\u611f\u7684\u8bbe\u5b9a\u3002</p> <p>\u6709\u4e9b\u6b20\u6241\u7684QTE\uff0c\u6700\u751a\u7684\u662f\u6700\u7ec8boss\u6218\u91cc\u7684\u63a8\u77f3\u5934\uff0c\u8981\u72c2\u6309\u4e0d\u540c\u7684\u51e0\u4e2a\u6309\u952e\uff0c\u5fc3\u75bc\u624b\u67c4\u3002</p> <p>\u7531\u5355\u4eba\u8fc7\u5173\u6539\u4e3a\u4e86\u548cAI\u642d\u6863\uff0c\u6700\u6028\u6068\u7684\u662f\u4e3a\u4e86\u62ff\u641c\u96c6\u9e21\u86cb\u7684trophy\u56db\u5904\u627e\u4f4e\u6982\u7387 \u8910\u8272\u86cb\uff0c\u7ed3\u679c\u521a\u7784\u5230\u5c31\u88abAI\u62a2\u5148\u62ff\u6765\u5403\u4e86\u3002\u3002\u3002</p> <p>\u4e0d\u8fc7\u5237\u94b1\u4e5f\u86ee\u597d\u73a9\uff0c\u547d\u4e5f\u4e0d\u8981\u4e86\u72c2\u627e\u5b9d\u7269</p> <p>\u6253Jill\u7684\u90a3\u573a\u6ca1\u7b11\u6b7b\u6211\uff0c\u62fc\u547d\u626fJill\u80f8\u524d\u7684\u63a7\u5236\u5668\uff0c\u626f\u4e86N\u6b21\u624d\u4e0b\u6765\uff0c\u96be\u9053\u662f\u4e07 \u80fd\u80f6\u8d34\u7684\uff0c\u800c\u4e14\u8fd9\u4e1c\u897f\u76f4\u63a5\u8d34\u5230\u8089\u4e0a\u7684\uff0c\u770b\u7740\u90fd\u75bc\u3002</p>"},{"location":"writing/2010/#apr-5-2010","title":"Apr 5, 2010","text":"<p>\u751f\u53165 pro\u96be\u5ea6\u901a\u5173\uff0c\u6742\u5175\u9760\u65e0\u9650\u5f39\u836f\uff0c\u96be\u7f20\u7684boss\u4e0a\u65e0\u9650\u706b\u7bad\u7b52\u3002\u8bf4\u96be\u4e0d\u96be\uff0c \u6b66\u5668\u5f3a\u5927\uff0c\u654c\u4eba\u51fa\u73b0\u7684\u4f4d\u7f6e\u56fa\u5b9a\uff0c\u6709\u4e9b\u5173\u7528\u6765\u590d\u67aa\u706d\u6389\u57cb\u4f0f\u7684\u654c\u4eba\u518d\u60a0\u95f2\u7684\u8d70\u8fdb \u53bb\u3002\u8bf4\u6613\u4e0d\u6613\uff0c\u4e00\u51fb\u6bd9\u547d\u7684\u8bbe\u5b9a\u4e0d\u4eba\u9053\uff0c\u5373\u4f7f\u78b0\u5230\u5c0f\u5175\u4e5f\u4e00\u6478\u5c31\u6302\uff0cpro\u96be\u5ea6\u4e0b \u7684AI\u89c1\u6211\u5feb\u6302\u4e86\u4e5f\u50bb\u6123\u7740\u4e0d\u6551\u3002\u597d\u5728\u5b58\u76d8\u70b9\u591a\uff0c\u4e00\u88ab\u6253\u5c31\u91cd\u6765\u4e5f\u4e0d\u4f1a\u8d39\u592a\u591a\u529f\u592b\u3002 \u6700\u53d8\u6001\u7684\u5de8\u4eba\u5173\uff0c\u6b7b\u4e8643\u6b21\u3002\u6253Jill\u4e5f\u70e6\u4eba\uff0cAI\u8001\u81ea\u5df1\u8dd1\u53bb\u6328\u67aa\u6302\u6389\uff0c\u558a\u90fd\u558a\u4e0d \u56de\u6765\u3002</p> <p>\u6e38\u620f\u53ef\u73a9\u5ea6\u8fd8\u662f\u4e0d\u9519\uff0c\u901a\u5173\u591a\u6b21\u6ca1\u89c9\u5f97\u65e0\u804a\uff0c\u53ea\u662f\u60c5\u8282\u51e0\u4e4e\u4e27\u5931\u4e86\u751f\u5316\u5371\u673a\u7684\u8001 \u5143\u7d20\uff0c\u4e0d\u6050\u6016\uff0c\u800c\u4e14\u4e0d\u89c9\u5f97\u6253\u7684\u662f\u50f5\u5c38\uff0c\u66f4\u50cf\u662f\u5165\u4fb5\u975e\u6d32\u539f\u59cb\u90e8\u843d\u3002</p>"},{"location":"writing/2010/#mar-7-2010","title":"Mar 7, 2010","text":"<p>\u8fd1\u671f\u5b9a\u7684\u51e0\u4e2a\u76ee\u6807\uff0c\u4e24\u4e2a\u534a\u6708\u5185\u5b8c\u6210\uff0c\u7136\u540e\u53bb\u9ec4\u77f3\u516c\u56ed\u73a9\uff0c\u4e5f\u8bb8\u987a\u4fbf\u56de\u56fd\u3002</p> <ul> <li>\u751f\u5316\u5371\u673a5 pro\u6a21\u5f0f\u901a\u5173\uff0c\u62ff\u767d\u91d1trophy</li> <li>\u7533\u8bf7\u7ecf\u8d39</li> <li>\u6587\u7ae0\u5199\u597d\u6295\u51fa\u53bb</li> <li>\u901a\u8fc7\u535a\u58eb\u751f\u8d44\u683c\u8003\u8bd5</li> </ul> <p>\u7ed3\u679c1\u610f\u6599\u4e4b\u5916\u63d0\u524d\u5b8c\u6210\uff0c\u4e3a\u4e862-4\u7684\u6210\u529f\uff0c\u6e38\u620f\u624b\u67c4\u80fd\u501f\u7684\u501f\u51fa\u53bb\uff0c\u6e38\u620f\u5df2\u501f\u51fa \u53bb\u522b\u8981\u56de\u6765\uff0c\u5269\u4e0b\u7684\u7edf\u7edf\u9501\u8d77\u6765\u628a\u94a5\u5319\u79fb\u4ea4\u670b\u53cb\u7ba1\u7406</p> <p>\u987a\u4fbfBS\u4e0bSONY\uff0c\u53d6\u6d88\u4e86\u5bf9linux\u7684\u652f\u6301\uff0c\u4e3a\u4e86\u7ee7\u7eed\u7528\u5927\u7535\u89c6\u770bpaper\u4e0a\u7684\u56fe\uff0c\u53ea\u597d \u4e0d\u5347\u7ea7\u80a5\u7248PS3\u4e86\u3002</p>"},{"location":"writing/2010/#apr-12-2010","title":"Apr 12, 2010","text":"<p>\u770b\u4e86&lt;\u62fe\u7a57\u8005&gt;\uff0c\u89d2\u5ea6\u7279\u522b\u7684\u7eaa\u5f55\u7247\uff0c\u8001\u592a\u592a\u4e3e\u7740\u6444\u50cf\u673a\u56db\u5904\u5bfb\u627e\u62fe\u7a57\u8005\u3002\u6240\u8c13\u62fe \u7a57\uff0c\u5e7f\u4e49\u6765\u8bf4\u5c31\u662f\u6361\u522b\u4eba\u5269\u4e0b\u4e0d\u8981\u7684\u4e1c\u897f\uff0c\u6bd4\u5982\u7530\u91ce\u91cc\u5728\u6536\u83b7\u5b63\u8282\u540e\u6361\u5269\u4f59\u6c34\u679c \u852c\u83dc\u7684\uff0c\u6bd4\u5982\u57ce\u91cc\u5728\u83dc\u5e02\u573a\uff0c\u9762\u5305\u5e97\u5916\uff0c\u5783\u573e\u6876\u91cc\u627e\u5403\u7684\uff0c\u6bd4\u5982\u6536\u96c6\u522b\u4eba\u6254\u6389\u7684 \u5bb6\u5177\u7535\u5668\u3002\u6bcf\u4e2a\u4eba\u90fd\u6709\u4e0d\u540c\u76ee\u7684\uff0c\u6709\u7684\u662f\u751f\u6d3b\u6240\u8feb\uff0c\u6709\u7684\u770b\u4e0d\u60ef\u73b0\u4ee3\u4eba\u7684\u6d6a\u8d39\uff0c \u6709\u7684\u4e3a\u4e86\u827a\u672f\uff0c\u6709\u7684\u8ff7\u604b\u65e7\u7269\u6240\u5305\u542b\u7684\u5386\u53f2\u3002\u6700\u6709\u8da3\u7684\u662f\u4e2a\u9752\u5e74\u4eba\uff0c\u6bcf\u5929\u5230\u8857\u4e0a \u6361\u852c\u83dc\u6c34\u679c\u9762\u5305\uff0c\u6bcf\u62ff\u8d77\u4e00\u6837\u4e1c\u897f\uff0c\u90fd\u628a\u8425\u517b\u8bf4\u5f97\u5934\u5934\u662f\u9053\uff0c\u975e\u5e38\u6ce8\u91cd\u996e\u98df\u7ed3\u6784\uff0c \u4e00\u95ee\uff0c\u539f\u6765\u662f\u4e2a\u751f\u7269\u7855\u58eb\uff0c\u4f4f\u96be\u6c11\u7a9f\uff0c\u6559\u6587\u76f2\u4eec\u8bc6\u5b57\u3002\u955c\u5934\u8f6c\u5230\u4ed6\u7684\u8bc6\u5b57\u73ed\uff0c\u6559 \u5b66\u767d\u677f\u753b\u5f97\u975e\u5e38\u7ec6\u81f4\uff0c\u751a\u81f3\u6bd4\u6211\u4e0a\u8fc7\u7684\u4efb\u4f55\u8bfe\u90fd\u7ec6\u81f4\uff0c\u6bcf\u4e2a\u8bcd\u8bed\u90fd\u914d\u6709\u76f8\u5f53\u4e0d\u9519 \u7684\u5f69\u56fe\u3002\u8fd9\u662f\u4f55\u7b49\u7684\u5949\u732e\u7cbe\u795e\u554a\u3002</p>"},{"location":"writing/2010/#apr-13-2010","title":"Apr 13, 2010","text":"<p>\u5728\u5e08\u59d0\u7684\u9f13\u52b1\u4e0b\u53bb\u5b66\u6c34\u5f69\u4e86\uff0c\u62d6\u7740\u61d2\u6563\u7684\u6b65\u5b50\u51fa\u95e8\uff0c\u60ca\u559c\u7684\u56de\u6765\u3002\u6709\u8001\u5e08\u6559\u5c31 \u662f\u4e0d\u4e00\u6837\uff0c\u4e5f\u5f88\u559c\u6b22\u90a3\u4e2a\u8272\u5f69\u6742\u4e71\u5374\u6709\u8da3\u7684\u6c1b\u56f4\u3002\u8001\u5e08\u8bf4\uff0cto draw is to make connection\uff0c\u4e00\u53e5\u8bdd\u5c31\u8ba9\u6211\u5fc3\u7518\u60c5\u613f\u7684\u7f34\u94b1\u4e0a\u8bfe\u4e86\u3002</p>"},{"location":"writing/2010/#apr-14-2010","title":"Apr 14, 2010","text":"<p>\u6211\u671f\u76fc\u80fd\u51fa\u73b0\u5374\u89c9\u5f97\u4e0d\u5927\u6709\u5e0c\u671b\u7684\u53d1\u660e\uff1a</p> <ul> <li>\u53ef\u6298\u53e0\u7684\u7535\u5b50\u4e66\uff0c\u8ddf\u7eb8\u4e00\u6837\u8584\uff0c\u6253\u5f00\u4e3aA4\u5927\u5c0f\uff0c\u6298\u53e0\u540e\u53ef\u4ee5\u585e\u94b1\u5305\u91cc\u3002</li> <li>\u4e00\u79cd\u663e\u5f71\u6280\u672f\uff0c\u80fd\u8ba9\u4eba\u7269\u8bf4\u8bdd\u5e26\u5bf9\u8bdd\u6846\uff0c\u50cfRPG\u6e38\u620f\u91cc\u4e00\u6837\uff0c\u8ba9\u751f\u6d3b\u66f4\u6709\u6e38\u620f\u611f\u3002</li> <li>\u627e\u5b9e\u7269\u53ef\u4ee5\u5f00\u641c\u7d22\u680f</li> </ul>"},{"location":"writing/2010/#apr-19-2010","title":"Apr 19, 2010","text":"<p>\u61d2\u5bb3\u6b7b\u4eba\u3002\u60f3\u6574\u7406\u4e0a\u4f20\u4e00\u4e9b\u6700\u8fd1\u753b\u7684\u753b\uff0c\u82b130\u5206\u949f\u641c\u7d22\u52a0\u601d\u8003\u5bfb\u627e\u6574\u7406\u56fe\u7247\u7684\u6700 \u4f73\u65b9\u6cd5\uff0c\u82b110\u5206\u949f\u601d\u8003\u6211\u4e3a\u4ec0\u4e48\u61d2\u5f97\u628a\u626b\u63cf\u4eea\u642c\u51fa\u6765\uff0c\u6700\u540e\u5934\u75bc10\u5206\u949f\uff0c\u4f5c\u7f62\u3002</p>"},{"location":"writing/2010/#apr-20-2010","title":"Apr 20, 2010","text":"<p>\u8001\u677f\u5413\u6211\u8bf4\u8d44\u683c\u8003\u8bd5\u8981\u5c3d\u65e9\u5b9a\u65f6\u95f4\uff0c\u56e0\u4e3acommittee member\u7684\u65e5\u7a0b\u8868\u6ee1\u5f97\u7279\u522b\u5feb\uff0c \u56db\u4e2a\u4eba\u53c8\u96be\u7edf\u4e00\u3002\u6211\u4e00\u6025\uff0c\u80e1\u4e71\u9009\u4e865\u4e2a\u65f6\u95f4\u5c31\u53d1\u51fa\u53bb\uff0c\u679c\u7136\u6700\u540e\u53ea\u6709\u4e00\u4e2a\u80fd\u6ee1 \u8db3\u6240\u6709\u4eba\uff0c\u56de\u5934\u4e00\u770b\uff0c\u662f\u516d\u4e00\u513f\u7ae5\u8282\u3002</p>"},{"location":"writing/2010/#apr-24-2010","title":"Apr 24, 2010","text":"<p>\u5174\u8da3\u7231\u597d\u4e00\u5411\u6316\u5751\u4e0d\u586b\u5751\uff0c\u4e00\u4e0b\u53cd\u5e94\u8fc7\u6765\u8fd8\u6709\u79d1\u7814\u8fd9\u4e2a\u6700\u5927\u7684\u5751</p>"},{"location":"writing/2010/#apr-26-2010","title":"Apr 26, 2010","text":"<p>\u7b49\u8f66\u7684\u65f6\u5019\u65e0\u804a\u89c2\u5bdf\u4eba\uff0c\u53d1\u73b0\u5927\u4f53\u6765\u8bf4\uff0c\u5185\u516b\u5b57\u7684\u4eba\u8f6c\u5f2f\u5148\u51fa\u8fdc\u79bb\u8f6c\u8f74\u7684\u811a\uff0c\u4e8e \u662f\u53d8\u65b9\u5411\u7684\u77ac\u95f4\u8fd8\u662f\u5185\u516b\u5b57\uff1b\u5916\u516b\u5b57\u7684\u4eba\u8f6c\u5f2f\u5219\u5148\u51fa\u9760\u8fd1\u8f6c\u8f74\u7684\u811a\uff0c\u53d8\u65b9\u5411\u77ac\u95f4 \u662f\u5916\u516b\u5b57\u3002\u6211\u771f\u662f\u65e0\u804a\u963f\u3002\u3002\u3002</p>"},{"location":"writing/2010/#apr-27-2010","title":"Apr 27, 2010","text":"<p>\u6628\u5929\u68a6\u5230\u7533\u8bf7\u7ecf\u8d39\uff0c\u7814\u7a76\u53d1\u5446\u7684\u539f\u7406\uff0c\u4e0d\u540c\u7269\u79cd\u95f4\u6bd4\u8f83\u3002</p>"},{"location":"writing/2010/#apr-29-2010","title":"Apr 29, 2010","text":"<p>\u6700\u8fd1\u7684\u65f6\u95f4\u88ab\u5b9e\u9a8c\u548c\u51c6\u5907\u8d44\u683c\u8003\u8bd5\u6324\u5f97\u53d7\u4e0d\u4e86\uff0c\u800c\u62d6\u62c9\u7684\u9996\u8981\u8bf1\u56e0\u662f\u61d2\u60f0\u548c\u90c1\u95f7\uff0c \u4e8e\u662f\u8111\u888b\u72af\u8d31\u60f3\u51fa\u4e2a\u529e\u6cd5\uff0c\u9760\u8fd0\u52a8\u6765\u7ed9\u8111\u888b\u91cc\u9001\u5185\u5561\u80bd\uff0c\u4fdd\u6301\u5fc3\u60c5\u6109\u5feb\uff0c\u5982\u679c\u4e00 \u5929\u5b8c\u6210\u9884\u5b9a\u4efb\u52a1\uff0c\u665a\u4e0a\u5c31\u5403\u9897\u7cd6\uff0c\u5f62\u6210\u5fc3\u7406\u6b63\u53cd\u9988\u3002\u76ee\u524d\u6548\u679c\u5c45\u7136\u4e0d\u9519\uff0c\u4e00\u5468\u5927 \u698210\u5c0f\u65f6\u7684\u8fd0\u52a8\u65f6\u95f4\uff0c\u56e0\u4e3a\u6d88\u8017\u5927\u6240\u4ee5\u996d\u91cf\u4e5f\u6210\u4e861.5-2\u500d\u3002\u552f\u4e00\u4e0d\u723d\u7684\u662f\u6bcf\u5929 \u4e0b\u6765\u90fd\u6bd4\u8f83\u7d2f\uff0c\u8ba9\u6211\u6000\u7591\u8fd9\u4e2a\u8ba1\u5212\u5c31\u662f\u5927\u8111\u5728\u7cca\u5f04\u8eab\u4f53\u3002</p>"},{"location":"writing/2010/#may-1-2010","title":"May 1, 2010","text":"<p>\u559c\u6b22\u8fd0\u52a8\uff0c\u5bf9\u751f\u6d3b\u5145\u6ee1\u4e86\u61d2\u60f0\u4e4b\u5fc3\u3002</p>"},{"location":"writing/2010/#may-10-2010","title":"May 10, 2010","text":"<p>\u4e3a\u4e86\u597d\u597d\u7684\u4e0a\u6c34\u5f69\u8bfe\uff0c\u6309\u8981\u6c42\u4e70\u4e86\u8d28\u91cf\u597d\u7684\u753b\u6750\uff0c\u90a3\u4e2a\u8d35\u3002\u54ac\u7259\u5207\u9f7f\u7684\u5e7b\u60f3\u4ee5\u540e \u6709\u94b1\u4e86\u81ea\u5df1\u641e\u4e2a\u753b\u5ba4\uff0c\u518d\u96c7\u4e2a\u4eba\u5929\u5929\u7ed9\u6211\u5173\u6ce8\u753b\u6750\u6253\u6298\uff0c\u6ee1\u8db3\u6211\u5fc3\u6ee1\u610f\u8db3\u770b\u753b\u6750 \u964d\u4ef7\u7684\u5fc3\u6001\u3002</p>"},{"location":"writing/2010/#may-11-2010","title":"May 11, 2010","text":"<p>\u4eba\u54c1\u592a\u5dee\uff0c\u663e\u5fae\u955c\u4e00\u89c1\u5230\u6211\u5c31\u51fa\u95ee\u9898\uff0c\u56fe\u50cf\u5c45\u7136\u4ece\u8367\u51492D\u56fe\u53d8\u6210\u5e26\u9634\u5f71\u76843D\u4e86\uff0c \u800c\u8fd9\u53f0\u663e\u5fae\u955c\u5e76\u6ca1\u6709\u88c5DIC\u6210\u50cf\u7cfb\u7edf\uff0c\u6c57\u3002\u4f17\u4eba\u7eb7\u7eb7\u8868\u793a\u6ca1\u89c1\u8fc7\u663e\u5fae\u955c\u8fd9\u4e48\u7a7f\u8d8a \u7684\u3002</p>"},{"location":"writing/2010/#may-15-2010","title":"May 15, 2010","text":"<p>\u901a\u5173\u585e\u5c14\u8fbe\u4f20\u8bf4-\u7075\u9b42\u8f68\u8ff9\u3002\u753b\u98ce\u4e00\u5982\u65e2\u5f80\u7684\u53ef\u7231\uff0c\u516c\u4e3b\u6709\u6027\u683c\uff0c\u8ff7\u9898\u6709\u8da3\uff0c\u96be \u5f97\u8d8a\u5230\u540e\u9762\u8d8a\u7cbe\u5f69\u3002boss\u6218\u638c\u63e1\u6280\u5de7\u4e4b\u540e\u4e0d\u96be\u3002\u4e0d\u8db3\u4e4b\u5904\u662f\u5927\u5730\u56fe\u6ca1\u6709\u6377\u5f84\uff0c\u53bb \u54ea\u90fd\u8981\u5f00\u706b\u8f66\uff0c\u6709\u65f6\u633a\u7e41\u7410\u3002\u6536\u96c6\u7684\u7269\u54c1\u57fa\u672c\u6ca1\u6d3e\u4e0a\u7528\u573a\u3002</p> <p>NDSL\u7684L1\u952e\u574f\u6389\u4e86\uff0c\u600e\u4e48\u6309\u90fd\u6ca1\u53cd\u5e94\uff0c\u7f51\u4e0a\u641c\u5230\u4e00\u4e2a\u96be\u4ee5\u4fe1\u4efb\u7684\u65b9\u6cd5:\u7528\u5634\u628aL1 \u952e\u5468\u56f4\u5168\u90e8\u5c01\u4f4f\uff0c\u7136\u540e\u5439\u51e0\u6b21\u6c14\u3002\u5c45\u7136\u771f\u4fee\u597d\u4e86\u3002</p>"},{"location":"writing/2010/#may-16-2010","title":"May 16, 2010","text":"<p>\u60f3\u505a\u6df7\u5408\u578b\u4eba\u624d\uff0c\u5374\u6210\u4e86\u6df7\u86cb\u578b\u4eba\u624d</p> <p>\u6c34\u5f69\u8bfe\u8001\u592a\u592a\u5f88\u591a\uff0c\u53d1\u89c9\u65e0\u8bba\u4e2d\u5916\uff0c\u6211\u90fd\u662f\u8001\u592a\u592a\u4e4b\u53cb\u3002</p>"},{"location":"writing/2010/#may-26-2010","title":"May 26, 2010","text":"<p>\u649e\u5899\u611f\u65e5\u76ca\u52a0\u6df1\uff0c\u5199proposal\u8d8a\u5f80\u540e\u8d8a\u632a\u4e0d\u52a8\u952e\u76d8\uff0c\u867d\u7136\u4e1c\u897f\u90fd\u5dee\u4e0d\u591a\u4e86\uff0c\u4eca\u5929 \u8fd8\u5f97\u77e5\u867d\u7136\u8fd9\u4e00\u5e74\u505a\u7684\u9879\u76ee\u4e86\u7ed3\u4e0d\u9700\u8981\u518d\u505a\u4e0b\u53bb\uff0c\u4f46\u4e5f\u8981\u5199\u4e2a\u4e00\u9875\u7684\u62a5\u544a\uff0c\u8bc1\u660e \u6211\u7b2c\u4e00\u5e74\u4e0d\u662f\u65e0\u6240\u4e8b\u4e8b\u5ea6\u8fc7\u7684\u3002\u4e0b\u5468\u4e8c\u8003\uff0c\u6f14\u8bb2\u5e7b\u706f\u7247\u8fd8\u6ca1\u5f00\u5de5\uff0c\u4e00\u60f3\u5230\u5c31\u6d51\u8eab \u7d27\u5f20\u3002\u7d27\u5f20\u5c31\u7d27\u5f20\u5427\uff0c\u8981\u662f\u8fd9\u51e0\u5929\u628a\u5934\u8111\u91cc\u7684\u7d27\u5f20\u5c0f\u4eba\u5168\u90e8\u5413\u6b7b\uff0c\u5230\u8003\u65f6\u53cd\u800c\u4e0d \u7d27\u5f20\uff0c\u5012\u4e5f\u4e0d\u9519\u3002</p> <p>\u8981\u770b\u5927\u91cf\u6587\u7ae0\uff0c\u72d7\u6025\u8df3\u5899\u5bfb\u627e\u6700\u4f73\u8bfb\u6cd5\uff0c\u679c\u7136\u662f\u5e26\u7740\u95ee\u9898\u8bfb\u6700\u6709\u6548\u3002\u62ff\u5f20\u767d\u7eb8\uff0c \u628a\u6240\u60f3\uff0c\u6240\u95ee\u7684\u5199\u4e0b\uff0c\u76f4\u5954\u6587\u7ae0\u91cc\u5173\u952e\u6570\u636e\uff0c\u518d\u4ee5\u79d1\u666e\u6a21\u5f0f\u753b\u5728\u7eb8\u4e0a\u3002\u5c31\u50cf\u5907\u8bfe \u4e00\u6837\uff0c\u76ee\u7684\u662f\u53ef\u4ee5\u51ed\u7740\u8fd9\u5f20\u7eb8\u628a\u6587\u7ae0\u7684\u91cd\u70b9\u8f6c\u8ff0\u7ed9\u522b\u4eba\uff08\u5305\u62ec\u5c06\u6765\u7684\u81ea\u5df1\uff09\uff0c\u81f3 \u6b64\u5c31\u8bfb\u591f\u4e86\uff0c\u53d6\u51b3\u4e8e\u6587\u7ae0\uff0c\u770b\u4e00\u7bc75\uff0d30\u5206\u949f\uff0c\u5e73\u574715\u5206\u949f\u3002\u8fb9\u770b\u79ef\u6512\u7684\u6587\u7ae0\uff0c \u8fb9 \u8d23\u602a\u81ea\u5df1\u600e\u4e48\u4ee5\u524d\u4e0d\u8fd9\u4e48\u5e72\uff0c\u5168\u7559\u5230\u73b0\u5728\u8bfb\u3002</p> <p>\u9664\u4e86\u6587\u7ae0\u6ca1\u8bfb\u591f\uff0c\u8fd8\u6709\u4e2a\u5927\u9519\u662f\u6ca1\u5229\u7528\u597d\u4eba\u529b\u8d44\u6e90\uff0c\u76f4\u5230\u8001\u677f\u63d0\u793a\u6211\u4e0d\u662f\u4e00\u4e2a\u4eba \u6218\u6597\uff0c\u624d\u53bb\u5f00\u53e3\u50cf\u5b9e\u9a8c\u5ba4\u524d\u8f88\u95e8\u7d22\u8981\u4ed6\u4eec\u5f53\u521d\u8d44\u683c\u8003\u8bd5\u65f6\u51c6\u5907\u7684\u6750\u6599\u3002\u501f\u9274\u4e86\u522b \u4eba\u7684\u6587\u7ae0\u683c\u5f0f\uff0c\u56fe\u7247\uff0c\u5e7b\u706f\u7247\u540e\uff0c\u53d1\u73b0\u81ea\u5df1\u8d70\u4e86\u5f88\u591a\u5f2f\u8def\uff0cproposal\u91cc\u4e0d\u5fc5\u8981\u7684 \u4e1c\u897f\u5199\u4e00\u5806\uff0c\u91cd\u70b9\u5374\u64e6\u8fb9\u8fc7\u53bb\uff0c\u5e78\u597d\u53d1\u73b0\u5f97\u4e0d\u7b97\u592a\u665a\uff0c\u8d76\u7d27\u5927\u5e45\u5ea6\u4fee\u6539\u3002\u5176\u5b9e\u5199 \u4e4b\u524d\u5c31\u8be5\u591a\u95ee\u95ee\u3002</p> <p>\u4e0b\u5348\u5077\u95f2\u53bb\u540c\u5b66\u5bb6\u73a9rock band 2\uff0c\u5f53\u521d\u88ab\u8fd9\u4e2a\u6e38\u620f+\u540c\u5b66\u53d1\u7684\u7f8e\u5973\u9f13\u624b\u89c6\u9891\u5ffd \u60a0\u53bb\u4e70\u7535\u9f13\u5b66\uff0c\u81f3\u4eca\u7adf\u7136\u90fd\u67094\u4e2a\u591a\u6708\u4e86\uff0c\u8ba1\u5212\u89c4\u8303\uff0c\u65f6\u95f4\u5374\u96f6\u6563\u3002\u8fdb\u6b65\u8fd8\u662f\u53ef \u4ee5\u770b\u51fa\u6765\uff0c\u4e8e\u662f\u53c8\u6709\u52a8\u529b\u518d\u53bb\u7ec3\u57fa\u7840\u3002</p> <p>\u6211\u4eec\u5b9e\u9a8c\u5ba4\u53ea\u8981\u6709\u4eba\u8fc7\u751f\u65e5\uff0c\u5176\u4f59\u7684\u4eba\u5c31\u4f1a\u9001\u4e00\u5f20\u5361\u7247\uff0c\u6bcf\u4eba\u5199\u4e2a\u795d\u798f\u7684\u8bdd\uff0c\u4e0d \u8fc7\u7ea6\u5b9a\u4fd7\u6210\u4e0d\u80fd\u8ba9\u8fd9\u4e2a\u4eba\u63d0\u524d\u770b\u5230\u5927\u5bb6\u5728\u51c6\u5907\u5361\u7247\u3002\u522b\u4eba\u8fc7\u751f\u65e5\u65f6\u611f\u89c9\u4e0d\u5230\uff0c\u8f6e \u5230\u81ea\u5df1\u4e86\uff0c\u624d\u89c9\u5f97\u86ee\u597d\u73a9\u7684\uff0c\u8981\u88c5\u4f5c\u4e0d\u77e5\u9053\u5361\u7247\u7684\u5b58\u5728\uff0c\u4f46\u5176\u5b9e\u77e5\u9053\u522b\u4eba\u5728\u5077\u5077 \u4f20\u9012\u5361\u7247\u3002</p>"},{"location":"writing/2010/#may-31-2010","title":"May 31, 2010","text":"<p>\u660e\u5929\u8003\u8d44\u683c\u8003\u8bd5\uff0c\u4eca\u5929\u8bd5\u8bb2\u4e86\u4e0b\uff0c\u5b9e\u9a8c\u5ba4\u7684\u4eba\u5f88\u7ed9\u9762\u5b50\uff0c\u5927\u8fc7\u8282\u7684\u8fd8\u57fa\u672c\u90fd\u6765\u4e86\u3002 \u4e0d\u8fc7\u8c8c\u4f3c\u6211\u7684\u81ea\u4fe1\u503c\u548c\u88ab\u9f13\u52b1\u7684\u6b21\u6570\u6210\u53cd\u6bd4\uff0c\u8d8a\u88ab\u5938\u8d8a\u89c9\u5f97\u522b\u4eba\u770b\u9519\u4eba\u4e86\u3002\u597d\u5728 \u53cd\u6b63\u6211\u4e5f\u4e0d\u7d27\u5f20\u3002</p> <p>\u5ba4\u53cb\u660e\u5929\u5c31\u6d77\u5f52\uff0c\u7ed3\u679c\u8fd8\u662f\u53ea\u80fd\u996f\u522b\uff0c\u6ca1\u6cd5\u5f00\u8f66\u9001\u4e00\u4e0b\uff0c\u6700\u8fd1\u5fd9\u7740\u81ea\u5df1\u7684\u4e8b\uff0c\u4e5f \u6ca1\u6709\u591a\u805a\u4e00\u4e0b\uff0c\u6211\u679c\u7136\u4e0d\u662f\u4e2a\u5f88\u79f0\u804c\u7684\u670b\u53cb\uff0c\u4ece\u670b\u53cb\u90a3\u5f97\u5230\u7684\u603b\u662f\u6bd4\u6211\u7ed9\u670b\u53cb\u7684 \u591a\u3002</p>"},{"location":"writing/2010/#jun-1-2010","title":"Jun 1, 2010","text":"<p>\u4e00\u5927\u65e9\u9001\u5ba4\u53cb\u4e0a\u8f66\uff0c\u5f88\u7b80\u77ed\uff0c\u4e8b\u540e\u611f\u53f9\u81ea\u5df1\u8fde\u795d\u798f\u548c\u611f\u8c22\u7684\u8bdd\u90fd\u8bf4\u4e0d\u597d\u3002\u4e00\u76f4\u89c9 \u5f97\u6211\u4e0d\u662f\u4e2a\u5f88\u5408\u683c\u7684\u670b\u53cb\uff0c\u4e0d\u600e\u4e48\u61c2\u5f97\u4e3a\u4eba\u7740\u60f3\uff0c\u8868\u8fbe\u548c\u884c\u52a8\u80fd\u529b\u90fd\u504f\u4e0b\uff0c\u6ca1\u6cd5 \u8ba9\u4eba\u611f\u52a8\u4e0d\u8bf4\uff0c\u88ab\u670b\u53cb\u611f\u52a8\u4e86\u8fd8\u4e0d\u597d\u610f\u601d\u8c22\u4e00\u4e2a\uff0c\u6240\u4ee5\u548c\u4eba\u76f8\u5904\u6ca1\u5565\u4fe1\u5fc3\u3002\u76f4\u5230 \u642c\u5230\u8fd9\u4e2a\u5730\u65b9\u8ba4\u8bc6\u4e86\u5ba4\u53cb\uff0c\u53c2\u8003\u4e86\u4ed6\u7684\u4e3a\u4eba\u5904\u4e16\uff0c\u624d\u89e3\u51b3\u4e86\u4e4b\u524d\u7684\u5f88\u591a\u56f0\u60d1\uff0c\u4e5f \u6162\u6162\u660e\u767d\u5b64\u72ec\u7ed9\u6211\u7684\u76ca\u5904\u6ca1\u6709\u60f3\u8c61\u4e2d\u591a\u3002\u8fd9\u6b21\u8003\u8bd5\u987a\u5229\uff0c\u5f88\u5927\u90e8\u5206\u5f52\u529f\u4e8e\u8fd9\u6bb5\u65f6 \u95f4\u91cc\u966a\u6211\u5403\u559d\u73a9\u4e50\u7684\u670b\u53cb\u3002</p> <p>\u4e24\u5468\u51c6\u5907\u8d44\u683c\u8003\u8bd5\uff0c\u51c6\u5907\u7684\u597d\u4e0d\u597d\u5148\u4e0d\u8c08\uff0c\u8010\u5fc3\u5012\u662f\u78e8\u7684\u5dee\u4e0d\u591a\u4e86\u3002\u4e00\u5f00\u59cb\u6548\u7387 \u5f88\u9ad8\uff0c\u5728\u4e00\u4e2a\u5730\u65b9\u5446\u70e6\u4e86\u5c31\u6362\u4e2a\u5730\u513f\u3002\u540e\u6765\u56fe\u4e66\u9986\uff0c\u5b9e\u9a8c\u5ba4\uff0c\u5bb6\u91cc\uff0c\u9910\u5385\u90fd\u8bd5\u8fc7 \u4e00\u904d\u540e\uff0c\u65b0\u9c9c\u611f\u6d88\u5931\uff0c\u6548\u7387\u5927\u964d\u3002\u6628\u665a\u5b9e\u5728\u51c6\u5907\u4e0d\u4e0b\u53bb\uff0c\u5c31\u6253\u4e86\u4e00\u665a\u4e0a\u65b0\u8d85\u7ea7\u739b \u4e3d\u4e86\u4e8b\u3002\u65e9\u4e0a\u518d\u51c6\u5907\u4e86\u4e00\u628a\u6f14\u8bb2\uff0c\u4e34\u8003\u524d\u8fd8\u5728\u8d76\u7a3f\u5b50\uff0c\u4e00\u8fb9\u5543\u5348\u9910\u4e00\u8fb9\u5355\u624b\u6253\u5b57\uff0c \u5b8c\u6210\u8fdb\u5ea6\u62a5\u544a\uff0c\u4e5f\u4e0d\u7ba1\u5199\u6210\u5565\u6837\u4e86\u3002\u6253\u5370n\u4efd\u540e\u676f\u5177\u7684\u53d1\u73b05\u5f20\u6700\u7ec8\u7a3f\u91cc\u6df7\u4e86\u4e00\u5f20 \u8349\u7a3f\uff0c\u534a\u5929\u6311\u51fa\u6765\uff0c\u7136\u540e\u62ce\u7740\u6170\u52b3\u6559\u6388\u4eec\u7684\u98df\u7269\u8d76\u53bb\u4f1a\u8bae\u5ba4\uff0c\u5851\u6599\u7eb8\u5305\u88c5\u8fd8\u6495\u4e86 n\u4e45\u624d\u6253\u5f00\u3002\u7ec8\u4e8e\u5fcd\u4e0d\u4f4f\u5bf9\u81ea\u5df1\u8bf4\uff0c\u8003\u5565\u6837\u662f\u5565\u6837\uff0c\u6211\u4e0d\u7ba1\u4f60\u4e86\u3002</p> <p>\u4e34\u8003\u524d\u60b2\u6124\u8fd9\u4e48\u4e00\u4e0b\uff0c\u53cd\u800c\u5b8c\u5168\u4e0d\u7d27\u5f20\uff0c\u8bb2\u7684\u6bd4\u8f83\u7a33\uff0c\u4e5f\u6ca1\u5fd8\u8bb0\u8981\u70b9\u3002 committee\u6bd4\u8f83\u539a\u9053\uff0c\u6ca1\u600e\u4e48\u6253\u51fb\u6211\u7684\u5f31\u9879\u2013\u7f3a\u4e4f\u80cc\u666f\u77e5\u8bc6\uff0c\u6587\u7ae0\u770b\u4e0d\u591f\u591a\uff0c\u91cd\u8981 \u5b9e\u9a8c\u7ed3\u679c\u8bb0\u4e0d\u4f4f\uff0c\u91cd\u8981\u4eba\u7269\u8fc7\u76ee\u5c31\u5fd8\u3002\u867d\u7136\u8bf4\u4e86\u51e0\u6b21\u4e0d\u77e5\u9053\uff0c\u4f46\u5927\u90e8\u5206\u95ee\u9898\u90fd\u7b54 \u4e86\u3002</p> <p>\u524d\u8f88\u4eec\u7ed9\u7684\u5efa\u8bae\u5f88\u53d7\u7528\uff0c\u6bd4\u5982\u5fc3\u7406\u4e0a\u4e0d\u8981\u6015\u88ab\u4e3a\u96be\uff0c\u6bd5\u7adfcommittee\u7684\u76ee\u7684\u662f\u5e2e \u52a9\u6211\u8ba1\u5212\u5c06\u6765\u7684\u8bfe\u9898\uff0c\u95ee\u7684\u95ee\u9898\u518d\u5201\u94bb\uff0c\u4e5f\u662f\u5c06\u6765\u53ef\u80fd\u9047\u5230\u7684\u5b9e\u9645\u95ee\u9898\uff0c\u63d0\u65e9\u60f3 \u6ca1\u574f\u5904\u3002</p> <p>\u867d\u8bf4\u8fc7\u4e86\u4e24\u5468\u7684\u975e\u6b63\u5e38\u751f\u6d3b\uff0c\u4e5f\u4e0d\u5728\u7406\u60f3\u72b6\u6001\uff0c\u4f46\u4f9d\u7136\u6536\u76ca\u9887\u591a\uff0c\u51c6\u5907\u8003\u8bd5\u7684\u8fc7 \u7a0b\u4e2d\u4e0d\u65ad\u6316\u6398\u5230\u81ea\u5df1\u7684\u7f3a\u70b9\uff0c\u4ee5\u81f3\u4e8e\u518d\u4e0d\u5feb\u70b9\u8003\uff0c\u6211\u90fd\u8981\u7ed9\u81ea\u5df1\u5224\u4e0d\u53ca\u683c\u4e86\u3002</p> <p>\u665a\u4e0a\u5230\u5b9e\u9a8c\u5ba4\u4e00\u4e2a\u670b\u53cb\u5bb6\u805a\u4f1a\u73a9\u6e38\u620f\uff0crock band\uff0c\u6211\u8d1f\u8d23\u9f13\u3002\u676f\u5177\u7684\u662f\u9f13\u69cc\u53ea \u6709\u4e00\u6839\uff0c\u53ea\u597d\u7528\u52fa\u5b50\u4ee3\u66ff\u4e86\uff0c\u6548\u679c\u5c45\u7136\u8fd8\u4e0d\u7b97\u60e8\uff0c\u53ea\u662f\u53ef\u601c\u4e86\u670b\u53cb\u7684\u52fa\u5b50\u3002</p>"},{"location":"writing/2010/#jun-3-2010","title":"Jun 3, 2010","text":"<p>\u4e2d\u5348\u5b9e\u9a8c\u5ba4\u51fa\u53bb\u5403\u996d\u5e86\u795d\u6211\u8003\u8bd5\u901a\u8fc7\uff0c\u8d70\u7684\u65f6\u5019\u53d1\u73b0\u6240\u6709\u4eba\u90fd\u901a\u77e5\u4e86\u5c31\u6ca1\u901a\u77e5\u6211\u3002 \u5df2\u7ecf\u5728\u5916\u5403\u996d\u7684\u6211\u95fb\u8baf\u8d76\u56de\u6765\u5403\u4e86\u7b2c\u4e8c\u987f\uff0c\u4e00\u76f4\u7ba1\u9971\u7ba1\u5230\u4e0b\u5348\u56db\u70b9\uff0c\u6b63\u597d\u662f\u6ce2\u5170 \u5b66\u751f\u6f14\u8bb2\u5b8c\uff0c\u5f00\u5403\u6ce2\u5170\u98df\u7269\u7684\u65f6\u95f4\uff0c\u6491\u5f97\u4e0d\u884c\u66f4\u998b\u5f97\u4e0d\u884c\uff0c\u4e00\u54ac\u7259\u53c8\u6298\u817e\u6211\u51c4\u60e8 \u7684\u80c3\u3002</p>"},{"location":"writing/2010/#jun-10-2010","title":"Jun 10, 2010","text":"<p>\u4ece\u8003\u5b8c\u8bd5\u5f00\u59cb\uff0c\u538b\u529b\u7a81\u7136\u51cf\u5c0f\uff0c\u8111\u888b\u91cc\u7684\u61d2\u60f0\u5c0f\u4eba\u5168\u6b22\u5feb\u7684\u8dd1\u51fa\u6765\uff0c\u901a\u4e86\u4e24\u4e09\u4e2a \u6e38\u620f\uff0c\u53c8\u548c\u540c\u5b66\u5408\u6253\u4e09\u56fd\u65e0\u53cc\uff0c\u51e0\u6b21\u4e0d\u6253\u5230\u5929\u4eae\u4e0d\u7f62\u4f11\u3002\u8e0f\u8e0f\u5b9e\u5b9e\u61d2\u4e86\u4e03\u5929\u540e\uff0c \u516c\u4ea4\u8f66\u4e0a\u4e00\u53ea\u6bdb\u6bdb\u866b\u6389\u5728\u4e66\u5305\u4e0a\uff0c\u60f3\u5fc5\u4ee5\u4e3a\u6211\u662f\u61d2\u866b\u540c\u7c7b\u3002\u9057\u61be\u7684\u8bf4\u61d2\u65f6\u5e76\u4e0d\u6bd4 \u52e4\u5feb\u65f6\u5feb\u4e50\uff0c\u6d88\u8017\u5c11\u4e86\u5403\u996d\u90fd\u6ca1\u80c3\u53e3\uff0c\u7ec8\u4e8e\u5fcd\u4e0d\u4f4f\u5bf9\u81ea\u5df1\u8bf4\uff0c\u8fd9\u65e5\u5b50\u6211\u53d7\u591f\u4e86\uff0c \u8fd8\u662f\u5e72\u6d3b\u53bb\u5427\u3002</p>"},{"location":"writing/2010/#jul-22-2010","title":"Jul 22, 2010","text":"<p>\u56de\u56fd\u72c2\u5403\u72c2\u73a9\u4e86\u4e24\u5468\uff0c\u56de\u6765\u5374\u90c1\u95f7\u4e86\u4e00\u5929\u3002\u5bf9\u6bd4\u4e86\u4e0b\u81ea\u5df1\u60f3\u505a\uff0c\u505a\u8fc7\uff0c\u548c\u6b63\u5728\u505a \u7684\u4e8b\u60c5\uff0c\u7ed3\u8bba\u662f\u6211\u66fe\u7ecf\u4e00\u4e8b\u65e0\u6210\uff0c\u6b63\u5728\u4e00\u4e8b\u65e0\u6210\uff0c\u5e76\u5c06\u8981\u4e00\u4e8b\u65e0\u6210\u3002</p>"},{"location":"writing/2010/#jul-24-2010","title":"Jul 24, 2010","text":"<p>\u5165\u5173\u53ea\u80cc\u4e86\u4e2a\u4e66\u5305\uff0c\u56e0\u4e3a\u884c\u674e\u592a\u5c11\u800c\u88ab\u6d77\u5173\u76d8\u95ee\u4e86\u8bb8\u4e45\u3002</p> <p>\u56e0\u4e3a\u5ea6\u5047\u6ca1\u505a\u4e8b\u800c\u4ea7\u751f\u7684\u6127\u759a\u611f\u5f88\u5feb\u5c31\u6d88\u5931\u4e86\uff0c\u53cd\u5012\u89c9\u5f97\u6709\u4e00\u5806\u4e8b\u60c5\u7b49\u7740\u505a\u662f\u5e78 \u798f\u3002\u5f97\u77e5\u56db\u5929\u540e\u8981\u4e0a\u4ea4\u4e2agrant\uff0c\u5f53\u5934\u4e00\u68d2\u3002</p> <p>\u4ece\u673a\u573a\u5230\u8fbe\u81ea\u5df1\u7684\u7834\u5bbf\u820d\uff0c\u751a\u81f3\u6bd4\u56de\u56fd\u8fd8\u4eb2\u5207\uff0c\u6bd5\u7adf\u81ea\u5df1\u7684\u7834\u5e8a\u7761\u5f97\u6700\u8e0f\u5b9e\u3002\u60f3 \u5ff5\u5728\u829d\u52a0\u54e5\u7684\u670b\u53cb\u540c\u4e8b\u4eec\uff0c\u89c1\u9762\u867d\u4e0d\u6ea2\u4e8e\u8a00\u8868\uff0c\u5fc3\u91cc\u5374\u65e0\u6bd4\u9ad8\u5174\u3002</p> <p>\u901a\u5173\u4e86\u4f20\u8bf4\u4e2d\u7684Bayonetta\uff0c\u65e0\u6bd4\u4f18\u79c0\u7684\u52a8\u4f5c\u6e38\u620f\uff0c\u64cd\u4f5c\u65b9\u9762\u5f88\u6709\u56bc\u5934\u3002\u53ef\u60dc\u6211 \u592a\u83dc\uff0c\u8981\u4e0d\u662f\u4e3a\u4e86\u770b\u901a\u5173\u7684\u94a2\u7ba1\u821e\uff0c\u4f30\u8ba1\u56e0\u4e3a\u607c\u4eba\u7684\u9ed8\u8ba4\u89c6\u89d2\u5c31\u653e\u5f03\u4e86\u3002</p> <p>\u68a6\u89c1\u7535\u996d\u9505\u91cc\u7684\u996d\u653e\u4e86\u4e09\u4e2a\u661f\u671f\uff0c\u4e00\u770b\uff0c\u9505\u76d6\u8df3\u4e86\u4e00\u4e0b\uff0c\u6211\u4e00\u8eab\u9e21\u76ae\u7599\u7629\u4e0d\u77e5\u9053 \u600e\u4e48\u5904\u7406\u3002\u5c4b\u91cc\u94bb\u51fa\u4e00\u53ea\u677e\u9f20\uff0c\u539f\u6765\u662f\u996d\u91cc\u957f\u51fa\u6765\u7684\u3002\u60f3\u7528\u76c6\u76d6\u4f4f\uff0c\u53ef\u5bb6\u91cc\u4e00\u5806 \u76c6\u90fd\u6709\u6d1e\uff0c\u76d6\u4f4f\u4e86\uff0c\u5b83\u53c8\u94bb\u51fa\u6765\u54ac\u4eba\u62a5\u590d\u3002</p>"},{"location":"writing/2010/#jul-25-2010","title":"Jul 25, 2010","text":"<p>\u53ef\u80fd\u56e0\u4e3a\u996e\u98df\u8bcd\u6c47\u91cf\u5c11\uff0c\u6211\u53c8\u7279\u522b\u6015\u4ea4\u6d41\u969c\u788d\uff0c\u6240\u4ee5\u6015\u70b9\u83dc\uff0c\u5c24\u5176\u6015\u7ad9\u5230\u67dc\u53f0\u524d \u8fd8\u8981\u88ab\u95ee\u4e00\u5806\u95ee\u9898\u3002\u6765\u7f8e\u56fd\u7b2c\u4e00\u6b21\u8fdb\u9ea6\u5f53\u52b3\uff0c\u4e0d\u50cf\u56fd\u5185\u6709\u7eb8\u83dc\u5355\u62ff\uff0c\u53ea\u80fd\u62ac\u5934\u770b \u5899\u58c1\u3002\u6211\u5c31\u51fa\u6765\u4e86\uff0c\u597d\u51e0\u5e74\u540e\u624d\u53bb\u5403\u3002</p>"},{"location":"writing/2010/#aug-2-2010","title":"Aug 2, 2010","text":"<p>\u963f\u7518\u8bf4\u751f\u6d3b\u662f\u76d2\u5de7\u514b\u529b\uff0c\u6211\u89c9\u5f97\u4ed6\u6f0f\u4e86\u4e24\u4e2a\u5b57\uff1a\u8fc7\u671f\u3002</p>"},{"location":"writing/2010/#aug-7-2010","title":"Aug 7, 2010","text":"<p>\u6e38\u620f\u767d\u91d1\u8fbe\u6210\u540e\uff0c\u6709\u79cd\u5927\u75c5\u51fa\u72f1\u7684\u611f\u89c9\u3002\u4ecenormal\u96be\u5ea6\u52a8\u4e0d\u52a8\u88ab\u5361\u4f4f\uff0c\u8fdb\u5316\u5230 climax\u8fc5\u901f\u901a\u5173\uff0c\u770b\u6765\u4eba\u8111\u7684\u9002\u5e94\u80fd\u529b\u53ef\u89c2\u3002\u4e0d\u8fc7\u4e5f\u60ed\u6127\u7684\u8bf4\uff0c\u6709\u8fd9\u529f\u592b\uff0c\u505a\u7814 \u7a76\u53bb\u5427\u3002</p>"},{"location":"writing/2010/#aug-15-2010","title":"Aug 15, 2010","text":"<p>\u6700\u8fd1\u5199\u4fe1\u5199\u5f97\u591a\uff0c\u5c31\u4e0d\u7ed9\u81ea\u5df1\u5199\u65e5\u8bb0\u4e86</p>"},{"location":"writing/2010/#aug-22-2010","title":"Aug 22, 2010","text":"<p>\u8d85\u6267\u52002 NDS\u901a\u5173\u3002\u5e78\u597d\u6709\u4f4e\u96be\u5ea6\u7684\uff0c\u8981\u4e0d\u4f30\u8ba1\u4e0d\u80fd\u901a\u5173\u3002</p> <p>\u53c8\u5230\u540c\u5b66\u5bb6\u73a9rock band\u53bb\u4e86\uff0c\u8d81\u642c\u5bb6\u524d\u6700\u540e\u6572\u8bc8\u4e00\u6b21\u3002\u68c0\u9a8c\u4e86\u8fd1\u6765\u5b66\u9f13\u7684\u6210\u679c\u3002 \u4e00\u76f4\u57cb\u5934\u7ec3\u57fa\u672c\u529f\uff0c\u4e5f\u4e0d\u77e5\u9053\u90a3\u4e24\u672c\u8c31\u5b50\u85cf\u7740\u4ec0\u4e48\u5965\u79d8\u3002\u56de\u56fd\u552f\u4e00\u505a\u7684\u6b63\u4e8b\u5c31\u662f \u5bf9\u7740\u8282\u62cd\u5668\u6572\u6795\u5934\u3002\u73a9\u4e86rock band\u624d\u6b23\u559c\u7684\u53d1\u73b0\u679c\u7136\u6709\u8fdb\u6b65\u3002\u901f\u5ea6\u5feb\u4e86\u4e9b\uff0c\u624b \u811a\u72ec\u7acb\u6027\u4e5f\u589e\u5f3a\u3002</p> <p>\u7ec3\u4e60\u753b\u753b\uff0c\u62ff\u73a9\u5177\u5f53\u6a21\u7279\uff0c\u7ec3\u4e60\u8868\u8fbe\u7acb\u4f53\u611f\u3002\u6211\u6700\u5927\u7684\u95ee\u9898\u5728\u4e8e\u5bf9\u6574\u4f53\u7684\u660e\u6697\u6ca1 \u6709\u4e2a\u628a\u63e1\uff0c\u753b\u9634\u5f71\u65f6\u4e00\u4e0b\u89c9\u5f97\u8fd9\u4e2a\u662f\u6697\u5904\uff0c\u4e00\u4e0b\u53c8\u89c9\u5f97\u8fd9\u4e2a\u4e5f\u662f\u6697\u5904\uff0c\u6700\u540e\u6574\u4e2a \u56fe\u90fd\u753b\u6210\u4e86\u9634\u5f71\u3002\u9ad8\u5149\u548c\u6700\u6697\u7684\u5730\u65b9\u672c\u8be5\u70b9\u5230\u4e3a\u6b62\uff0c\u6211\u5374\u975e\u8981\u624b\u8d31\u591a\u6d82\u51e0\u7b14\uff0c\u672c \u6765\u5bf9\u6bd4\u9c9c\u660e\u7684\u5730\u65b9\u53c8\u6a21\u7cca\u4e00\u7247\u4e86\u3002</p>"},{"location":"writing/2010/#sep-15-2010","title":"Sep 15, 2010","text":"<p>\u9ad8\u8fbe-\u5409\u7fc1\u524d\u7ebfPS2</p> <p>\u4e3b\u89d2\u65b9\u5f88\u5bb9\u6613\u88ab\u79d2\u6740\u3002\u4ee5\u5409\u7fc1\u7684\u89d2\u5ea6\u6765\u770b\u4e00\u5e74\u6218\u4e89\u91cc\u7684\u5404\u4e2a\u5c0f\u6218\u5f79\u3002\u9664\u4e8613\u4e2a\u60c5 \u8282\u5173\u5361\uff0c\u8fd8\u670930\u591a\u4e2a\u77ed\u5c0f\u7684\u8bad\u7ec3\u5173\uff0c\u5185\u5bb9\u4e30\u5bcc\uff0c\u91cd\u590d\u6027\u4e0d\u5927\u3002\u654c\u6211\u6218\u529b\u60ac\u6b8a\uff0c\u8054 \u90a6\u7684\u9ad8\u8fbe\u53ea\u8981\u4e00\u70ae\uff0c\u624e\u53e4\u5c31\u5012\u4e0b\uff0c\u5373\u4f7f\u9762\u5bf9\u5c0f\u5175\uff0c\u80cc\u540e\u88ab\u51fb\u4e2d\u4e5f\u5b8c\u86cb\uff0c\u6240\u4ee5\u7cbe\u9ad3 \u662f\u6218\u672f\u3002\u5173\u548c\u5173\u7684\u5dee\u5f02\u5f88\u5927\uff0c\u6ca1\u6709\u7edf\u4e00\u653b\u7565\uff0c\u5982\u679c\u4e0d\u7814\u7a76\u654c\u4eba\u8def\u7ebf\uff0c\u76f4\u51b2\u4f1a\u6b7b\u5f88 \u60e8\u3002\u8981\u662f\u6709\u5174\u81f4\uff0c\u6162\u6162\u7814\u7a76\u53c2\u6218\u5c0f\u961f\uff0c\u88c5\u5907\uff0c\u6218\u6597\u8def\u7ebf\uff0c\u4f1a\u76f8\u5f53\u6709\u6210\u5c31\u611f\u3002\u6218\u6597 \u7684\u64cd\u63a7\u611f\u4e0d\u9519\uff0c\u867d\u7136\u4e0a\u624b\u8981\u4e00\u5b9a\u65f6\u95f4\u3002\u6bcf\u6b21\u51fa\u6218\u53ef\u4ee5\u90091-3\u4e2a\u5c0f\u961f\uff0c\u968f\u65f6\u53ef\u4ee5\u5207 \u6362 \u64cd\u4f5c\u54ea\u4e2a\u961f\u4f0d\uff0c\u4e0d\u64cd\u4f5c\u7684\u961f\u4f0d\u7684\u53ef\u4ee5\u4ea4\u7ed9AI\uff0c\u4e5f\u53ef\u4ee5\u9009\u62e9\u539f\u5730\u5f85\u547d\u3002</p> <p>\u4efb\u52a1\u5206\u4e09\u7c7b\uff0c\u6d88\u706d\uff0c\u4fdd\u536b\uff0c\u6216\u8005\u63a2\u6d4b\u3002\u4e0d\u8fc7\u5b9e\u9645\u4e0a\u5dee\u522b\u4e0d\u5927\uff0c\u90fd\u662f\u6839\u636e\u73af\u5883\u4e0d\u540c\uff0c \u5207\u6362\u4e09\u79cd\u96f7\u8fbe\u6765\u5bfb\u627e\u654c\u4eba\uff0c\u540c\u65f6\u4e5f\u5e72\u6270\u654c\u4eba\u7684\u96f7\u8fbe\u6765\u8eb2\u907f\uff0c\u7ed5\u5230\u8eab\u540e\u6253\u6e38\u51fb\u6218\u3002 \u9700\u8981\u6478\u9ad8\u8fbe\u7684\u5173\u90fd\u4e0d\u5bb9\u6613\uff0c\u7ecf\u5e38\u521a\u627e\u5230\uff0c\u5c31\u88ab\u9ad8\u8fbe\u5148\u51fa\u624b\u706d\u4e86\uff0c\u6574\u5173\u91cd\u6253\u3002\u540e\u671f \u6709\u4e86\u5e72\u6270\u5668\uff0c\u624d\u53ef\u4ee5\u5207\u9ad8\u8fbe\u5982\u5207\u83dc\u3002\u6e38\u620f\u96be\u5ea6\u53d6\u51b3\u4e8e\u6218\u672f\uff0c\u548c\u5c11\u91cf\u7684\u8fd0\u6c14\u3002\u4e8c\u5468 \u76ee\u6709\u5956\u52b1\u8bad\u7ec3\u5173\u5361\u548c\u88c5\u5907\uff0c\u5982\u679c\u5168\u8bc4\u4ef7S\u8fd8\u80fd\u5956\u52b1\u4e00\u4e9b\u8bad\u7ec3\u5173\u5361\u548c\u9690\u85cf\u673a\u4f53\u3002\u6bd4 \u8f83\u9ebb\u70e6\u7684\u662f\u4e0d\u80fd\u81ea\u7531\u9009\u5173\uff0c\u5982\u679c\u67d0\u5173\u6ca1\u62ff\u5230S\uff0c\u4e0d\u80fd\u76f4\u63a5\u91cd\u590d\u6253\u4e00\u6b21\uff0c\u5f97\u91cd\u8bfb\u6863\u3002 \u6240\u4ee5\u4e8c\u5468\u76eeS\u5230\u7b2c9\u5173\u6211\u5c31\u5c01\u76d8\uff0c\u4e0d\u6311\u6218\u5168S\u4e86\u3002</p>"},{"location":"writing/2010/#sep-17-2010","title":"Sep 17, 2010","text":"<p>\u5e02\u4e2d\u5fc3\u5f00\u4f1a\u56de\u6765\uff0c\u8def\u8fc7\u4e00\u5730\u4e0b\u901a\u9053\uff0c\u7b49\u7ea2\u706f\u65f6\u53ea\u6709\u6211\u4e00\u8f86\u8f66\uff0c\u4e0d\u8fdc\u5904\u51e0\u4e2a\u4f30\u8ba1\u662f \u62a2\u8f66\u515a\u4eba\u7269\uff0c\u5176\u4e2d\u4e00\u4e2a\u7b11\u8138\u8fc7\u6765\u81ea\u8350\u4e0a\u8f66\u6307\u8def\u3002\u53ea\u597d\u6de1\u5b9a\u76b1\u7709\u88c5\u542c\u4e0d\u61c2\u8ba9\u4ed6\u91cd\u590d\uff0c \u968f\u65f6\u505a\u597d\u95ef\u7ea2\u706f\u8dd1\u6389\u7684\u51c6\u5907\u3002\u7eff\u706f\u4e00\u4eae\uff0c\u8d76\u7d27\u5f80\u5730\u9762\u4e0a\u843d\u8352\u800c\u9003\uff0c\u51b3\u5b9a\u518d\u4e0d\u5165\u5730 \u4e0b\u3002</p>"},{"location":"writing/2010/#oct-5-2010","title":"Oct 5, 2010","text":"<p>\u4eca\u5929\u4ece\u65e9\u5f00\u59cb\u5012\u9709\uff0c\u4e0b\u5348\u6253\u7b97\u4e70\u7f50\u53ef\u4e50\u8212\u7f13\u5fc3\u60c5\uff0c\u4e00\u5143\u7eb8\u5e01\u8fdb\u53bb\uff0c\u8d29\u5356\u673a\u663e\u793a\u5356 \u5b8c\u4e86\uff0c\u9000\u7ed9\u6211\u6c89\u7538\u7538\u768420\u4e2a5\u5206\u786c\u5e01\u3002\u4eba\u5012\u9709\uff0c\u8fde\u673a\u5668\u90fd\u8981\u6765\u8c03\u620f\u3002\u56de\u5230\u5b9e\u9a8c\u5ba4\uff0c \u505c\u6c34\u4e86\uff0c\u53ef\u601c\u6211\u8d70\u4e4b\u524d\u7559\u4e0b\u4e00\u5806\u8981\u6d17\u7684\u70c2\u644a\u5b50\u3002\u627e\u4e86\u4e2a\u5927\u53f7\u7684\u914d\u6eb6\u6db2\u7684\u676f\u5b50\u5f53\u76c6\uff0c \u63a5\u6c34\u7bb1\u7684\u6c34\u5148\u7528\u7740\uff0c\u8fb9\u6d17\u8fb9\u5bf9\u5927\u676f\u5b50\u6df1\u601d\uff0c\u770b\u6765\u676f\u5177\u8db3\u591f\u5927\uff0c\u5c31\u6210\u53d8\u6d17\u5177\u4e86\u3002</p>"},{"location":"writing/2010/#oct-13-2010","title":"Oct 13, 2010","text":"<p>\u5bc4\u5e0c\u671b\u4e8e\u4f53\u611f\u6e38\u620f\u673a\u7684\u53d1\u5c55\uff0c\u671f\u5f85\u4ee5\u540e\u51fa\u73b0\u9ad8\u7cbe\u5ea6\u6a21\u62df\u683c\u6597\u6e38\u620f\uff0c\u8ba9\u6211\u7b49\u624b\u65e0\u7f1a \u9e21\u4e4b\u529b\u7684\u4e09\u5934\u8eab\u4eba\u7269\u4e5f\u80fd\u5c1d\u5230\u62df\u771f\u6241\u4eba\u7684\u4e50\u8da3\u3002\u5347\u9f99\u62f3\uff0c\u65cb\u98ce\u817f\u4e4b\u7c7b\u8fdd\u53cd\u91cd\u529b\u7684 \u62db\u6570\u5c31\u7b97\u4e86\uff0c\u4f46\u53d1\u6ce2\u5e94\u8be5\u662f\u53ef\u4ee5\u52a0\u5165\u7684\u3002\u9274\u4e8e\u65e0\u6cd5\u4f30\u7b97\u6240\u9700\u65f6\u95f4\uff0c\u73b0\u9636\u6bb5\u53ea\u80fd\u575a \u6301\u953b\u70bc\u8eab\u4f53\uff0c\u4fdd\u8bc15\uff0c60\u5c81\u4e86\u8fd8\u80fd\u62f3\u6253\u811a\u8e22\u3002</p>"},{"location":"writing/2010/#nov-26-2010","title":"Nov 26, 2010","text":"<p>\u901b\u4e86\u611f\u6069\u8282\u7684\u5348\u591c\u573a\uff0c\u4eba\u751f\u5b8c\u6574\u4e86\u3002\u5934\u5929\u665a\u4e0a\u6253\u6e38\u620f\u63d0\u795e\u5230\u534a\u591c1\u70b9\uff0c\u51fa\u53d1\uff0c2\u70b9 \u5230\u8fbe\u3002\u6218\u6597\u5b8c\u7b2c\u4e8c\u5929\u65e9\u4e0a11\u70b9\uff0c\u89c2\u5bdf\u901b\u8857\u4eba\u7fa4+\u8ddf\u98ce\u901b\u8857\uff0c\u524d\u8005\u66f4\u6709\u610f\u601d\u3002\u6ca1\u770b \u4e2d \u4ec0\u4e48\u4e1c\u897f\uff0c\u53ea\u6dfb\u4e86\u4e24\u6761\u725b\u4ed4\u88e4\u3002\u4e00\u76f4\u51b7\u7740\u6240\u4ee5\u6682\u65f6\u6ca1\u6709\u7761\u610f\uff0c\u867d\u7136\u6b64\u65f6\u5df2\u7ecf 26\u5c0f \u65f6\u4e4b\u5185\u53ea\u5408\u773c\u4e862\u5c0f\u65f6\u3002\u56de\u5230\u4e34\u65f6\u505c\u8f66\u573a\u4e00\u770b\uff0c\u4e00\u7247\u7a7a\u5730\u53ea\u6709\u4e00\u8f86\u8f66\uff0c\u770b\u6765 \u6211\u4eec\u662f \u540c\u65f6\u5230\u8fbe\u7684\u4e00\u6279\u4eba\u91cc\u6700\u5f3a\u608d\u7684\uff0c\u522b\u4eba\u90fd\u8d70\u4e86\u3002\u4e09\u4eba\u96c6\u4e2d\u529b\u90fd\u660e\u663e\u4e0d\u884c\uff0c \u672c\u6765\u60f3\u9014 \u7ecf\u6e38\u620f\u5546\u5e97\u518d\u6253\u6e38\u620f\u63d0\u63d0\u795e\uff0c\u540e\u6765\u53d1\u73b0\u6211\u53cd\u5e94\u80fd\u529b\u4e0b\u964d\u592a\u591a\uff0c\u8def\u4e0a\u591a \u4e00\u5f00\u5206\u949f\u90fd \u662f\u5371\u9669\uff0c\u5c31\u8d76\u7d27\u56de\u5bb6\u4e86\uff0c\u4e00\u8def\u5f00\u56de\u6765\u867d\u7136\u6ca1\u51fa\u4e8b\uff0c\u8fd8\u662f\u5f88\u540e\u6015\u3002</p>"},{"location":"writing/2010/#nov-29-2010","title":"Nov 29, 2010","text":"<p>gran turismo\u6536\u85cf\u7248\u6536\u5230\u4e86\uff0c\u611f\u52a8\u7684\u6253\u5f00\uff0c\u628a\u8dd1\u8f66\u6a21\u578b\u6446\u51fa\u6765\u4f9b\u7740\uff0c\u8fd8\u9001\u4e86\u672c\u8d5b \u8f66\u624b\u518c\u3002\u4e1c\u897f\u90fd\u505a\u7684\u5f88\u7cbe\u7ec6\uff0c\u867d\u8bf4\u6027\u4ef7\u6bd4\u5f88\u4f4e\uff0c\u4e5f\u7b97\u5bf9\u5f97\u8d77\u7c89\u4e1d\u4e86\u3002\u6253\u5f00\u6e38\u620f\uff0c \u7247\u5934\u662f\u8f66\u4ece\u5382\u91cc\u7ec4\u88c5\u5230\u4e0a\u8def\u7684\u9ad8\u6e05\u5f71\u7247\uff0c\u5206\u91cf\u8db3\u3002</p> <p>\u611f\u6069\u8282\u6ca1\u4e70\u4ec0\u4e48\u4e1c\u897f\uff0c\u6e38\u620f\u5012\u662f\u7eb7\u7eb7\u964d\u4ef7\uff0c\u5fc3\u91cc\u957f\u8349\u7684\u6e38\u620f\u57fa\u672c\u90fd\u5c6f\u8d77\u6765\u4e86\u3002\u987a \u4fbf\u73a9\u4e86\u5927\u5c0f\u661f\u7403\uff0c\u7c98\u571f\u4e16\u754c\u7684\u753b\u98ce\uff0c\u8d85\u7ea7\u739b\u4e3d\u7684\u64cd\u4f5c\u548c\u6d41\u7a0b\uff0c\u53ea\u8981\u6709\u59b9\u5b50\u4f5c\u5ba2\u90fd \u53ef\u4ee5\u628a\u8fd9\u4e2a\u6e38\u620f\u62ff\u51fa\u6765\u3002\u96be\u5ea6\u8fd8\u53ef\u4ee5\uff0c\u867d\u7136\u6709\u4e00\u4e9b\u5173\u6709\u4e9b\u607c\u4eba\uff0c\u8fd8\u662f\u80fd\u57285\u6b21\u5c1d \u8bd5\u4e4b\u5185\u901a\u8fc7\u3002\u5355\u4eba\u6e38\u620f\u6d41\u7a0b\u610f\u5916\u7684\u77ed\uff0c\u6240\u4ee5\u5f88\u5feb\u901a\u5173\uff0c\u4e4b\u540e\u4e0a\u7f51\u8ddf\u4eba\u5408\u4f5c\u6253\u5173\uff0c \u56e0\u4e3a\u6709\u4e9b\u5730\u65b9\u9700\u8981\u591a\u4eba\u8e29\u673a\u5173\u624d\u80fd\u53bb\uff0c\u5927\u5bb6\u9ed8\u9ed8\u7684\u914d\u5408\uff0c\u7b97\u4e2d\u7b49\u6709\u8da3\uff0c\u5982\u679c\u662f \u548c\u719f\u4eba\u8fb9\u6253\u8fb9\u804a\u5c31\u8d85\u7ea7\u6709\u8da3\u4e86\u3002</p> <p>11\u6708\uff0c12\u6708\uff0c1\u6708\uff0c2\u6708\uff0c\u9690\u7ea6\u89c9\u5f97\u5c06\u8981\u6709\u5927\u628a\u7684\u4e1c\u897f\u8981\u5199\uff0c\u4e00\u4e9b\u4f1a\u8bae\uff0c\u4e00\u4e9b\u7ecf\u8d39 \u7533\u8bf7\uff0c\u4e00\u4e9b\u8bfe\u7a0b\u7533\u8bf7\uff0c\u60f3\u5230\u5c31\u5934\u75bc\u3002\u6709\u65f6\u771f\u4e0d\u77e5\u9053\u600e\u4e48\u6df7\u5230\u73b0\u5728\u7684\uff0c\u7ecf\u5e38\u89c9\u5f97\u8fd9 \u4e5f\u6765\u4e0d\u53ca\uff0c\u90a3\u4e5f\u6765\u4e0d\u53ca\uff0c\u6700\u540e\u5c45\u7136\u90fd\u8d76\u4e0a\u3002</p>"},{"location":"writing/2010/#dec-4-2010","title":"Dec 4, 2010","text":"<p>\u6628\u5929\u5728\u5bb6\u6253gran turismo 5\uff0c\u8bf4\u662fcar porn\u4e0d\u4e3a\u8fc7\uff0c\u8f66\u8f86\u6027\u80fd\u62df\u771f\u5ea6\u9ad8\uff0c\u7167\u7247\u6a21 \u5f0f\u4e5f\u8ddf\u771f\u7684\u4e00\u6837\uff0c\u5b9e\u5728\u662f\u9002\u5408\u5bf9\u5f00\u8f66\u6709\u7231\u7684\u4eba\u3002\u8fd8\u53ef\u4ee5\u6512\u4e86\u94b1\u4e70\u81ea\u5df1\u5e73\u65f6\u5f00\u7684\u8f66\u3002 \u529f\u592b\u4e0d\u8d1f\u6709\u5fc3\u4eba\uff0c\u8e72amazon\u51e0\u5929\uff0c\u5c45\u7136\u5728\u65f6\u5e38\u65ad\u8d27\u7684\u72b6\u6001\u62a2\u5230\u4e86\u964d\u4ef7\u65b9\u5411\u76d8\u3002</p> <p>\u7f8e\u56fd\u5fc3\u810f\u534f\u4f1a\u7684\u7533\u8bf7\u7ed3\u679c\u4e0b\u6765\u4e86\uff0c\u5206\u6570\u5dee\u90a3\u4e48\u4e00\u70b9\uff0c\u4e09\u4e2a\u5ba1\u7a3f\u4eba\uff0c\u4e00\u8d1f\u4e24\u6b63\uff0c\u6b63 \u7684\u975e\u5e38\u6b63\uff0c\u8d1f\u7684BS\u4e86\u4e00\u4e0b\u6211\u6781\u4f4e\u7684\u7814\u7a76\u751f\u6210\u7ee9\uff0c\u5e76\u6307\u51fa\u7a3f\u5b50\u5199\u5f97\u592a\u8349\uff0c\u6709\u4e0d\u5c11\u8bed \u6cd5\u9519\u8bef\uff0c\u4e00\u770b\u5c31\u662f\u8d76\u5de5\u4e4b\u4f5c\uff0c\u5efa\u8bae\u6211\u597d\u597d\u5199\u4e86\u91cd\u65b0\u9012\u4ea4\u3002\u865a\u5fc3\u63a5\u53d7\uff0c\u4eba\u5bb6\u4e5f\u6ca1\u8bf4 \u9519\u3002\u8001\u677f\u5012\u662f\u5f88\u53d7\u9f13\u821e\uff0c\u8bf4\u63a5\u8fd1\u4e86\uff0c\u5ba1\u7a3f\u4eba\u8a00\u4e0b\u4e4b\u610f\u662f\u597d\u597d\u5199\u4e86\u5c31\u7ed9\u94b1\u3002</p>"},{"location":"writing/2010/#dec-6-2010","title":"Dec 6, 2010","text":"<p>\u54ea\u90fd\u51b7\uff0c\u8f66\u70ed\u4e0d\u8d77\u6765\u65b9\u5411\u76d8\u51b7\uff0c\u53ea\u597d\u6307\u5934\u6233\u7740\u5f00\uff0c\u56de\u5bb6\u89c9\u5f97\u6697\u5904\u6709\u51b7\u98ce\uff0c\u627e\u4e86\u4e00 \u5708\u539f\u6765\u662f\u7a97\u5f0f\u7a7a\u8c03\u6f0f\u98ce\u3002\u6012\u62c6\uff0c\u6297\u4e0b\u6765\uff0c\u8001\u7a97\u6237\u5374\u5361\u6b7b\u4e86\u5173\u4e0d\u4e0a\u3002google\uff0c\u8981\u70b9 \u4e00\u662f\u4e0d\u8981\u7528\u86ee\u529b\uff0c\u9632\u6b62\u73bb\u7483\u7834\u788e\uff0c\u4e8c\u662f\u5c1d\u8bd5\u80a5\u7682\u6c34\uff0c\u4e09\u662f\u9524\u5b50\u6709\u8010\u5fc3\u7684\u6162\u6162\u6572\u3002 \u6de1\u5b9a\u7684\u6572\u4e86n\u4e0b\uff0c\u7ec8\u4e8e\u628a\u7a97\u6237\u653e\u4e0b\u6765\uff0c\u623f\u95f4\u624d\u5347\u6e29\u4e86\u3002</p> <p>\u611f\u6069\u8282\u4e70\u4e86\u6240\u6709\u60f3\u8981\u7684\u6e38\u620f\uff0c\u770b\u7740\u4e00\u67dc\u5b50\u7684\u6e38\u620f\u76d2\u5b50\u53d1\u50bb\uff0c\u6ca1\u90a3\u4e48\u5927\u6b32\u671b\u6253\u4e86\u3002 \u6700\u8fd1\u5728\u901acall of duty\u6545\u4e8b\u6a21\u5f0f\uff0c\u6b63\u5e38\u96be\u5ea6\u3002\u6e38\u620f\u8d28\u91cf\u4e0a\u4e58\uff0c\u4efb\u52a1\u591a\u6837\u5316\uff0c\u6b63\u9762 \u6218\u573a\uff0c\u9003\u4ea1\uff0c\u6551\u4eba\uff0c\u76f4\u5347\u673a\u652f\u63f4\u5730\u9762\uff0c\u72d9\u51fb\u6f5c\u5165\u7b49\u7b49\u3002\u7aef\u7740\u67aa\u4e0d\u77e5\u6240\u4ee5\u7136\u7684\u8d76\u8def\uff0c \u673a\u67aa\u58f0\u4e00\u76f4\u5f88\u5435\uff0c\u53c8\u6ca1\u6709\u5bfb\u5b9d\u7684\u8bbe\u5b9a\uff0c\u6240\u4ee5\u5355\u4eba\u6e38\u620f\u73a9\u6ca1\u6709\u7279\u522b\u559c\u6b22\uff0c\u5730\u56fe\u548c\u6b66 \u5668\u4e0d\u591f\u719f\u6089\u4e5f\u4e0d\u60f3\u5c1d\u8bd5\u591a\u4eba\u6a21\u5f0f\u3002\u552f\u4e00\u73a9\u5f97\u6fc0\u52a8\u7684\u5c31\u662fprice\u4e2d\u5c09\u6f5c\u5165\u6697\u6740\u7684\u90a3 \u4e00\u5173\uff0c\u6211\u6700\u949f\u7231\u5927\u8001\u8fdc\u7684\u7528\u6765\u590d\u67aa\u628a\u4eba\u653e\u5012\uff0c\u518d\u5927\u6447\u5927\u6446\u8d70\u8fc7\u53bb\u3002</p>"},{"location":"writing/2010/#dec-8-2010","title":"Dec 8, 2010","text":"<p>call of duty 4\u901a\u5173\uff0c\u8d8a\u5230\u540e\u9762\u8d8a\u6709\u73a9\u5934\uff0c\u62c6\u5766\u514b\uff0c\u9003\u4ea1\u3002\u6e38\u620f\u5bf9\u6b7b\u4ea1\u7684\u5904\u7406\u4e0d \u9519\uff0c\u6162\u955c\u5934\u52a0\u6a21\u7cca\u5904\u7406\uff0c\u53ef\u63a7\u5236\u89c6\u89d2\uff0c\u771f\u5b9e\u611f\u5f3a\u3002\u672b\u5c3e\u773c\u770b\u961f\u53cb\u4e00\u4e2a\u4e2a\u6b7b\u53bb\uff0c\u63a5 \u8fc7Price\u62fc\u6b7b\u6254\u8fc7\u6765\u7684\u67aa\uff0c\u6323\u624e\u7740\u706d\u6389boss\uff0c\u7ec8\u4e8e\u6491\u5230\u4e86\u6551\u63f4\u961f\u4f0d\u6765\u4e34\u65f6\uff0c\u5c45\u7136 \u6709\u70b9\u611f\u4eba\uff0c\u6d3b\u4e0b\u6765\u771f\u4e0d\u5bb9\u6613\uff0c\u5374\u53ea\u5269\u81ea\u5df1\u4e86\u3002</p> <p>\u7528\u65b9\u5411\u76d8\u73a9Gran turismo 5\u5f88\u8d5e\uff0c\u5728\u6e38\u620f\u91cc\u4e70\u4e86\u81ea\u5df1\u7684\u7834\u8f66\u5f00\uff0c\u5404\u9879\u6027\u80fd\u5305\u62ec\u8f6c \u5411\u5e45\u5ea6\u90fd\u5f88\u4eff\u771f\u3002\u73a9\u6700\u5f31\u96be\u5ea6\u7684\u6bd4\u8d5b\u7a33\u62ff\u51a0\u519b\uff0c\u4e0d\u8fc7\u96be\u5ea6\u518d\u9ad8\u4e00\u70b9\u5c31\u56e0\u4e3a\u52a0\u901f\u592a \u8089\u800c\u843d\u540e\u4e86\uff0c\u679c\u7136\u4eff\u771f\u3002\u548c\u540c\u5b66\u5171\u540c\u8ba4\u4e3a\u540e\u9057\u75c7\u662f\u5f00\u5b8cGT5\u518d\u53bb\u5f00\u771f\u8f66\uff0c\u5f88\u5bb9\u6613 \u5fd8\u8bb0\u81ea\u5df1\u5f00\u7684\u662f\u7834\u8f66\uff0c\u603b\u5fcd\u4e0d\u4f4f\u8981\u5f00\u5feb\uff0c\u592a\u5371\u9669\u30022D\u548c3D\u8fd8\u662f\u8981\u5206\u5f00\u3002\u539f\u4ee5 \u4e3a\u7528\u65b9\u5411\u76d8\u73a9\u81ea\u5df1\u5c31\u725b\u4e86\uff0c\u5176\u5b9e\u4e5f\u4e0d\u6bd4\u624b\u67c4\u5f3a\u5230\u54ea\u91cc\u53bb\u3002</p> <p>\u81ea\u544a\u594b\u52c7\u8d1f\u8d23\u6253\u5370\u5b9e\u9a8c\u5ba4\u6240\u6709\u4eba\u7684\u7ec6\u80de\u5e74\u4f1a\u6d77\u62a5\u3002A\u540c\u5b66\u7684\u6d77\u62a5\u6587\u4ef6\u592a\u5927\uff0c\u5bfc\u81f4 \u5370\u4e2d\u5fc3\u6ca1\u6536\u5230\u6211\u53d1\u7684\u90ae\u4ef6\uff0c\u6211\u4e5f\u6ca1\u6536\u5230\u4efb\u4f55\u9519\u8bef\u62a5\u544a\u3002\u4eca\u5929\u4e0b\u5348\u548c\u8001\u677f\u6b22\u5929\u559c \u5730\u7684\u53bb\u53d6\u6d77\u62a5\uff0c\u4e0d\u4f46\u7a7a\u624b\u800c\u5f52\uff0c\u8fd8\u8981\u62a2\u5728\u4e0b\u73ed\u4e4b\u524d\u4eba\u8089\u6295\u9012\u6587\u4ef6\uff0c\u4e8e\u662f\u6293\u8d77\u79fb \u52a8\u786c\u76d8 \u5c31\u96ea\u5730\u72c2\u5954\uff0c\u8d76\u5728\u4e0b\u73ed\u524d\u4e00\u5206\u949f\u5230\u8fbe\u3002</p>"},{"location":"writing/2010/#dec-17-2010","title":"Dec 17, 2010","text":"<p>\u5927\u534a\u5468\u7684\u7ec6\u80de\u5e74\u4f1a\u5f00\u5b8c\u4e86\uff0c\u73b0\u5728\u8fd8\u6ca1\u7f13\u8fc7\u795e\u6765\u3002\u7b80\u77ed\u611f\u53d7\u5c31\u662f\u4f5c\u4e3a\u4e00\u4e2a\u4f4e\u7b49\u7ea7\u7684 \u5c0f\u867e\u7c73\u8ddf\u968f\u9ad8\u7b49\u7ea7\u7684\u961f\u4f0d\u53bb\u6253\u602a\uff0c\u961f\u53cb\u961f\u957f\u6253\u51fan\u591a\u9ad8\u7ea7\u88c5\u5907\uff0c\u6211\u5374\u8fbe\u4e0d\u5230\u7b49\u7ea7 \u8981\u6c42\u7a7f\u4e0d\u4e0a\u3002</p> <p>\u6240\u8c13\u5e74\u4f1a\u5c31\u662f5\u5929\u65f6\u95f4\u4ece\u65e9\u5230\u665a\u5b89\u6392\u6ee1\uff0c\u65e9\u665a\u542c\u62a5\u544a\uff0c\u4e2d\u9014\u5c55\u89c8\u6d77\u62a5\u3002\u5373\u4f7f\u6709\u95ea \u7535\u4fa0\u7684\u901f\u5ea6\uff0c\u4e0d\u5403\u996d\uff0c\u4e5f\u4f1a\u56e0\u4e3a\u62a5\u544a\u548c\u6d77\u62a5\u592a\u591a\uff0c\u800c\u4f1a\u9519\u8fc7\u5f88\u591a\u611f\u5174\u8da3\u7684\u3002\u6211\u8fd9 \u51e0\u5929\u5904\u4e8e\u5b66\u672f\u4ea2\u594b\u72b6\u6001\uff0c\u751f\u6d3b\u5f97\u5f88\u60e8\u6de1\uff0c\u6bcf\u5929\u65e9\u996d\u540e\u5f00\u4f1a\uff0c\u4e2d\u5348\u996d\u53ea\u6709\u4e00\u5929\u5403\u5230 \u4e86\uff0c\u5176\u5b83\u51e0\u5929\u90fd\u662f\u76f4\u5230\u4e0b\u5348\u5f00\u5b8c\u4f1a\uff0c\u624d\u548c\u4e24\u773c\u53d1\u76f4\u7684\u5b9e\u9a8c\u5ba4\u540c\u4f34\u4eec\u51b2\u53bb\u5403\u665a\u996d\u3002 \u6211\u8ddfA\u540c\u5b66\u8bf4\uff0c\u6211\u89c9\u5f97\u6211\u90fd\u53ef\u4ee5\u5403\u77e5\u8bc6\u7ba1\u9971\u4e86\u3002\u7761\u89c9\u8d28\u91cf\u4e5f\u4e0d\u662f\u5f88\u597d\uff0c\u6211\u4e3b\u52a8\u53bb \u7761\u6298\u53e0\u5f0f\u6c99\u53d1\uff0c\u8d28\u91cf\u4e0d\u662f\u5f88\u597d\uff0c\u623f\u95f4\u53c8\u51b7\uff0c\u66f4\u53d1\u51b7\u7684\u662f\u6bcf\u6b21\u9192\u6765\u8c8c\u4f3c\u505a\u7684\u90fd\u662f\u5206 \u5b50\u751f\u7269\u68a6\u3002</p> <p>\u7b14\u8bb0\u8bb0\u4e86\u4e00\u5806\uff0c\u89c9\u5f97\u6211\u8981\u662f\u5e73\u65f6\u8fd9\u4e48\u9ad8\u9891\u7387\u770b\u6587\u7ae0\uff0c\u5c31\u4e0d\u81f3\u4e8e\u843d\u5230\u73b0\u5728\u8fd9\u4e2a\u7ecf\u9a8c \u503c\u6c34\u5e73\u4e86\u3002\u53bb\u770b\u6d77\u62a5\uff0c\u542c\u522b\u4eba\u7684\u8bb2\u89e3\u95ee\u4e0d\u51fa\u6df1\u523b\u95ee\u9898\uff0c\u5c31\u50bb\u7ad9\u7740\u70b9\u5934\uff0c\u522b\u4eba\u95ee\u6211 \u7684\u6d77\u62a5\uff0c\u6211\u8bb2\u5f97\u66f4\u4e0d\u662f\u5f88\u597d\uff0c\u7b54\u95ee\u9898\u7ecf\u5e38\u72af\u50bb\uff0c\u591a\u6b21\u9519\u8fc7\u4e86\u5b66\u672f\u4ea4\u6d41\u7684\u597d\u673a\u4f1a\u3002 \u6211\u5f88\u4e0d\u64c5\u957f\u63a8\u9500\u81ea\u5df1\u505a\u7684\u4e1c\u897f\uff0c\u7d27\u5f20\uff0c\u53c8\u663e\u5f97\u592a\u4e00\u672c\u6b63\u7ecf\uff0c\u5c31\u8bb2\u5f97\u5e72\u5df4\u5df4\u7684\uff0c\u4e0d \u663e\u5f97\u81ea\u5df1\u6709\u70ed\u60c5\u3002\u5dee\u4e0d\u591a\u7684\u4e1c\u897f\u770b\u5230\u522b\u4eba\u8bb2\u5f97\u7709\u98de\u8272\u821e\u7684\uff0c\u6548\u679c\u5c31\u5b8c\u5168\u4e0d\u4e00\u6837\u3002</p> <p>\u8001\u677f\u5f00\u4f1a\u4f30\u8ba1\u641e\u5b66\u672f\u8054\u7cfb\u4e5f\u662f\u4e2a\u76ee\u7684\uff0c\u9f13\u52b1\u6211\u4eec\u627e\u76f8\u5173\u9886\u57df\u7684\u4eba\u8c08\uff0c\u4e5f\u5c3d\u529b\u4ecb\u7ecd \u4eba\u7ed9\u6211\u4eec\u8ba4\u8bc6\u3002\u6211\u6bd4\u8f83\u4e0d\u4e89\u6c14\uff0c\u4e92\u76f8\u4ecb\u7ecd\u5b8c\uff0c\u6211\u5c31\u65e0\u8bdd\u4e86\uff0c\u4e0d\u4e3b\u52a8\u4e0d\u70ed\u60c5\u3002\u793e\u4ea4 \u7f3a\u9677\u4e00\u65f6\u534a\u4f1a\u4e5f\u514b\u670d\u4e0d\u4e86\uff0c\u53ea\u80fd\u6162\u6162\u8ddf\u522b\u4eba\u5b66\u4e86\uff0c\u6211\u4e0d\u6253\u7b97\u7ed9\u81ea\u5df1\u592a\u5927\u538b\u529b\u3002</p>"},{"location":"writing/2010/#dec-24-2010","title":"Dec 24, 2010","text":"<p>\u504f\u504f\u5728\u8282\u524d\u60f3\u8d77\u4e0d\u5c11\u6709\u8da3\u7684\u5b9e\u9a8c\uff0c\u65e0\u5948\u697c\u5c42\u7a7a\u8361\u8361\u7684\u5c31\u6211\u4e00\u4e2a\u4eba\uff0c\u6ca1\u52a8\u529b\u505a\u5b9e\u9a8c\uff0c \u5c31\u56de\u5bb6\u6253\u6e38\u620f\u4e86\u3002Valkyria Chronicles\uff0c\u597d\u4e45\u6ca1\u73a9\u65e5\u5f0f\u6e38\u620f\uff0c\u8fd8\u4e0d\u9002\u5e94\u5267\u60c5\uff0c \u89c9\u5f97\u6ca1\u4ec0\u4e48\u5473\u9053\uff0c\u5c31\u5168\u8df3\u8fc7\u4e86\u3002\u4e0d\u8fc7\u6218\u6597\u7cfb\u7edf\u5f88\u65b0\u9896\uff0c\u4ee5\u56de\u5408\u5236\u6218\u7565\u4e3a\u57fa\u7840\uff0c\u4f46 \u662f\u4e00\u822c\u7684\u56de\u5408\u5236\u6218\u7565\uff0c\u90fd\u53ea\u80fd\u5b9e\u65bd\u653b\u51fb\uff0c\u79fb\u52a8\uff0c\u9632\u5b88\u7b49\u7b49\u6307\u4ee4\uff0c\u7136\u540e\u5c31\u653e\u4e0b\u624b\u67c4 \u770b\u6218\u6597\u52a8\u753b\u4e86\u3002\u7136\u800cValkyria Chronicles\u91cc\uff0c\u9009\u5b9a\u60f3\u63a7\u5236\u7684\u4eba\u4e4b\u540e\uff0c\u5c31\u53d8\u6210\u4e86 \u52a8\u4f5c\u6e38\u620f\uff0c\u5b8c\u5168\u7531\u81ea\u5df1\u63a7\u5236\u58eb\u5175\u7684\u8dd1\u52a8\uff0c\u8eb2\u907f\uff0c\u5c04\u51fb\u7b49\u7b49\uff0c\u53c2\u4e0e\u611f\u5927\u5927\u52a0\u5f3a\u3002\u540c \u65f6\uff0c\u6218\u6597\u4e0d\u662f\u4e00\u5bf9\u4e00\u7684\uff0c\u6240\u6709\u5c04\u7a0b\u8303\u56f4\u5185\u7684\u4eba\u90fd\u4f1a\u653b\u51fb\uff0c\u6240\u4ee5\u4e0d\u80fd\u50cf\u4f20\u7edfSLG\u90a3 \u6837\u6254\u4e2a\u9006\u5929\u4eba\u7269\u5230\u654c\u4eba\u5806\u91cc\uff0c\u4e00\u4e2a\u4e2a\u5355\u6311\u3002\u4eba\u7269\u5c5e\u6027\u4e4b\u95f4\u867d\u7136\u6709\u5dee\u5f02\uff0c\u4f46\u4e5f\u4e0d\u662f \u8bf4\u6ca1\u6709\u8c01\u5c31\u8d62\u4e0d\u4e86\uff0c\u6216\u8005\u6709\u4e86\u8c01\u5c31\u5fc5\u80dc\u2013\u6218\u672f\u6700\u91cd\u8981\u3002\u6bd4\u8d77\u4f20\u7edf\u7684\u6218\u68cb\u7c7b\u6e38\u620f\uff0c \u8fd9\u4e2a\u66f4\u50cf\u4e0b\u68cb\u3002</p>"},{"location":"writing/2010/#dec-28-2010","title":"Dec 28, 2010","text":"<p>\u7ee7\u7eed\u5728\u73a9Valkyria Chronicles\uff0c\u57fa\u672c\u638c\u63e1\u4e86\u62ffA\u7ea7\u8bc4\u4ef7\u7684\u8981\u9886\uff0c\u4fa6\u5bdf\u5175\u5f80\u524d\u51b2\u5c31 \u662f\u4e86\u3002\u4e0d\u8fc7\u8981\u8f85\u52a9\u5b58\u76d8\u8bfb\u76d8\u5927\u6cd5\uff0c\u800c\u8bfb\u6863\u65f6\u95f4\u53c8\u5f88\u957f\uff0c\u90fd\u53ef\u4ee5\u8fb9\u505a\u9898\u8fb9\u73a9\u4e86\u3002</p>"},{"location":"writing/2011/","title":"2011","text":""},{"location":"writing/2011/#jan-20-2011","title":"Jan 20, 2011","text":"<p>\u6212\u6e38\u620f\u672c\u8eab\u6ca1\u6709\u4ec0\u4e48\u56f0\u96be\uff0c\u53bb\u6389\u73a9\u6e38\u620f\uff0c\u60f3\u6e38\u620f\uff0c\u548c\u5199\u6e38\u620f\u7684\u65f6\u95f4\uff0c\u7a81\u7136\u591a\u51fa\u5927 \u628a\u7684\u7a7a\u95f2\uff0c\u624d\u53d1\u89c9\u751f\u6d3b\u4e2d\u8fd8\u6709\u90a3\u4e48\u591a\u6709\u8da3\u7684\u4e8b\u60c5\u53ef\u4ee5\u505a\u3002\u4e0d\u8fc7\u4e5f\u4e0d\u56e0\u6b64\u5996\u9b54\u5316\u6e38 \u620f\uff0c\u6e38\u620f\u4f9d\u7136\u662f\u597d\u4e1c\u897f\uff0c\u53ea\u662f\u4e2a\u4eba\u539f\u56e0\u4e0d\u518d\u9002\u5408\u73a9\uff0c\u627e\u522b\u7684\u4e50\u5b50\u53bb\u4e86\u3002\u5356\u6e38\u620f\u5219 \u662f\u4e2a\u95ee\u9898\uff0c\u60c5\u611f\u4e0a\u7684\u7275\u8fde\u53bb\u4e0d\u6389\uff0c\u4e0b\u4e0d\u4e86\u624b\uff0c\u6bd5\u7adf\u4ece\u5c0f\u73a9\u5230\u5927\uff0c\u5df2\u7ecf\u73a9\u6210\u4e8c\u6b21\u5143 \u751f\u547d\u7684\u4e00\u90e8\u5206\u4e86\u3002</p>"},{"location":"writing/2011/#mar-9-2011","title":"Mar 9, 2011","text":"<p>\u82e6\u60f3\u4e86\u534a\u5929\u65f6\u95f4\u5206\u914d\u7684\u95ee\u9898\u3002\u5065\u8eab\u8be5\u4f5c\u4e3a\u65e5\u5e38\u7684\u4e00\u90e8\u5206\uff1b\u9f13\u4e0d\u9700\u8981\u6bcf\u5929\u7ec3\u5f88\u4e45\uff0c \u4f46\u8981\u6301\u7eed\uff0c\u624d\u80fd\u719f\u80fd\u751f\u5de7\uff1b\u753b\u753b\u6709\u4e86\u4e00\u5b9a\u57fa\u7840\uff0c\u5468\u672b\u6709\u7a7a\u5f04\u4e00\u4e0b\u5c31\u53ef\u4ee5\u8fdb\u6b65\u3002\u770b \u8d77\u6765\u7b80\u5355\uff0c\u4f46\u524d\u4e00\u9635\u5b50\u8fd9\u4e9b\u90fd\u52aa\u529b\u53bb\u505a\uff0c\u5fc3\u5374\u4e5f\u5206\u6563\u4e86\uff0c\u6b63\u4e8b\u53cd\u5012\u53d7\u5f71\u54cd\u3002\u7406\u8bba \u4e0a\u5373\u4f7f\u6211\u4e0d\u5728\u7231\u597d\u4e0a\u82b1\u65f6\u95f4\uff0c\u65f6\u95f4\u4e5f\u4e00\u6837\u4f1a\u88ab\u6d6a\u8d39\u6389\uff0c\u6240\u4ee5\u5e94\u8be5\u4e0d\u662f\u8d39\u65f6\u7684\u95ee\u9898\u3002 \u4eca\u5929\u91cd\u65b0\u6574\u7406\u4e86\u4e0b\uff0c\u731c\u6d4b\u662f\u56e0\u4e3a\u6211\u6bcf\u5929\u90fd\u8981\u82b1\u65f6\u95f4\u8ba1\u5212\u5565\u65f6\u5019\u8be5\u5e72\u561b\u3002</p> <p>\u6240\u4ee5\u5c1d\u8bd5\u4e00\u4e2a\u89e3\u51b3\u529e\u6cd5\uff0c\u5c31\u662f\u518d\u628a\u6bcf\u5929\u7684\u8ba1\u5212\u50bb\u74dc\u5316\uff0c\u65f6\u95f4\u5206\u6210\u8c46\u8150\u5757\uff0c\u54ea\u4e2a\u65f6 \u5019\u5e72\u4ec0\u4e48\u4e00\u6e05\u4e8c\u695a\uff0c\u6253\u5370\u51fa\u6765\u3002</p>"},{"location":"writing/2011/#mar-10-2011","title":"Mar 10, 2011","text":"<p>\u5728\u8001\u677f\u7684\u6210\u529f\u5ffd\u60a0\u4e0b\uff0c\u8ba4\u8bc6\u5230\u5230\u5904\u95ee\u4e5f\u662f\u505a\u7814\u7a76\u7684\u4e00\u90e8\u5206\u3002\u6709\u65f6\u8bfb\u5b83n\u7247\u6587\u7ae0\u8bd5m \u4e2a\u5b9e\u9a8c\uff0c\u8fd8\u4e0d\u5982\u95ee\u4e00\u53e5\u6765\u5f97\u5feb\u3002\u8be5\u505a\u4f38\u624b\u515a\u7684\u65f6\u5019\u5c31\u8981\u505a\u4f38\u624b\u515a\u3002\u8fd9\u65b9\u9762\u8001\u677f\u662f \u4e2a\u699c\u6837\uff0c\u5230\u5904\u627e\u5b66\u672f\u8fde\u63a5\uff0c\u6240\u4ee5\u4e00\u76f4\u6709\u8001\u677f\u627e\u6211\u4eec\u5b9e\u9a8c\u5ba4\u5408\u4f5c\uff0c\u751a\u81f3\u6211\u672c\u6765\u8be5\u81ea \u5df1\u53bb\u627e\u7684\u4eba\uff0c\u8001\u677f\u770b\u6211\u534a\u5929\u4e0d\u52a8\u4e5f\u53bb\u5e2e\u6211\u627e\u4e86\u3002\u73b0\u5728\u610f\u8bc6\u5230\u8fd9\u51e0\u5e74\u6211\u9700\u8981\u6bb5\u7ec3\u51fa \u8fd9\u4e2a\u80fd\u529b\uff0c\u518d\u6b21\u539a\u8138\u76ae\u3002\u8fd9\u4e2a\u5bf9\u6709\u4e9b\u4eba\u5bb9\u6613\uff0c\u5bf9\u6211\u5374\u662f\u6311\u6218\uff0c\u4e3b\u52a8\u627e\u4e0d\u8ba4\u8bc6\u7684\u4eba \u95ee\u95ee\u9898\uff0c\u8981\u72b9\u8c6b\u8001\u534a\u5929\uff0c\u6216\u62d6\u6765\u62d6\u53bb\u5c31\u7b97\u4e86\u3002\u4ece\u660e\u5929\u5f00\u59cb\u5c1d\u8bd5\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff1a\u4e0d \u518d\u628a\u627e\u4eba\u5f53\u4f5c\u5b9e\u9a8c\u4e4b\u5916\u53ef\u6709\u53ef\u65e0\u7684\u4e8b\uff0c\u800c\u628a\u5b83\u6309\u91cd\u8981\u5b9e\u9a8c\u5bf9\u5f85\uff0c\u5f52\u5230\u65e5\u5e38\u5b9e\u9a8c\u5b89 \u6392\u91cc\u3002\u53e6\u5916\u591a\u60f3\u60f3\u964c\u751f\u4eba\u8dd1\u6765\u95ee\u6211\u95ee\u9898\uff0c\u6216\u8dd1\u6765\u6211\u4eec\u5b9e\u9a8c\u5ba4\u95ee\u7684\u65f6\u5019\uff0c\u6211\u5982\u4f55\u770b \u5f85\u964c\u751f\u4eba\u3002\u5c31\u5982\u540c\u6211\u4e0d\u4f1a\u628a\u964c\u751f\u4eba\u5403\u4e86\uff0c\u964c\u751f\u4eba\u4e5f\u4e0d\u8be5\u4f1a\u628a\u6211\u5403\u4e86\uff0c\u5e76\u4e14\u5927\u90e8\u5206 \u65f6\u5019\u8fd8\u4e50\u610f\u5e2e\u5fd9\u3002\u5e0c\u671b\u7528\u7406\u6027\u514b\u670d\u4ea4\u6d41\u969c\u788d\uff0c\u4e3a\u4e86\u7814\u7a76\uff0c\u4e5f\u4e3a\u4e86\u751f\u6d3b\u3002</p>"},{"location":"writing/2011/#mar-14-2011","title":"Mar 14, 2011","text":"<p>\u9762\u5bf9\u6d8c\u73b0\u7684\u5927\u91cf\u6587\u732e\uff0c\u9700\u8981\u66f4\u65b0\u9605\u8bfb\u65b9\u5f0f\u3002\u60f3\u8d77\u4ee5\u524d\u4e0a\u8fc7\u7684\u6f2b\u753b\u8bb2\u5ea7\uff0c\u8001\u5e08\u8bf4\u4e00 \u4e2a\u7269\u4f53\uff0c\u7ed910\u5206\u949f\u753b\uff0c\u7136\u540e\u65f6\u95f4\u9010\u6e10\u51cf\u5c11\uff0c\u76f4\u5230\u53d8\u4e3a15\u79d2\uff0c10 \u79d2\uff0c5\u79d2\uff0c\u6700\u540e3 \u79d2\u3002\u65f6\u95f4\u51cf\u5c11\u7684\u8fc7\u7a0b\u4e2d\uff0c\u601d\u8003\u5982\u4f55\u8ba9\u522b\u4eba\u4e00\u770b\u5c31\u8ba4\u51fa\u6765\uff0c\u4e0d\u4f1a\u4ee5\u4e3a\u662f\u522b\u7684\u7269\u4f53\u3002 \u6362\u53e5\u8bdd\u8bf4\uff0c\u627e\u7279\u70b9\uff0c\u820d\u5f97\u7701\u7565\u4e0d\u5fc5\u8981\u7684\u7ec6\u8282\uff0c\u54ea\u6015\u8fd9\u4e2a\u7ec6\u8282\u5f88\u6f02\u4eae\u3002</p> <p>\u8fd9\u4e2a\u529e\u6cd5\u7528\u5230\u8bfb\u6587\u732e\u4e0a\uff0c\u9996\u5148\u628a\u6587\u7ae0\u5206\u4e09\u516d\u4e5d\u7b49\uff0c\u503c\u5f97\u770b5\u5206\u949f\uff0c10\u5206\u949f\uff0c20\u5206 \u949f\uff0c\u8fd8\u662f30\u5206\u949f\u7ec6\u8bfb\u3002\u65e0\u8bba\u8bfb\u591a\u4e45\uff0c\u5173\u6ce8\u4ee5\u4e0b\uff1a1.\u95ee\u9898\u662f\u4ec0\u4e48\u30022.\u539f\u6750\u6599\u30023.\u65b9 \u6cd5\u30024.\u7ed3\u8bba\uff0c\u6211\u7684\u7ed3\u8bba\uff0c\u5982\u679c\u8ddf\u4f5c\u8005\u7ed3\u8bba\u4e0d\u4e00\u6837\uff0c\u7279\u522b\u6ce8\u660e\u4e00\u4e0b\u30025.\u72ec\u7279\u4e4b\u5904\u3002 \u6709\u591a\u5c11\u65f6\u95f4\u770b\u591a\u5c11\uff0c\u770b\u4e0d\u5b8c\u5c31\u7b97\u4e86\u3002\u8bbe\u5b9a\u4e3a5\u5206\u949f\u7684\u6587\u7ae0\uff0c\u8fd1\u671f\u5185\u6211\u53ea\u9700\u8981\u80fd\u591f \u4e00\u53e5\u8bdd\u590d\u8ff0\uff0c\u65e0\u987b\u505a\u592a\u591a\u7b14\u8bb0\uff0c\u5982\u679c\u9700\u8981\uff0c\u6211\u4e5f\u53ef\u4ee5\u901a\u8fc7\u5bfb\u627e\u7b14\u8bb0\u8ffd\u5bfb\u5230\u6587\u7ae0\u518d \u8bfb\u3002</p>"},{"location":"writing/2011/#mar-16-2011","title":"Mar 16, 2011","text":"<p>\u518d\u6b21\u8bc1\u660e\u6ca1\u6709\u4ec0\u4e48\u6bd4\u65ad\u7f51\u66f4\u6539\u53d8\u751f\u6d3b\u3002\u8fd1\u671f\u5bb6\u91cc\u6ca1\u7f51\uff0c\u53ea\u5728\u5b9e\u9a8c\u5ba4\u4e0a\uff0c\u6240\u4ee5\u6d6a\u8d39 \u7684\u65f6\u95f4\u6709\u4e0a\u9650\u3002\u5728\u5bb6\u90fd\u5fd8\u8bb0\u4e86\u4e0a\u7f51\u8fd9\u56de\u4e8b\uff0c\u53ea\u6709\u5199\u65e5\u8bb0\uff0c\u5e72\u6b63\u4e8b\u7684\u65f6\u5019\u624d\u60f3\u8d77\uff0c \u4e5f\u6ca1\u89c9\u5f97\u635f\u5931\u3002</p>"},{"location":"writing/2011/#mar-29-2011","title":"Mar 29, 2011","text":"<p>\u7ee7\u7eed\u8d5eDebian Squeeze\uff0c\u65b0\u7684Nautilus\u4e3b\u52a8\u8ba4\u8bc6\u6240\u6709\u5206\u533a\uff0c\u53ef\u76f4\u63a5\u6302\u8f7d\u3002RMVB\u76f4 \u63a5\u53ef\u89c2\u770b\uff0c\u867d\u7136MPlayer\u4e0d\u9ed8\u8ba4\u652f\u6301\uff0c\u4f46Totem\u662f\uff0c\u6709\u8da3\u7684\u662f\u539f\u4ee5\u4e3a\u53ea\u7ba1\u7f51\u7edc\u7535\u89c6 \u7684Miro\u4e5f\u80fd\u770bRMVB\u3002</p>"},{"location":"writing/2011/#mar-31-2011","title":"Mar 31, 2011","text":"<p>\u505a\u68a6\u88ab\u5bfc\u6e38\u5e26\u5230\u4e00\u4e2a\u5730\u65b9\uff0c\u5929\u4e0a\u6709\u4e2a\u53d1\u5149\u7684\u533a\u57df\uff0c\u5bfc\u6e38\u8bf4\u9760\u8fd1\u4e86\u5c31\u80fd\u770b\u5230\u771f\u7406\uff0c \u4f46\u662f\u770b\u5230\u4eba\u4e5f\u5c31\u6302\u4e86\u3002\u6211\u8bf4\u90a3\u6211\u73b0\u5728\u5728\u68a6\u4e2d\uff0c\u6302\u4e86\u5c31\u9192\u6765\uff0c\u6ca1\u5565\u540e\u679c\u5427\u3002\u5bfc\u6e38\u8bf4 \u68a6\u4e2d\u6302\u4e86\u5c31\u9192\u4e0d\u6765\uff0c\u6211\u72b9\u8c6b\u4e86\u4e00\u4e0b\u5c31\u4e0d\u53bb\u770b\u771f\u7406\u4e86\u3002</p>"},{"location":"writing/2011/#apr-1-2011","title":"Apr 1, 2011","text":"<p>\u4e00\u76f4\u4fdd\u6301\u7740\u6d6a\u8d39\u65f6\u95f4\u7684\u826f\u597d\u4e60\u60ef\uff0c\u4ec0\u4e48\u90fd\u662f\u6709\u73b0\u6210\u7684\u4e0d\u7528\uff0c\u975e\u8981\u81ea\u5df1\u4ece\u5934\u6574\u8d77\u3002 \u73b0\u5728\u53c8\u5fc3\u75d2\u60f3\u81ea\u5df1\u88c5\u7535\u8111\uff0c\u76fc\u7740\u54ea\u5929\u7533\u8bf7\u5230\u4e2a\u9879\u76ee\u7ecf\u8d39\u6da8\u5de5\u8d44\uff0c\u642d\u4e2a\u4e13\u95e8\u505a\u8ba1\u7b97 \u7684\u7535\u8111\uff0c\u914d\u4e2a\u7279\u88c5X\u7684\u7684\u673a\u7bb1\uff0c\u5c31\u5b8c\u7f8e\u4e86\u3002</p>"},{"location":"writing/2011/#apr-9-2011","title":"Apr 9, 2011","text":"<p>\u9047\u5230\u4e0d\u6025\u7684\u5408\u4f5c\u8005\u771f\u662f\u6025\u3002\u4e00\u4e2a\u7a0b\u5e8f\u8981\u4e86\u4e24\u6b21\u6e90\u4ee3\u7801\uff0c\u73b0\u5728\u8fd8\u6ca1\u53d1\u8fc7\u6765\uff0c\u800c\u6211\u4eec \u53c8\u9700\u8981\u5728\u4e0b\u5468\u4f1a\u9762\u7684\u65f6\u5019\u8ba8\u8bba\u6211\u73a9\u51fa\u4e86\u4ec0\u4e48\u3002\u51c6\u5907\u5468\u4e00\u518d\u53bb\u5f53\u9762\u50ac\u4e00\u4e0b\uff0c\u8981\u662f\u518d \u5fd8\u8bb0\uff0c\u6211\u90fd\u4e0d\u597d\u610f\u601d\u518d\u50ac\u4e86\u3002</p> <p>\u4ee5\u524d\u4ee5\u4e3a\u6709\u7684\u9f13\u624b\u6234\u624b\u5957\u662f\u88c513\uff0c\u76f4\u5230\u6700\u8fd1\u53d1\u73b0\u9f13\u69cc\u7528\u592a\u4e45\u4f1a\u78e8\u5f97\u6253\u6ed1\uff0c\u6211\u8fd9\u9f9f \u901f\u7684\u6572\u6cd5\u4e5f\u80fd\u628a\u9f13\u69cc\u98de\u51fa\u53bb\uff0c\u624d\u660e\u767d\u624b\u5957\u786e\u5b9e\u6709\u7528\u3002</p> <p>\u6700\u8fd1\u6211\u8ddf\u79bb\u5fc3\u673a\u8fc7\u4e0d\u53bb\uff0c\u4e0a\u5468\u63d0\u7eaf\u7684\u86cb\u767d\uff0c\u5230\u4e86\u6700\u540e\u4e00\u6b65\u79bb\u5fc3\uff0c\u7ba1\u5b50\u88c2\u7f1d\u4e86\uff0c\u62ff \u51fa\u6765\u540e\u6b32\u54ed\u65e0\u6cea\u7684\u4ece\u79bb\u5fc3\u673a\u91cc\u62ef\u6551\u51fa\u4e00\u4e9b\u6837\u672c\uff0c\u4e0d\u8fc7\u4e5f\u77e5\u9053\u4e0d\u80fd\u7528\u4e86\u3002\u51e0\u5929\u7684\u529f \u592b\u963f\u3002\u540e\u6765\u518d\u63d0\u7eaf\uff0c\u9ad8\u901f\u79bb\u5fc3\u673a\u7684\u7ba1\u5b50\u53c8\u7834\u4e86\u4e2a\u5c0f\u7f1d\uff0c\u5e78\u597d\u635f\u5931\u4e0d\u591a\uff0c\u53ef\u662f\u79bb\u5fc3 \u673a\u4e0d\u884c\u4e86\uff0c\u4e00\u8f6c\u5230\u9ad8\u901f\u5c31\u560e\u5431\u560e\u5431\u3002</p>"},{"location":"writing/2011/#apr-10-2011","title":"Apr 10, 2011","text":"<p>\u4eba\u4e3a\u4ec0\u4e48\u6709\u61d2\u60f0\u8fd9\u79cd\u5c5e\u6027\u8bbe\u5b9a\u3002\u3002</p>"},{"location":"writing/2011/#apr-26-2011","title":"Apr 26, 2011","text":"<p>\u7269\u6b32+\u624b\u75d2\uff0c\u5e7b\u60f3\u535a\u58eb\u540e\u65f6\u5019\u4f1a\u62e5\u6709\u7684\u4e1c\u897f</p> <p>\u81ea\u88c5\u7535\u8111\uff0c\u9002\u5408\u4f5c\u8ba1\u7b97\uff0c\u5916\u89c2\u88c5X\uff0c\u6269\u5c55\u6027\u5f3a\uff0c\u6709\u4e0d\u9519\u7684\u58f0\u5361\uff0c\u53ef\u4ee5\u5f55\u6e38\u620f\u7684\u663e \u5361\u3002 Roland\u7684\u4e2d\u7aef\u7535\u5b50\u9f13\uff0c\u4ee5\u53ca\u81ea\u5236\u7684\u9694\u97f3\u88c5\u7f6e\uff0c\u6027\u4ef7\u6bd4\u9ad8\u7684\u529f\u653e\uff0c\u4fbf\u5b9c\u7684\u97f3 \u7bb1\u3002\u4e8c\u624b\u7684\u5409\u666e\uff0c\u5982\u679c\u5230\u65f6\u5019\u6709\u53ef\u518d\u751f\u80fd\u6e90\u7684\u5c31\u597d\u4e86\u3002XBOX\uff0c\u7f57\u6280\u5e26\u624b\u52a8\u7684\u9ad8 \u7ea7\u65b9\u5411\u76d8\u3002</p> <p>\u5e7b\u60f3\u5f52\u5e7b\u60f3\uff0c\u5176\u5b9e\u90fd\u6ca1\u6253\u7b97\u4e70\uff0c\u56e0\u4e3a\u76f8\u4fe1\u4e50\u8da3\u6765\u81ea\u4e8e\u4f7f\u7528\u8005\uff0c\u4e0d\u5728\u4e8e\u4e1c\u897f\u672c\u8eab\u3002 \u5982\u679c\u6ca1\u6709\u53d1\u6325\u51fa\u5f53\u524d\u7269\u54c1\u7684\u6240\u6709\u6f5c\u529b\uff0c\u4f55\u5fc5\u5347\u7ea7\u3002</p>"},{"location":"writing/2011/#may-2-2011","title":"May 2, 2011","text":"<p>\u6700\u8fd1\u624b\u8d31\u53c8\u5f00\u59cb\u6446\u5f04linux\u3002\u8d8a\u6765\u8d8a\u503e\u5411\u4e8e\u7b80\u5316\u7cfb\u7edf\uff0c\u5bfb\u627e\u6027\u80fd/\u8d44\u6e90\u6d88\u8017\u6bd4\u6700\u9ad8 \u7684\u8f6f\u4ef6\uff0c\u4e8e\u662f\u66f4\u503e\u5411\u4e8e\u6587\u5b57\u754c\u9762\uff0c\u4e0d\u5b8c\u5168\u662f\u4e3a\u4e86\u64cd\u4f5c\u65b9\u4fbf\uff0c\u800c\u662f\u5f88\u591a\u6587\u5b57\u754c\u9762\u7684 \u8f6f\u4ef6\u529f\u80fd\u6bd4\u56fe\u5f62\u754c\u9762\u7684\u540c\u7c7b\u8f6f\u4ef6\u591a\u3002</p>"},{"location":"writing/2011/#may-3-2011","title":"May 3, 2011","text":"<p>\u628a\u7cfb\u7edf\u6e05\u4e86\u4e00\u904d\uff0c\u7a0b\u5e8f\u90fd\u8f7b\u91cf\u7ea7\u5316\u4e86\uff0c\u4e0d\u5fc5\u8981\u7684\u90fd\u4e0d\u8fd0\u884c\uff0c\u8fd0\u884c\u7684\u90fd\u6311\u4e0d\u592a\u5360\u8d44 \u6e90\u7684\u3002\u679c\u7136\u7701\u4e0b\u4e00\u5806\u7cfb\u7edf\u8d44\u6e90\uff0c\u660e\u663e\u611f\u5230\u5ef6\u8fdf\u65f6\u95f4\u8f83\u5c11\uff0c\u66f4\u6bd4\u5b9e\u9a8c\u5ba4\u914d\u7f6e\u66f4\u597d\u7684 \u82f9\u679c\u673a\u5feb\u591a\u4e86\u3002</p>"},{"location":"writing/2011/#may-4-2011","title":"May 4, 2011","text":"<p>\u8bd5\u4e86\u8bd5openbox\uff0c\u5c45\u7136\u5b8c\u5168\u6ca1\u6709\u8f7d\u5165\u65f6\u95f4\uff0c\u592a\u5f3a\u5927\u4e86\uff0c\u4ee5\u540e\u6162\u6162\u7814\u7a76\u3002\u7531\u4e8e\u53ea\u78e8 \u5200\u4e0d\u780d\u67f4\u7684\u4e60\u60ef\u5f88\u4e0d\u597d\uff0c\u6700\u8fd1\u5c31\u4e0d\u8981\u73a9\u5f04\u7535\u8111\u4e86\uff0c\u597d\u597d\u7528\u6765\u5e72\u6d3b\u3002</p>"},{"location":"writing/2011/#may-12-2011","title":"May 12, 2011","text":"<p>\u5b9e\u9a8c\u5ba4\u5927\u626b\u9664\u8fc7\u540e\uff0c\u6052\u6e29\u6c34\u6d74\u7bb1\u91cc\u7684\u5851\u6599\u5c0f\u9e2d\u5b50\u4e0d\u89c1\u4e86\uff0c\u7814\u7a76\u8fc7\u540e\u5927\u5bb6\u628a\u8001\u677f\u5b9a \u4e3a\u5acc\u7591\u72af\u3002\u8001\u677f\u65e0\u5948\u4e70\u4e86\u51e0\u53ea\u66f4\u5927\u7684\u5c0f\u9e2d\u5b50\uff0c\u4e00\u53ea\u653e\u6c34\u6d74\u7bb1\uff0c\u5176\u5b83\u51e0\u53ea\u9001\u7ed9\u6700\u5728 \u4e4e\u5c0f\u9e2d\u5b50\u7684\u4eba\u3002</p>"},{"location":"writing/2011/#may-16-2011","title":"May 16, 2011","text":"<p>\u91cd\u6e29\u5934\u6587\u5b57D\u5168\u96c6\uff0c\u6700\u5927\u7684\u611f\u60f3\u662f\uff0c\u4e1c\u897f\u6ca1\u6709\u7528\u574f\u5c31\u4e0d\u8981\u6362\uff0c\u7528\u574f\u4e86\u4fee\u4fee\u80fd\u7528\u4e5f \u4e0d\u8981\u6362\uff0c\u8ba9\u65e7\u4e1c\u897f\u53d1\u6325\u4f59\u70ed\u624d\u53eb\u80fd\u529b\u3002</p>"},{"location":"writing/2011/#may-28-2011","title":"May 28, 2011","text":"<p>\u89c1\u8bc6\u5230\u5b9e\u9a8c\u5ba4\u524d\u8f88\u4eec\u5199\u6bd5\u4e1a\u8bba\u6587\u7684\u54c0\u568e\u72b6\uff0c\u81ea\u5df1\u4e5f\u611f\u89c9\u5230\u6570\u636e\u91cf\u7684\u538b\u8feb\u611f\uff0c\u6240\u4ee5 \u7740\u624b\u5199\u6bd5\u4e1a\u8bba\u6587\uff0c\u53cd\u6b63\u4ece\u5934\u5230\u5c3e\u7684\u4e1c\u897f\u90fd\u8981\u585e\u8fdb\u53bb\uff0c\u4e0d\u5982\u505a\u5230\u54ea\u5199\u5230\u54ea\u3002\u4e0a\u7f51\u627e \u4e86\u4e2a\u5356\u76f8\u597d\u7684\u6bd5\u4e1a\u8bba\u6587LaTeX\u6a21\u677f\uff0c\u8fb9\u7528\u8fb9\u4fee\u6539\u3002\u957f\u6587\u6863\u8fd8\u662f\u7528TeX\u8d34\u5fc3\uff0c\u6f02\u4eae\u53c8 \u542c\u8bdd\u3002</p>"},{"location":"writing/2011/#jun-6-2011","title":"Jun 6, 2011","text":"<p>\u5728\u5bb6\u91cc\u7ec3\u524d\u6eda\u7ffb\u540e\u6eda\u7ffb\uff0c\u54af\u5f97\u80a9\u8180\u75bc\u3002\u591a\u6eda\u6eda\u5e94\u8be5\u4f1a\u597d\u4e9b\u3002</p>"},{"location":"writing/2011/#jul-7-2011","title":"Jul 7, 2011","text":"<p>\u73a9\u4e86\u4f20\u8bf4\u4e2d\u5f88\u96be\u7684\u6076\u9b54\u4e4b\u9b42\uff0c\u7b2c\u4e00\u5173\u5c31\u6b7b\u4e86\u65e0\u6570\u6b21\uff0c\u611f\u53f9\u73a9\u8fd9\u4e2a\u6e38\u620f\u6bd4\u559c\u6b22\u4e00\u4e2a \u4eba\u56f0\u96be\u591a\u4e86\u3002\u540e\u6765\u638c\u63e1\u4e86\u7a8d\u95e8\u770b\u4e86\u653b\u7565\uff0c\u53c8\u89c9\u5f97\u8fd8\u662f\u559c\u6b22\u4e00\u4e2a\u4eba\u66f4\u56f0\u96be\uff0c\u81f3\u5c11\u73a9 \u6e38\u620f\u53ef\u4ee5\u5b9a\u65f6\u5b9a\u91cf\u3002</p>"},{"location":"writing/2011/#jul-10-2011","title":"Jul 10, 2011","text":"<p>\u6ca1\u5929\u7406\u554a\uff0c\u65e9\u4e0a8\u70b9\u5728\u4e00\u6761\u4e00\u70b9\u90fd\u4e0d\u504f\u50fb\u7684\u8def\u7684\u5c0f\u5df7\u91cc\uff0c\u90fd\u6709\u4eba\u88ab\u6301\u67aa\u62a2\u52ab\uff0c\u94b1 \u5305\u548c\u4e66\u5305\u3002\u8fd9\u5e74\u5934\u80cc\u4e66\u5305\u88c5\u7a77\u90fd\u4e0d\u7ba1\u7528\u4e86\u3002\u9ed8\u5ff5\u65e9\u70b9\u6bd5\u4e1a\u65e9\u70b9\u6bd5\u4e1a\u3002\u3002</p>"},{"location":"writing/2011/#jul-22-2011","title":"Jul 22, 2011","text":"<p>\u521a\u5f00\u59cb\u5b66\u9f13\u65f6\uff0c\u65e0\u6cd5\u7406\u89e3\u4e3a\u4ec0\u4e48\u51e0\u4e4e\u6240\u6709\u6253\u9f13\u7684\u4eba\u90fd\u63a8\u5d07AC/DC\u7684Back in black\uff0c \u542c\u4e86\u5f88\u591a\u904d\u4f9d\u7136\u96be\u542c\uff0c\u800c\u4e14\u9f13\u70b9\u65e2\u4e0d\u5feb\uff0c\u4e5f\u4e0d\u590d\u6742\u3002\u6700\u8fd1\u91cd\u542c\u4e86\u5f88\u591a\u904d\uff0c\u5374\u8d8a\u542c \u8d8a\u89c9\u5f97\u7ecf\u5178\uff0c\u4e5f\u4f53\u4f1a\u4e86\u8d8a\u57fa\u7840\u7684\u9f13\u70b9\u8d8a\u96be\u6253\u597d\uff0c\u7a0d\u5fae\u6709\u70b9\u504f\u5dee\u5c31\u80fd\u542c\u51fa\u6765\u3002</p>"},{"location":"writing/2011/#jul-30-2011","title":"Jul 30, 2011","text":"<p>\u5230\u5e97\u91cc\u8bd5\u73a9\u4e86Roland TD 20\uff0c\u592a\u534e\u4e3d\u4e86\uff0c\u4f4e\u9f13\u7f51\u9762\u4e4b\u7075\u654f\uff0c\u9f13\u9762\u89e6\u611f\u4e4b\u4eff\u771f\uff0c\u6700 \u91cd\u8981\u7684\u662f\u5b89\u9759\uff0c\u4e0d\u6270\u6c11\u3002\u5fc3\u91cc\u7acb\u9a6c\u957f\u4e86\u8349\u3002\u4e0d\u8fc7\u9ad8\u7aef\u7535\u9f13\u7684\u4ef7\u683c\u9ad8\u7684\u4e0d\u6210\u6837\u5b50\uff0c \u6211\u7834\u8f66\u90fd\u53ef\u4ee5\u4e70\u4e24\u4e2a\u4e86\u3002</p>"},{"location":"writing/2011/#aug-3-2011","title":"Aug 3, 2011","text":"<p>\u770b\u5b8c\u8f7b\u97f3\u5c11\u5973\uff0c\u70ed\u8840\u6f8e\u6e43\u5730\u60f3\u771f\u6b63\u505a\u4e2a\u534a\u540a\u5b50\u9f13\u624b\u3002\u4e00\u76f4\u662fyoutube\u6559\u5b66\u515a\uff0c\u7279 \u522b\u6015\u7ec3\u5e9f\uff0c\u6216\u517b\u6210\u4e0d\u826f\u4e60\u60ef\u8981\u780d\u6389\u91cd\u7ec3\u3002\u4e00\u62cd\u8111\u74dc\uff0c\u7acb\u9a6c\u62dc\u5e08\u3002\u7b2c\u4e00\u8282\u8bfe\u6572\u963f\u6572 \u505a\u6c34\u5e73\u6d4b\u8bd5\uff0c\u5e08\u5085\u8868\u626c\u8bf4\u52a8\u4f5c\u5c45\u7136\u5f88\u6807\u51c6\u4e0d\u9700\u8981\u6539\u8fdb\uff0c\u5fc3\u91cc\u6697\u559c\u3002\u7531\u4e8e\u5728\u516c\u5bd3\u91cc \u7ec3\u6015\u6270\u6c11\uff0c\u6211\u4e00\u76f4\u8f7b\u624b\u8f7b\u811a\uff0c\u6b6a\u6253\u6b63\u7740\uff0c\u56e0\u4e3a\u4ece\u8f7b\u5230\u91cd\u6613\uff0c\u4ece\u91cd\u5230\u8f7b\u96be\uff0c\u800c\u4e14\u8f7b \u7740\u7ec3\u5bf9\u638c\u63e1\u6280\u672f\u6709\u597d\u5904\u3002\u6709\u8001\u5e08\u786e\u5b9e\u4e0d\u4e00\u6837\uff0c\u51e0\u756a\u6d4b\u8bd5\u540e\u5c31\u6293\u4f4f\u4e86\u7f3a\u70b9\uff0c\u4e8e\u662f\u5236 \u5b9a\u4f5c\u4e1a\u3002\u4e4b\u524d\u7684\u7ec3\u4e60\u5bf9\u5de6\u817f\u953b\u70bc\u4e0d\u591f\uff0c\u5de6\u817f\u64cd\u4f5c\u7684\u94dc\u94b9\u4ece\u6765\u4e0d\u7528\uff0c\u6240\u4ee5\u5de6\u817f\u4e00\u52a0 \u8fdb\u6765\u5c31\u4e0d\u548c\u8c10\u4e86\u3002</p>"},{"location":"writing/2011/#aug-10-2011","title":"Aug 10, 2011","text":"<p>\u5728\u663e\u5fae\u955c\u623f\u5446\u4e86\u4e00\u5929\uff0c\u7761\u5f97\u6b7b\u53bb\u6d3b\u6765\u3002\u505a\u7684\u8fd9\u4e2a\u5b9e\u9a8c\u8981\u628a\u5f71\u7247\u62cd\u5b8c\u4e86\u653e\u4e00\u904d\u624d\u77e5 \u9053\u7ed3\u679c\uff0c\u4e0d\u80fd\u8fb9\u505a\u8fb9\u770b\uff0c\u6240\u4ee5\u5927\u90e8\u5206\u65f6\u5019\u5bf9\u7740\u955c\u5934\u53d1\u5446\uff0c\u5076\u5c14\u8c03\u6574\u4e0b\u7126\u8ddd\u3002\u627e\u4e86 \u5404\u79cd\u4e8b\u60c5\u505a\uff0c\u4e00\u4f1a\u505a\u56db\u80a2\u72ec\u7acb\u7ec3\u4e60\uff0c\u4e00\u4f1a\u7ec3\u51e0\u4e2a\u9f13\u70b9\uff0c\u4e00\u4f1a\u5de6\u624b\u62ff\u7740\u94c5\u7b14\u5f53\u4f5c\u9f13 \u69cc\u6572\uff0c\u53f3\u624b\u73a9\u626b\u96f7\u3002\u4e0d\u8fc7\u90fd\u662f\u91cd\u590d\u6027\u52a8\u4f5c\uff0c\u52a0\u4e0a\u663e\u5fae\u955c\u6bcf\u969410\u79d2\u62cd\u4e00\u6b21\u7167\u7684\u5494\u5693 \u58f0\uff0c\u6ca1\u591a\u4e45\u5c31\u7761\u7740\u4e86\u3002\u4e0b\u6b21\u5f97\u5e26\u4e2a\u6e38\u620f\u6253\u6253\uff0c\u6216\u8005\u5e26\u4e0a\u7535\u8111\u770b\u661f\u9645\u4e89\u9738\u6bd4\u8d5b\u5f55\u50cf\u3002</p>"},{"location":"writing/2011/#aug-13-2011","title":"Aug 13, 2011","text":"<p>\u770b\u4e86\u5f88\u591a\u904d\u8f7b\u97f3\u5c11\u5973\u7684\u7247\u5934\uff0c\u5c45\u7136\u5f88\u611f\u52a8\u3002\u6ca1\u6709\u60f3\u5230\u4e00\u4e2a\u52a8\u753b\u6709\u90a3\u4e48\u5927\u7684\u9b54\u529b\uff0c \u8ba9\u6211\u7a81\u7136\u5bf9\u97f3\u4e50\u5174\u8da3\u90a3\u4e48\u5927\uff0c\u51b3\u5b9a\u597d\u597d\u5b66\u4e60\u4e50\u7406\uff0c\u542c\u5404\u79cd\u6d3e\u7cfb\uff0c\u953b\u70bc\u8033\u6735\u3002\u54b1\u867d \u7136\u4e94\u97f3\u4e0d\u5168\u4e0d\u5531\u6b4c\uff0c\u4f46\u4e5f\u6709\u4e2a\u52a0\u5165\u4e50\u961f\u7684\u7406\u60f3\uff0c\u8def\u98de\u4e0d\u4f1a\u6e38\u6cf3\u4e0d\u662f\u4e5f\u80fd\u505a\u6d77\u8d3c\u4e48\u3002</p>"},{"location":"writing/2011/#aug-16-2011","title":"Aug 16, 2011","text":"<p>\u81ea\u4ece\u6700\u8001\u8d44\u683c\u7684\u5b66\u751f\u6bd5\u4e1a\uff0c\u73b0\u5728\u5927\u5bb6\u6709\u95ee\u9898\u90fd\u6765\u627e\u6211\u4e86\uff0c\u800c\u4e0d\u662f\u6bd4\u6211\u5927\u4e00\u7ea7\u7684\u5b85 \u7537\u540c\u5b66\u3002\u7d2f\u6b7b\u6211\u4e86\uff0c\u5b9e\u9a8c\uff0c\u5206\u6790\u6570\u636e\uff0c\u4f5c\u56fe\u90fd\u4e0d\u65ad\u6709\u4eba\u95ee\u3002\u6709\u65f6\u5019\u89c9\u5f97\u4e00\u5929\u5fd9\u5f97 \u5f88\uff0c\u5230\u5934\u6765\u90fd\u662f\u5e2e\u4eba\uff0c\u51fa\u7684\u6570\u636e\u90fd\u662f\u522b\u4eba\u7684\uff0c\u6211\u81ea\u5df1\u7684\u53cd\u5012\u6ca1\u5565\u7a7a\u95f2\u505a\u5b9e\u9a8c\u4e86\u3002</p>"},{"location":"writing/2011/#aug-17-2011","title":"Aug 17, 2011","text":"<p>\u5df2\u7ecf\u6ca1\u6709\u4eba\u80fd\u963b\u6321\u6211\u5931\u8d25\u7684\u6b65\u4f10\u4e86\u3002\u8fd9\u5468\u7528\u5565\u5565\u574f\uff0c\u4ec0\u4e48\u5b9e\u9a8c\u90fd\u6ca1\u6709\u505a\u8d77\u6765\uff0c\u6574 \u5929\u5728\u542d\u54e7\u542d\u54e7\u7684\u627e\u9519\u3002</p>"},{"location":"writing/2011/#aug-25-2011","title":"Aug 25, 2011","text":"<p>\u60f3\u7ffb\u51fa\u4ee5\u524d\u753b\u8fc7\u7684\u77ed\u7bc7\u8349\u7a3f\u7092\u4e2a\u51b7\u996d\uff0c\u56e0\u4e3a\u9898\u6750\u8fd8\u86ee\u559c\u6b22\u3002\u7ffb\u7bb1\u5012\u67dc\u627e\u4e0d\u5230\uff0c\u4e0d \u77e5\u9053\u4ee5\u524d\u753b\u7684\u90fd\u6254\u54ea\u53bb\u4e86\uff0c\u53ea\u627e\u5230\u4e00\u580618+\u7684\u56fe\u3002\u3002</p>"},{"location":"writing/2011/#sep-9-2011","title":"Sep 9, 2011","text":"<p>\u751f\u5316\u5371\u673a\u4e86\u30026\u697c\u5fae\u751f\u7269\u5b9e\u9a8c\u5ba4\u53bb\u5e74\u6b7b\u4e86\u4e2a\u4eba\uff0c\u6700\u8fd1\u53c8\u51fa\u4e86\u4e8b\uff0c\u4e0d\u8d1f\u8d23\u4efb\u7684\u6559\u6388 \u8fdd\u53cd\u89c4\u5b9a\uff0c\u7814\u7a76\u5371\u9669\u7b49\u7ea72.5\u7684\u75c5\u83cc\uff0c\u800c\u6211\u4eec\u8fd9\u680b\u697c\u6700\u591a\u5141\u8bb8\u7814\u7a76\u7b49\u7ea72\u7684\u75c5\u83cc\u3002 \u6559\u6388\u5b9e\u9a8c\u5ba4\u7c97\u5fc3\u7684\u7537\u5b66\u751f\u56e0\u4e3a\u4e0d\u6362\u624b\u5957\u628a\u75c5\u83cc\u5e26\u5230\u4e86\u9694\u58c1\u5b9e\u9a8c\u5ba4\uff0c\u9694\u58c1\u5b9e\u9a8c\u5ba4\u7684 \u5973\u53d7\u5bb3\u8005\u4e0d\u5e78\u7684\u6234\u7740\u624b\u5957\u78b0\u4e86\u4e0b\u8138\uff0c\u611f\u67d3\u4e86\uff0c\u73b0\u5728\u9700\u8981\u505a\u8138\u90e8\u624b\u672f\u3002\u5fae\u751f\u7269\u90e8\u95e8 \u5f88\u4e0d\u539a\u9053\u7684\u5728\u7b2c\u4e00\u65f6\u95f4\u5c01\u9501\u6d88\u606f\uff0c\u8fc7\u4e86\u4e00\u9635\u5b50\u624d\u516c\u5f00\uff0c\u8bf4\u8981\u597d\u597d\u6d88\u6bd2\u30026\u697c\u7684\u4eba \u5168\u90e8\u64a4\u79bb\uff0c\u697c\u9053\u7535\u68af\u90fd\u6d88\u6bd2\u3002\u73b0\u5728\u5750\u7535\u68af\u518d\u4e5f\u6ca1\u6709\u4eba\u63096\u3002</p>"},{"location":"writing/2011/#sep-22-2011","title":"Sep 22, 2011","text":"<p>\"\u6765\u7897\u841d\u535c\u725b\u6742\uff0c\u4e0d\u8981\u725b\u6742\"</p> <p>\u670d\u52a1\u5458\u7b11\u5f00\u4e86\u82b1\uff0c\u7aef\u4e0a\u6765\u6ee1\u6ee1\u4e00\u7897\u841d\u535c\u3002</p> <p>\u8def\u9047\u4e00\u9ed1\u4eba\u5927\u5988\uff0c\u624b\u91cc\u63e1\u7740\u5757\u5199\u5b57\u677f\uff0c\u4e0a\u9762\u5199\u7740\u5927\u5927\u7684\"\u4e50\u5929\u738b\u5199\u5b57\u677f\"\uff0c\u4e50\u4e86\u3002</p>"},{"location":"writing/2011/#oct-13-2011","title":"Oct 13, 2011","text":"<p>\u5fc3\u788e\u4e86\uff0c\u5b66\u6821\u7684\u7ec3\u9f13\u5ba4\u6ca1\u6709\u9f13</p>"},{"location":"writing/2011/#oct-14-2011","title":"Oct 14, 2011","text":"<p>\u505a\u4e00\u5929\u663e\u5fae\u955c\uff0c\u4e00\u624b\u8c03\u4e86\u4e00\u5929\u7126\u8ddd\uff0c\u53e6\u4e00\u624b\u73a9\u4e86\u4e00\u5929\u626b\u96f7\u3002\u6ee1\u773c\u5730\u96f7\u7684\u5fc6\u8d77\uff0c\u4e00 \u5929\u8001\u677f\u5e26\u4ed6\u7238\u53c2\u89c2\uff0c\u63a8\u5f00\u663e\u5fae\u955c\u5ba4\u95e8\u8bf4\u7238\u7ed9\u4f60\u770b\u770breal scientist\u662f\u600e\u4e48\u505a\u5b9e\u9a8c \u7684\uff0c\u6211\u7acb\u9a6c\u5173\u626b\u96f7\u4f46\u5c3c\u739b\u7684\u673a\u5668\u6709\u5ef6\u8fdf\uff0c\u4e8e\u662f\u4e09\u4eba\u77f3\u5316\u3002\u4ece\u6b64\u6211\u8bb0\u5f97\u505a\u663e\u5fae\u955c\u8981 \u9501\u95e8\u3002</p>"},{"location":"writing/2011/#nov-6-2011","title":"Nov 6, 2011","text":"<p>\u6628\u5929\u548c\u672c\u7cfb\u4e2d\u56fd\u540c\u5b66\u5403\u996d\uff0c\u5e2d\u95f4\u516b\u5366\u7cfb\u91cc\u54ea\u4e00\u5c4a\u8c01\u662fgay\uff0c\u7136\u540e\u8ba8\u8bba\u5f88\u591agay\u4e0d\u8bf4 \u6240\u4ee5\u770b\u4e0d\u51fa\u6765\u3002\u4f5c\u4e3a08\u5c4a\u7684\u4ee3\u8868\uff0c\u6211\u7b11\u800c\u4e0d\u8bed\uff0c\u6709\u70b9\u50cf\u542c\u4eba\u8ba8\u8bba\u5927\u718a\u732b\u884c\u4e3a\u548c\u751f \u6d3b\u4e60\u6027\uff0c\u5fc3\u60f3\u5bf9\u963f\u5bf9\u963f\u8fd9\u91cc\u5c31\u5750\u7740\u4e00\u4e2a\u4f60\u4eec\u4e0d\u4e5f\u662f\u770b\u4e0d\u51fa\u6765\u4e48\u3002</p>"},{"location":"writing/2011/#nov-11-2011","title":"Nov 11, 2011","text":"<p>\u6076\u9b54\u4e4b\u9b42\u592a\u6076\u5fc3\u4e86\uff0c\u5404\u79cd\u5751\u7239\u6b7b\u6cd5\uff0c\u5404\u79cd\u5751\u7239\u5730\u5f62\u3002\u672c\u6765\u56e0\u4e3a\u88ab4-2\u6076\u5fc3\u5230\uff0c\u51c6 \u5907\u5356\u6389\uff0c\u4f46\u53c8\u89c9\u5f97\u81ea\u8650\u4e86\u90a3\u4e48\u4e45\u4e0d\u901a\u5173\u4e0d\u7518\u5fc3\uff0c\u53c8\u6361\u56de\u6765\u7ee7\u7eed\u73a9\u3002\u5f13\u7bad\u840e\u7f29\u6d41\u6253 \u53d1\u8fc7\u4e864-2\uff0c\u5f00\u59cb\u62535-2\uff0c\u624d\u77e5\u9053\u4ec0\u4e48\u53eb\u6ca1\u6709\u6700\u6076\u5fc3\uff0c\u53ea\u6709\u66f4\u6076\u5fc3\u3002\u6cbc\u6cfd\u5730\u91cc\u6389\u8840 \u4e0d\u8bf4\uff0c\u8fd8\u52a8\u4f5c\u6162\uff0c\u4e0d\u80fd\u95ea\u907f\u3002</p>"},{"location":"writing/2011/#nov-15-2011","title":"Nov 15, 2011","text":"<p>Noah and the Whale\u6f14\u5531\u4f1a</p> <p>\u5230Lincoln Hall\u542c\u7684\uff0c\u4e0d\u5927\u7684\u5385\u4f46\u6548\u679c\u5f88\u597d\u3002\u5e08\u59b9\u662f\u7c89\u4e1d\u6240\u4ee5\u6211\u4eec\u5f88\u65e9\u5c31\u53bb\u4e86\uff0c \u8d34\u7740\u821e\u53f0\u6b63\u4e2d\uff0c\u5e94\u8be5\u662f\u6700\u597d\u7684\u4f4d\u7f6e\u4e86\u3002\u5934\u4e00\u6b21\u8fd9\u4e48\u8fd1\u8ddd\u79bb\uff0c\u889c\u5b50\u90fd\u770b\u5f97\u4e00\u6e05\u4e8c\u695a\uff0c \u5e78\u597d\u6211\u6234\u4e86\u8033\u585e\u3002\u6696\u573a\u7684\u4e50\u961f\u4e3b\u5531\u662f\u4e2a\u7626\u9ad8\u7f8e\u5973\uff0c\u53f0\u98ce\u5f88\u597d\u3002\u7136\u540eN and W\u6765\u4e86\uff0c \u73b0\u573a\u5f88\u4e0d\u9519\uff0c\u8d1d\u53f8\u624b\u90a3\u4e2a\u6d3b\u8dc3\uff0c\u62cd\u7684\u7167\u7247\u5168\u662f\u7cca\u7684\u3002</p>"},{"location":"writing/2011/#dec-7-2011","title":"Dec 7, 2011","text":"<p>\u4e39\u4f5b\u5f00\u4f1a\u56de\u6765\u3002\u5c55\u51faposter\u7684\u90a3\u5929\u7ad9\u4e864\u4e2a\u591a\u5c0f\u65f6\uff0c\u53d7\u5ba0\u82e5\u60ca\u3002\u8fd8\u7b97\u633a\u987a\u5229\uff0c\u88ab \u5938\u7684\u98d8\u7136\u4e86\u3002\u4e0d\u8fc7\u770b\u5230\u4eba\u5bb6\u505a\u7684\u4e1c\u897f\u624d\u611f\u89c9\u5230\u4e0d\u80fd\u4eba\u6bd4\u4eba\uff0c\u5c24\u5176\u522b\u8ddf\u522b\u7684phd\u6bd4\u3002 \u5728\u573a\u5730\u4e71\u8e7f\u7684\u65f6\u5019\u88ab\u4e24\u4e2a\u4eba\u62c9\u4f4f\u95ee\u613f\u4e0d\u613f\u610f\u63a5\u53d72\u5206\u949f\u91c7\u8bbf\uff0c\u672c\u60f3\u8bf4\u6211\u5356\u58f0\u4e0d\u5356 \u8138\uff0c\u4f46\u4eba\u8bf4\u6709\u514d\u8d39t-shirt\u62ff\uff0c\u6211\u5c31\u8ddf\u8fc7\u53bb\u4e86\u3002</p> <p>\u53bb\u56fd\u9645\u79d1\u7814\u4eba\u5458\u8ba8\u8bba\u4f1a\u6253\u4e86\u4e2a\u9171\u6cb9\uff0c\u611f\u60f3\u5c31\u662f\u5370\u5ea6\u4eba\u771f\u80fd\u8bf4\uff0c\u5f88\u4e3b\u52a8\u4f5c\u4e3a\u5c0f\u7ec4\u4ee3 \u8868\u4e0a\u53f0\u8bf4\u8bdd\uff0c\u602a\u4e0d\u5f97\u5728\u7f8e\u56fd\u6bd4\u534e\u4eba\u6df7\u5f97\u5f00\u3002</p> <p>\u7ecf\u53d7\u90a3\u4e48\u591a\u5929\u7684\u5b66\u672f\u6d17\u793c\uff08\u867d\u7136\u5269\u4e0b\u65f6\u95f4\u90fd\u5728\u9152\u5e97\u7761\u5927\u89c9\uff0c\u4fdd\u5b88\u4f30\u8ba1\u4e00\u592910\u4e2a\u5c0f \u65f6\uff09\uff0c\u89c9\u5f97\u6162\u6162\u51cf\u5f31\u7684\u6597\u5fd7\u53c8\u56de\u6765\u4e86\u3002\u5f88\u4e45\u4ee5\u524d\u4f1a\u6beb\u4e0d\u72b9\u8c6b\u7684\u53bb\u4e89\u53d6\u8ba4\u4e3a\u81ea\u5df1\u591f \u4e0d\u7740\u7684\u4e1c\u897f\uff0c\u4f46\u5f88\u591a\u65f6\u5019\u90fd\u610f\u5916\u7684\u591f\u7740\u4e86\uff0c\u4e8e\u662f\u89c9\u5f97\u5347\u7ea7\u6210\u529f\u3002\u73b0\u5728\u5374\u5f88\u88ab\u52a8\uff0c \u89c9\u5f97\u81ea\u5df1\u80fd\u591f\u8fbe\u523090\u5206\u7684\u4e1c\u897f\uff0c\u53ea\u4f1a\u671d\u5f97\u523080\u5206\u52aa\u529b\u3002\u4e0d\u7ba1\u600e\u4e48\u8bf4\uff0c\u53d7\u5230\u523a\u6fc0\u4e4b \u540e\u4eca\u5929\u4ece\u673a\u573a\u76f4\u63a5\u56de\u4e86\u5b9e\u9a8c\u5ba4\uff0c\u4e3a\u660e\u5929\u7684\u5b9e\u9a8c\u51c6\u5907\u4e86\u4e00\u4e9b\u4e1c\u897f\u3002</p>"},{"location":"writing/2011/#dec-9-2011","title":"Dec 9, 2011","text":"<p>\u4eca\u5929\u7535\u8111\u600e\u4e48\u542f\u52a8\u90fd\u6b7b\u673a\uff0c\u6362\u6210xfce\u7167\u6837\u6b7b\uff0c\u6362\u6210\u82f1\u6587\u7cfb\u7edf\u5374\u987a\u7545\uff0c\u627e\u4e86\u534a\u5929\u539f \u6765\u662fscim-pinyin\u7684\u95ee\u9898\uff0c\u5565\u95ee\u9898\u4e0d\u77e5\u9053\uff0c\u4e0d\u8fc7\u53ea\u8981\u88c5\u4e86\u5b83\u5c31\u4f1a\u6b7b\u3002\u4e8e\u662f\u5220\u6389\uff0c \u53cd\u6b63\u6211\u4e5f\u7528google\u62fc\u97f3\u3002\u592a\u795e\u5947\u4e86\uff0c\u4ee5\u524d\u4e00\u76f4\u7528\u7684\u597d\u597d\u7684\uff0c\u4e3a\u5565\u4eca\u5929\u51fa\u95ee\u9898\u3002</p>"},{"location":"writing/2011/#dec-19-2011","title":"Dec 19, 2011","text":"<p>\u628a\u6218\u795e1\u5f00\u5c01\u62ff\u6765\u73a9\u4e86\u3002\u8272\u60c5\u90e8\u5206\u8fd8\u597d\uff0c\u5c0f\u6e38\u620f\u633a\u6709\u8da3\u3002\u8840\u8165\u672c\u8eab\u6ca1\u5565\uff0c\u4f46\u62ff\u58eb \u5175\u8840\u796d\u7684\u955c\u5934\u8d85\u8d8a\u6211\u5bf9\u60c5\u8282\u7684\u5e95\u7ebf\u4e86\u3002\u4f5c\u4e3a\u52a8\u4f5c\u6e38\u620f\u6ca1\u5f97\u8bf4\uff0c\u4e0d\u8fc7\u60c5\u8282\u65e0\u6cd5\u63a5\u53d7\uff0c \u5356\u4e4b\u3002</p>"},{"location":"writing/2012/","title":"2012\u65e5\u8bb0","text":""},{"location":"writing/2012/#jan-10-2012","title":"Jan 10, 2012","text":"<p>\u6076\u9b54\u4e4b\u9b42\u53d7\u8650\u5e76\u901a\u5173\u4e4b\u540e\u5c31\u4e00\u76f4\u60f3\u7740\u4e70\u9ed1\u6697\u4e4b\u9b42</p> <p>\u653e\u5f03\u4e86\u65e0\u5c3d\u6c99\u52a0\uff0c\u6e38\u620f\u662f\u5f88\u6709\u5473\u9053\uff0c\u5c31\u662f\u4e0a\u624b\u592a\u4e0d\u9002\u5e94\uff0c\u4e4b\u524d\u73a9RPG\u7684\u7ecf\u9a8c\u5b8c\u5168 \u7528 \u4e0d\u4e0a\uff0c\u6574\u4e2a\u5c31\u662f\u4e0d\u4e00\u6837\u7684\u7cfb\u7edf\u3002\u5012\u662f\u5f88\u6709\u684c\u9762\u6e38\u620f\u7684\u5473\u9053\uff0c\u8f6c\u8f6e\u76d8\uff0c\u56de\u5408\u5236 \u884c\u52a8\u3002 \uff08\u4e3a\u4e86\u8282\u7ea6\u5236\u4f5c\u6210\u672c\uff09\u753b\u9762\u59822D\u7eb8\u7247\uff0c\u4e0d\u8fc7\u6c34\u5f69\u771f\u6f02\u4eae\u3002Ventus\u8def\u7ebf\u73a9 \u5230\u4e00\u5927 \u534a\uff0c\u7ec8\u4e8e\u88ab\u5404\u79cd\u607c\u4eba\u7684\u968f\u673a\u6027\u548c\u91cd\u590d\u7684\u8ff7\u5bab\u7ed9\u70e6\u5230\uff0c\u4e0d\u73a9\u4e86\u3002\u5176\u5b9eRuby \u8def\u7ebf\u7684\u60c5 \u8282\u5f88\u6709\u610f\u601d\uff0c\u8981\u662f\u4e00\u5f00\u59cb\u73a9\u5979\u5c31\u597d\u4e86\u3002</p> <p>NDS\u4e0a\u590d\u523b\u7684\u9a6c\u91cc\u596564\u901a\u5173\uff0c\u628a\u516c\u4e3b\u6551\u51fa\u6765\u4e86\u3002\u53ef\u4ee5\u81ea\u5df1\u9009\u7528\u89d2\u8272\u662f\u4e2a\u4eae\u70b9\uff0c\u5927 \u90e8 \u5206\u5173\u5361\u6211\u90fd\u7528\u5c0f\u6050\u9f99\u8000\u897f\u901a\u7684\uff0c\u9700\u8981\u6362\u4eba\u7684\u65f6\u5019\u5c31\u627e\u5230\u76f8\u5e94\u4eba\u7269\u7684\u5e3d\u5b50\u53d8\u8eab\u3002 \u955c \u5934\u5f88\u5751\u7239\uff0c\u7ecf\u5e38\u8d70\u7740\u8d70\u7740\u4e1c\u897f\u5c31\u628a\u4eba\u6321\u4f4f\u770b\u4e0d\u89c1\u4e86\uff0c\u6216\u8005\u624b\u67c4\u6ca1\u8ddf\u7740\u955c\u5934\u8c03 \u6574\u4eba \u8d70\u7684\u597d\u597d\u7684\u5c31\u6389\u4e0b\u53bb\u4e86\uff0c\u5c24\u5176\u662f\u8df3\u8dc3\u7684\u65f6\u5019\u96be\u638c\u63e1\u7740\u9646\u70b9\u3002\u4e00\u822c\u7684\u5173\u5361\u90fd \u8fd8\u597d\uff0c \u53cd\u6b63\u4e0d\u957f\uff0c\u6700\u540e\u4e00\u4e2aboss\u5173\u524d\u7684\u90a3\u6bb5\u8def\u5c31\u51c4\u60e8\u4e86\u70b9\uff0c\u6389\u4e0b\u53bb\u65e0\u6570\u6b21\uff0c\u955c \u5934\u4e00\u6643\u4eba \u7269\u6d88\u5931\u65e0\u6570\u6b21\uff0c\u57fa\u672c\u8981\u8d70\u4e00\u4e0b\u624b\u52a8\u8c03\u4e00\u4e0b\u955c\u5934\u3002\u597d\u5728\u5236\u4f5c\u7ec4\u6bd4\u8f83\u4f53\u8d34\uff0c \u6311\u6218boss \u5931\u8d25\u540e\u7684\u590d\u6d3b\u70b9\u65c1\u8fb9\u5c31\u80fd1UP\u52a0\u4eba\u5934\uff0c\u6240\u4ee5\u7b49\u4e8e\u662f\u53ef\u65e0\u6570\u6b21\u6311\u6218\uff0c\u65e0\u9700 \u518d\u6b21\u8d70\u5751\u7239 \u8def\u3002\u5982\u4ee5\u5f80\u7684\u8d85\u7ea7\u739b\u4e3d\uff0cboss\u638c\u63e1\u8981\u70b9\u540e\u5f88\u7b80\u5355\u3002\u590d\u523b\u7248\u8bd5\u56fe\u53d1\u6325 NDS\u89e6\u6478\u5c4f\u7684\u4f5c \u7528\uff0c\u53ef\u60dc\u5f88\u5931\u8d25\uff0c\u6e38\u620f\u4e2d\u591a\u6b21\u8d34\u5fc3\u63d0\u793a\u70b9\u89e6\u6478\u5c4f\u79fb\u52a8\u4eba\u7269\u66f4\u5e73\u7a33\uff0c \u66f4\u7cbe\u786e\uff0c\u73a9\u8d77 \u6765\u6839\u672c\u4e0d\u90a3\u4e48\u56de\u4e8b\uff0c\u9664\u4e86\u65b9\u5411\u597d\u63a7\u5236\uff0c\u5176\u5b83\u5168\u4e0d\u597d\u63a7\u5236\uff0c\u800c\u4e14\u770b \u4e0d\u8def\u4e2d\u95f4\u7684\u654c \u4eba\uff0c\u969c\u788d\u7269\u3002</p> <p>\u7ec8\u4e8e\u7ed98\u5e74\u8001\u7684dell\u7816\u5934\u7b14\u8bb0\u672c\u627e\u5230\u4e86\u5de5\u4f5c\uff0c\u4e00\u5934\u63a5\u7535\u89c6\uff0c\u4e00\u5934\u63a5\u76d1\u542c\u97f3\u7bb1\uff0c\u64ad \u653e \u6548\u679c\u8fd8\u771f\u50cf\u90a3\u4e48\u56de\u4e8b\u3002</p> <p>\u8bf4\u8d77\u76d1\u542c\u97f3\u7bb1\uff0c\u5fcd\u4e0d\u4f4f\u5938\u4e00\u756aRoland CM 30 cube monitor\uff0c\u4ece\u672a\u7528\u8fc7\u7c7b\u4f3c\u4ea7\u54c1 \u6240 \u4ee5\u5f88\u5f53\u4e2a\u5b9d\u3002\u8fd9\u4e2a\u76d1\u542c\u65b9\u5757\u5f88\u6709\u8da3\u7684\u3002\u4e24\u4e2a\u53cc\u58f0\u9053\u8f93\u5165\uff0c\u4e00\u4e2a\u9ea6\u514b\u98ce\u8f93\u5165\uff0c \u4e00\u4e2a \u97f3\u9891\u8f93\u5165\u3002\u9664\u4e86\u6709\u5168\u5c40\u97f3\u91cf\uff0c\u524d\u4e09\u4e2a\u8f93\u5165\u90fd\u6709\u5355\u72ec\u7684\u97f3\u91cf\u65cb\u94ae\u3002\u7acb\u4f53\u58f0\u53ef \u4ee5\u8f93\u51fa \u5230\u8033\u673a\uff0c\u4e5f\u53ef\u4ee5\u8f93\u51fa\u5230\u66f4\u5927\u529f\u7387\u7684\u5916\u63a5\u97f3\u7bb1\u3002\u540c\u65f6\u8fd8\u6709equalizer\u3002\u4e0d\u8fc7 \u6211\u6839\u672c\u542c \u4e0d\u51fa\u5565\u662f\u7acb\u4f53\u58f0\u5565\u4e0d\u662f\uff0c\u6240\u4ee5\u81f3\u4eca\u4e0d\u77e5\u9053\u5b83\u81ea\u8eab\u97f3\u7bb1\u653e\u51fa\u6765\u7684\u662f\u5565\u3002 \u5982\u679c\u641e\u4e24 \u4e2a\u65b9\u5757\u8fde\u4e00\u5757\uff0c\u8fd8\u6709\u66f4\u591a\u73a9\u6cd5\u3002</p> <p>\u6211\u5012\u662f\u7528\u4e0d\u5230\u90a3\u4e48\u590d\u6742\u7684\u529f\u80fd\uff0c\u6211\u53ea\u7528\u6765\u8f93\u5165\u9f13\u548c\u80cc\u666f\u97f3\u4e50\uff0c\u53ef\u4ee5\u5355\u72ec\u8c03\u97f3\u91cf\u975e \u5e38\u65b9\u4fbf\u3002\u9664\u6b64\u4e4b\u5916\u6700\u5927\u7684\u6536\u83b7\u662f\u660e\u767d\u597d\u97f3\u7bb1\u662f\u53ef\u4ee5\u6e05\u695a\u542c\u5230\u4f4e\u97f3\u7684\u3002\u56e0\u4e3a\u4ece\u6765\u90fd \u53ea\u7528\u6807\u914d\u4f4e\u7aef\u97f3\u7bb1/\u8033\u673a\uff0c\u6240\u4ee5\u4e00\u76f4\u4e0d\u77e5\u9053\u53ef\u4ee5\u90a3\u4e48\u6e05\u695a\u7684\u542c\u5230\u8d1d\u53f8\uff0c\u628a\u97f3\u4e50\u62ff \u6765 \u91cd\u65b0\u542c\uff0c\u6709\u5f88\u591a\u65b0\u53d1\u73b0\uff08\u5fc3\u7406\u4f5c\u7528\uff1f\uff09\u3002\u662f\u6211\u8fd9\u4e2a\u4f4e\u97f3\u63a7\u7684\u798f\u97f3\u554a\u3002</p>"},{"location":"writing/2012/#jan-11-2012","title":"Jan 11, 2012","text":"<p>\u9f13\u624b\u5404\u79cd\u5c71\u5be8\u7ec3\u529f\u65b9\u6cd5\uff0c\u6709\u6ca1\u6709\u6548\u53e6\u8bf4\uff0c\u53cd\u6b63\u95f2\u7740\u4e5f\u662f\u95f2\u7740\uff1a</p> <ul> <li>\u770b\u7535\u8111\u7684\u65f6\u5019\u70b9\u5730\u677f\u7ec3\u811a\u8155</li> <li>\u7b14\u80fd\u6572\u5565\u6572\u5565\uff0c\u4e3b\u8981\u7ec3\u5373\u5174\uff0c\u8282\u594f\u611f\uff0c\u5bf9\u6280\u672f\u5012\u662f\u6ca1\u5e2e\u52a9</li> <li>\u624b\u62cd\u5927\u817f\uff0c\u53cc\u811a\u8dfa\u5730\u7ec3\u56db\u80a2\u72ec\u7acb</li> <li>\u770b\u7247\u7684\u65f6\u5019\u9f13\u69cc\u6572\u6bdb\u5dfe</li> <li>\u6bcf\u5929\u5403\u65e9\u9910\u65f6\u548c\u7761\u524d\u90fd\u8ba4\u771f\u542c\u4e00\u9996\u6b4c\uff0c\u8bad\u7ec3\u4e50\u611f\u3002</li> </ul> <p>\u4e2d\u5348\u542c\u7ba1\u996d\u7684seminar\uff0c\u4e2d\u4e1c\u83dc\u91cc\u53d1\u73b0\u4e86\u864e\u76ae\u9752\u6912\uff0c\u7a7f\u8d8a\u4e86\u3002</p>"},{"location":"writing/2012/#feb-22-2012","title":"Feb 22, 2012","text":"<p>\u4ece\u56fd\u5185\u56de\u6765\u4e86\uff0c\u518d\u6b21\u89c9\u5f97\u8fd9\u91cc\u624d\u662f\u5bb6\uff0c\u56e0\u4e3a\u81ea\u5df1\u662f\u4e3b\u4eba\u3002</p>"},{"location":"writing/2012/#mar-1-2012","title":"Mar 1, 2012","text":"<p>\u6cd5\u56fd\u535a\u540e\u6765\u4e86\uff0c\u6709\u70b9geek\u6240\u89c1\u7565\u540c\u7684\u611f\u89c9\u3002\u6709\u5929\u5230\u53e6\u5916\u4e00\u680b\u697c\u505a\u5b9e\u9a8c\uff0c\u74f6\u74f6\u7f50\u7f50 \u7684\u8981\u5e26\u8fc7\u53bb\uff0c\u6211\u8bf4\u6211\u5e2e\u4f60\u62ff\u70b9\u597d\u4e86\uff0c\u4ed6\u8bf4\u4e0d\u7528\uff0c\u7136\u540e\u628a\u51e0\u74f6\u8bd5\u5242\u5f80\u4e66\u5305\u91cc\u4e00\u80cc\u5c31 \u51fa\u53d1\u3002\u6211\u8fd8\u7b2c\u4e00\u6b21\u89c1\u4eba\u7528\u4e66\u5305\u80cc\u8bd5\u5242\u3002</p> <p>\u4eca\u5929\u6280\u672f\u5458\u8bf4\uff0c\u5927\u90e8\u5206phd\u90fd\u6ca1\u6709\u610f\u8bc6\u5230\u81ea\u5df1\u548c\u6b63\u5e38\u4eba\u6709\u591a\u4e48\u7684\u4e0d\u540c\uff0c\u6211\u4eec\u60f3\u4e86 \u60f3\uff0c\u89c9\u5f97\u5979\u5f88\u5bf9\u3002</p> <p>\u4e70\u4e86\u76c6\u864e\u5c3e\u5170\u653e\u5367\u5ba4\u91cc\uff0c\u636e\u8bf4\u51c0\u5316\u7a7a\u6c14\u751f\u4ea7\u6c27\u6c14\u3002\u4e0d\u7ba1\u662f\u771f\u6548\u8fd8\u662f\u5fc3\u91cc\u4f5c\u7528\uff0c\u89c9 \u5f97\u786e\u5b9e\u4e0d\u9519\uff0c\u800c\u4e14\u597d\u4fbf\u5b9c\u554a\u3002\u3002</p> <p>ebay\u4e0a\u627e\u5230\u4e86\u4e0d\u9519\u7684\u4e8c\u624bRoland TD9KX2\uff0c\u770b\u4e86\u51e0\u5929\uff0c\u89c9\u5f97\u4e0d\u5fcd\u5fc3\u4e0d\u4e70\uff0c\u4e5f\u8be5\u9e1f \u67aa \u6362\u70ae\u628a\u5bb6\u91cc\u7684\u96c5\u5417\u54c8\u7535\u9f13\u627e\u4e2a\u4e0b\u4e00\u4efb\u4e86\uff0c\u4e8e\u662f\u6fc0\u52a8\u7684\u4e0b\u5355\u3002</p>"},{"location":"writing/2012/#mar-2-2012","title":"Mar 2, 2012","text":"<p>\u4eca\u5e74\u9762\u8bd5\u7684\u5b66\u751f\u91cc\uff0c\u6709\u4e2a\u4e2a\u4eba\u9648\u8ff0\u5927\u6982\u7528\u6a21\u677f\u5fd8\u8bb0\u6539\u6821\u540d\u4e86\uff0c\u5199\u4ed6\u60f3\u53bb\u54c8\u4f5b\u3002\u7cfb \u91cc\u5012\u4e5f\u6ca1\u5f53\u592a\u5927\u7684\u4e8b\uff0c\u53ea\u662f\u7b11\u8bdd\u4f20\u5f00\u800c\u5df2\uff0c\u4eba\u8fd8\u662f\u7167\u6837\u9762\u8bd5\u7684\u3002</p> <p>\u5b89\u88c5TD9KX2\u641e\u4e86\u51e0\u4e2a\u5c0f\u65f6\uff0c\u7531\u4e8e\u623f\u95f4\u6696\u6c14\u8db3\uff0c\u4e00\u8eab\u6c57\u3002\u603b\u7b97\u628a\u90e8\u4ef6\u4e0a\u597d\uff0c\u4f4d\u7f6e\u8c03 \u6574\u6210\u9002\u5408\u4f4e\u6d77\u62d4\u4eba\u58eb\u7684\u9ad8\u5ea6\u3002\u628a\u6240\u6709\u7684\u9f13\u76ae\u90fd\u677e\u4e86\u4e00\u4e0b\uff0c\u628a\u5f39\u6027\u51cf\u5c0f\u5230\u8ddf acoustic\u9f13\u4e00\u6837\uff0c\u518d\u628a\u5404\u79cd\u89e6\u53d1\u5668\u5fae\u8c03\u4e86\u4e00\u4e0b\u3002\u4e0d\u6127\u662fRoland\uff0c\u81ea\u7531\u5ea6\u5f88\u9ad8\u3002</p>"},{"location":"writing/2012/#mar-18-2012","title":"Mar 18, 2012","text":"<p>\u8c01\u8bf4\u5728Craiglist\u4e0a\u5356\u4e1c\u897f\u5feb\uff0c\u6211\u6302\u7684\u6e38\u620f\u673a\u5e7f\u544a\u51e0\u5929\u4e86\u90fd\u6ca1\u4eba\u7406\u3002\u5356\u4e1c\u897f\u771f\u662f\u591f\u6298\u817e\u3002</p> <p>\u6628\u5929\u5e26\u6cd5\u56fd\u535a\u540e\u8def\u8fc7\u9ea6\u5f53\u52b3\uff0c\u56e0\u4e3a\u4ed6\u5f88\u8654\u8bda\u7684\u8bf4\u4e00\u5b9a\u8981\u627e\u65f6\u95f4\u53bb\u4f53\u4f1a\u4f53\u4f1a\u8fd9\u4ee3\u8868 \u7740\u7f8e\u56fd\u7684\u5feb\u9910\u5e97\u3002\u6211\u63a5\u8bdd\u8bf4\u5f53\u9ea6\u5f53\u52b3\u5f15\u5165\u56fd\u5185\u65f6\uff0c\u4e5f\u7ed9\u6211\u7559\u4e0b\u4e86\u5bf9\u7f8e\u56fd\u7684\u7b2c\u4e00\u5370 \u8c61\u2013\u8d35\uff0c\u4e70\u4e0d\u8d77\u3002\u7f8e\u56fd\u540c\u5b66\u7eb7\u7eb7\u8868\u793a\u9707\u60ca\uff0c\u6ca1\u60f3\u5230\u8fd9\u8c61\u5f81\u7740\u4fbf\u5b9c\u5783\u573e\u98df\u54c1\u7684\u5feb\u9910 \u5e97\u5728\u6d77\u5916\u5f00\u82b1\u5f00\u7684\u90fd\u4eae\u778e\u72d7\u773c\u4e86\u3002\u5f53\u7136\u6211\u4e5f\u9707\u60ca\u8fd9\u4f4d\u6cd5\u56fd\u8001\u5144\u4f1a\u8fd9\u4e48\u7ed9\u5b83\u9762\u5b50\u3002</p> <p>\u751f\u6d3b\u91cc\u5356\u4e1c\u897f\u8ddf\u6e38\u620f\u91cc\u5356\u4e00\u6837\uff0c\u5356\u4e0d\u4e86\u51e0\u4e2a\u94b1\uff0c\u5356\u7684\u76ee\u7684\u4e5f\u4e0d\u662f\u8d5a\u94b1\uff0c\u800c\u662f\u56e0\u4e3a \u9053\u5177\u888b\u88c5\u6ee1\u4e86\u8981\u817e\u7a7a</p>"},{"location":"writing/2012/#mar-22-2012","title":"Mar 22, 2012","text":"<p>\u4e2d\u5956\u4e86\uff0c\u6536\u5230\u6cd5\u9662\u4f20\u966a\u5ba1\u5458\u7684\u90ae\u4ef6\u3002\u6253\u7535\u8bdd\u8fc7\u53bb\u8bf4\u54b1\u4e0d\u662f\u516c\u6c11\u4e0d\u5408\u683c\uff0c\u4eba\u8981\u6211\u62a4 \u7167\u4f20\u771f\u8fc7\u53bb\u8bc1\u660e\u6211\u4e0d\u662f\u516c\u6c11\u3002nnd\u516c\u6c11\u7684\u798f\u5229\u4ece\u6765\u6ca1\u4eab\u53d7\u8fc7\uff0c\u5c31\u4ea4\u7a0e\u65f6\u88ab\u5f53\u4f5c\u516c \u6c11\uff0c\u73b0\u5728\u8ba9\u6211\u5c65\u884c\u516c\u6c11\u4e49\u52a1\uff0c\u8fd8\u5f97\u8d39\u4e8b\u53bb\u4f20\u771f\uff0c\u53d1\u90ae\u4ef6\u90fd\u4e0d\u884c\u3002</p> <p>\u6cd5\u56fd\u535a\u540e\u5bf9\u65b0\u73af\u5883\u9002\u5e94\u7684\u5f88\u5feb\uff0c\u8fd8\u52a0\u5165\u4e86\u6c34\u7403\u961f\uff0c\u73a9\u7684\u4e0d\u4ea6\u4e50\u4e4e\u3002\u6781\u5176\u7fa1\u6155\u793e\u4ea4 \u65e0\u969c\u788d\u4eba\u58eb\u3002</p> <p>\u6709\u7684\u4eba\u957f\u5f97\u5f88\u7279\u522b\uff0c\u6837\u8c8c\u5982\u540c\u9650\u91cf\u7248\uff0c\u770b\u8fc7\u4e00\u773c\u5c31\u7edd\u5bf9\u4e0d\u4f1a\u8ba4\u9519\u3002\u800c\u6709\u7684\u4eba\u957f\u7740 \u526f\u6279\u91cf\u751f\u4ea7\u7684\u8138\uff0c\u8d70\u5927\u8857\u4e0a\u80fd\u770b\u5230\u5404\u79cd\u76f8\u4f3c\uff0c\u80fd\u770b\u51fa\u540c\u6837\u7684\u6a21\u677f\u3002\u800c\u6211\u662f\u5178\u578b\u7684 \u5927\u4f17\u8138\uff0c\u4e0d\u4f46\u662f\u6279\u91cf\u751f\u4ea7\u7ebf\u4e0a\u7684\u4e00\u5458\uff0c\u800c\u4e14\u8fd8\u662f\u539f\u578b\u3002\u4f5c\u4e3a\u60b2\u50ac\u7684\u57fa\u672c\u6b3e\uff0c\u6211\u542c \u7684\u6700\u591a\u7684\u5c31\u662f\uff1a\u54ce\u5440\u6211\u662f\u4e0d\u662f\u89c1\u8fc7\u4f60\uff1b\u6211\u4eec\u662f\u6821\u53cb\u5427\uff0c\u4e0a\u8bfe\u89c1\u8fc7\u4f60\uff1b\u4f60\u53bb\u5f00\u8fc7XX \u4f1a\u5427\uff0c\u6211\u4e5f\u5728\uff1b\u4f60\u597d\u50cf\u6211\u4e00\u4e2a\u5c0f\u5b66\u540c\u5b66\uff1b\u7b49\u7b49\u3002\u6211\u5fc3\u4e2d\u5410\u69fd\u8bf4\uff0c\u5f53\u7136\u4e86\uff0c\u90a3\u4e9b\u53ef \u90fd\u662f\u6211\u7684\u5347\u7ea7\u7248\uff0c\u6539\u826f\u7248\uff0c\u7eaa\u5ff5\u7248\u554a\u7b49\u7b49\u3002\u524d\u4e16\u5f97\u8dd1\u591a\u5c11\u4e2a\u9f99\u5957\uff0c\u624d\u80fd\u4fee\u6210\u4e00\u5f20 \u5927\u4f17\u8138\u3002</p> <p>\u6628\u5929\u505a\u4e86\u4e2a\u68a6\uff0c\u574f\u4eba\u5165\u4fb5\u4e86\uff0c\u628a\u5730\u7403\u4eba\u6293\u8d77\u6765\u5173\u623f\u95f4\u91cc\u505a\u8003\u5377\u3002\u4e00\u95f4\u5173\u4e24\u4e2a\u4eba\uff0c \u540c\u6837\u7684\u8003\u5377\u51b3\u6597\uff0c\u8c01\u5206\u9ad8\u8c01\u5c31\u80fd\u6d3b\u4e0b\u6765\u3002\u6211\u548c\u522b\u4eba\u4e00\u95f4\uff0c\u7075\u673a\u4e00\u52a8\uff0c\u4e92\u76f8\u6284\uff0c\u8fd9 \u6837\u4e24\u4eba\u5e76\u6392\u7b2c\u4e00\uff0c\u5c31\u90fd\u80fd\u6d3b\u4e86\uff0c\u53cd\u6b63\u4e5f\u6ca1\u8bf4\u4e0d\u80fd\u4f5c\u5f0a\u3002\u574f\u4eba\u5f88\u65e0\u5948\u3002\u540e\u6765\u600e\u4e48\u6837 \u5c31\u4e0d\u77e5\u9053\u4e86\u3002</p>"},{"location":"writing/2012/#apr-2-2012","title":"Apr 2, 2012","text":"<p>\u7325\u7410\u81f3\u6781\u662f\u6709\u6076\u679c\u7684\u3002\u53bb\u90ca\u533a\u627e\u670b\u53cb\uff0c\u4e3a\u4e86\u51e0\u4e2a\u6cb9\u94b1\uff0c\u4e00\u76f4\u6491\u7740\u4e0d\u5728\u5e02\u533a\u52a0\u6cb9\uff0c \u60f3\u5728\u5373\u5c06\u79bb\u5f00\u90ca\u533a\u65f6\u52a0\u6ee1\u4e86\u56de\u53bb\u3002\u56de\u53bb\u7684\u8def\u4e0a\u5728\u79bb\u52a0\u6cb9\u7ad9\u4e00\u767e\u7c73\u5904\u7184\u706b\u4e86\uff0c\u8fd9\u624d \u60f3\u8d77\u8f66\u7684\u6cb9\u6cf5\u574f\u8fc7\uff0c\u6240\u4ee5\u4f4e\u6cb9\u7184\u706b\u4e5f\u4e0d\u5947\u602a\uff0c\u4e0d\u7528\u7b49\u5230\u6cb9\u7528\u5149\u3002\u5bd2\u98ce\u4e2d\u4e09\u4eba\u5750\u5728 \u8f66\u91cc\u53d1\u6296\u7b49AAA\u9001\u6cb9\u3002\u4ee5\u540e\u4e0d\u6562\u4e86\u3002</p>"},{"location":"writing/2012/#may-22-2012","title":"May 22, 2012","text":"<p>\u4ee5\u524d\u5934\u75bc\u6212\u4e0d\u6389\u6e38\u620f\uff0c\u73b0\u5728\u5934\u75bc\u6839\u672c\u6324\u4e0d\u51fa\u65f6\u95f4\u73a9\u6e38\u620f\uff0c\u6709\u7a7a\u95f2\u7684\u65f6\u5019\u5c31\u60f3\u61d2 \u7740\uff0c\u90fd\u61d2\u5f97\u53bb\u5f00\u6e38\u620f\u673a\u3002Skyrim\u4e3b\u7ebf\u5c31\u6ca1\u600e\u4e48\u52a8\uff0c\u652f\u7ebf\u61d2\u61d2\u6563\u6563\u7684\u505a\u4e86\u4e00\u4e9b\uff0c\u4e3b \u8981\u662f\u7f34\u532a\uff0c\u7136\u540e\u6e38\u620f\u6401\u7f6e\u4e86\u5feb\u4e00\u4e2a\u6708\u4e5f\u6ca1\u6709\u91cd\u5927\u8fdb\u5ea6\u3002</p> <p>\u5b66\u9f13\u7684\u6001\u5ea6\u5230\u4e86\u4e2a\u74f6\u9888\uff0c\u6ca1\u4e8b\u5e72\u7684\u65f6\u5019\u4f1a\u7528\u624b\u5230\u5904\u6572\u6572\uff0c\u6ca1\u4e8b\u60f3\u7684\u65f6\u5019\u4f1a\u60f3\u8c61\u6253 \u9f13\uff0c\u4f46\u5c31\u662f\u9f13\u69cc\u62ff\u4e0d\u5230\u624b\u4e0a\u3002\u6c34\u5e73\u4e24\u7ea7\u5206\u5316\uff0c\u505a\u7684\u597d\u7684\u65b9\u9762\u8d8a\u7ec3\u8d8a\u987a\u624b\uff0c\u505a\u7684\u4e0d \u597d\u7684\u65b9\u9762\u5c31\u662f\u72e0\u4e0d\u4e0b\u5fc3\u6765\u731b\u653b\u3002\u56db\u80a2\u72ec\u7acb\u81ea\u4ee5\u4e3a\u505a\u7684\u5f88\u4e0d\u9519\uff0c\u7ed9\u4e2a\u65b0\u8c31\u5b50\u8bfb\u4e0a\u51e0 \u904d\u5c31\u80fd\u6572\u4e0b\u6765\u56db\u80a2\u4e0d\u6253\u67b6\u3002\u53f3\u817f\u62cd\u5b50\u4e0d\u51c6\u7684\u95ee\u9898\u5c1a\u5728\u8c03\u67e5\u4e2d\uff0c\u4e0d\u77e5\u9053\u662f\u7535\u9f13\u89e6\u53d1 \u5668\u7684\u95ee\u9898\uff0c\u8fd8\u662f\u6211\u4e0d\u9002\u5e94\u73b0\u5728\u7684\u8e0f\u677f\uff0c\u4e0b\u6b21\u4e0a\u539f\u58f0\u9f13\u7684\u65f6\u5019\u518d\u7814\u7a76\u4e0b\u3002\u901f\u5ea6\u95ee\u9898 \u8001\u65e9\u5c31\u505c\u6b62\u8fdb\u6b65\u4e86\uff0c\u4ee5\u6211\u8fd9\u61d2\u52b2\uff0c\u4f30\u8ba1\u5f88\u957f\u65f6\u95f4\u90fd\u53ea\u80fd\u641e\u6162\u8282\u594f\u7684\u4e1c\u897f\uff0c\u72ec\u594f\u5168 \u9760\u7235\u58eb\u98ce\u683c\u3002</p> <p>\u9e21\u808b\u5c0f\u7b14\u8bb0\u672c\u7535\u8111\u5356\u6389\u4e86\uff0c\u5b9a\u4ef7\u8fc7\u4f4e\uff0c\u521a\u53d1\u5e16\u4e00\u5929\u5c31\u6536\u5230\u5feb5\u5c01\u90ae\u4ef6\uff0c\u6700\u540e\u5356\u7ed9 \u4e86 \u7b2c\u4e00\u4f4d\u4e70\u4e3b\u3002\u4e70\u4e3b\u662f\u5b66\u653f\u6cbb\u7684\uff0c\u61c2linux\uff0c\u8c8c\u4f3c\u505a\u6a21\u62df\u8ba1\u7b97\uff0c\u53ef\u60dc\u6ca1\u804a\u804a\u4ed6\u8bfe \u9898\u662f \u54ea\u65b9\u9762\u3002</p> <p>\u9e21\u808b\u5927\u7b14\u8bb0\u672c\u4e0d\u4e45\u524d\u7ec8\u4e8e\u53cc\u7cfb\u7edf\u90fd\u88ab\u7528\u574f\u4e86\uff0c\u4e00\u4e2a\u8fdb\u4e0d\u53bb\uff0c\u4e00\u4e2a\u7528\u4e0d\u4e86\u3002\u4eca\u5929\u4e00 \u5927\u65e9\u5751\u5403\u5751\u5403\u88c5\u4e86\u5927\u8def\u8d27Ubuntu\uff0c\u88c5\u5b8c\u4e00\u5f00\u673a\u5c31\u50bb\u773c\u4e86\uff0c\u7cfb\u7edf\u53ea\u7528\u4e86\u663e\u793a\u5668\u5de6\u4e0a \u89d2\uff0c\u5176\u5b83\u5730\u65b9\u90fd\u9ed1\u7684\u3002\u800c\u4e1410\u5206\u949f\u7684\u65f6\u95f4\u90fd\u70eb\u624b\u4e86\uff0c\u4e8e\u662f\u7ec8\u4e8e\u653e\u5f03\uff0c\u6253\u7b97\u6bc1\u4e86\u786c \u76d8\u7535\u8111\u9001\u4eba\u3002</p>"},{"location":"writing/2012/#may-24-2012","title":"May 24, 2012","text":"<p>\u60ca\u559c\u7684\u53d1\u73b0Xsane\u80fd\u8ba4\u6211\u7684\u4f73\u80fd\u626b\u63cf\u4eea\uff0c\u4f46\u8fd9\u4e1c\u897f\u8ddf\u4ee5\u524d\u7684GIMP\u4e00\u4e2a\u6bdb\u75c5\uff0c\u4e00\u4e2a \u8f6f \u4ef6\u641e\u4e09\u4e2a\u72ec\u7acb\u7a97\u53e3\uff0c\u5207\u6362\u8f6f\u4ef6\u65f6\u5f88\u70e6\u4eba\u3002\u626b\u5b8c\u7528GIMP\u505a\u70b9\u7b80\u5355\u7f16\u8f91\uff0c\u975e\u5e38\u4e0d \u987a \u624b\uff0c\u4e8e\u662f\u7ee7\u7eed\u89c9\u5f97\u5728Linux\u4e0b\u5f04\u56fe\u662f\u7a77\u6298\u817e\u3002</p> <p>\u53c2\u8003PS3\u6e38\u620f\u7684\u6210\u5c31\u7cfb\u7edf\uff0c\u628a\u5404\u79cd\u751f\u6d3b\u76ee\u6807\u4e5f\u5206\u4e2a\u91d1\u94f6\u94dc\u5956\u676f\uff0c\u4e00\u4e2a\u4e2a\u7684\u62ff\u3002\u6311 \u6218 \u7684\u7b2c\u4e00\u4e2a\u94dc\u5956\uff0c\u662f\u6253\u9f13\u4e0d\u505c\u606f\u4e00\u5c0f\u65f6\u3002\u5c3c\u5417\u8fd9\u94dc\u724c\u5751\u7239\uff0c\u56e0\u4e3a\u6211\u4e0a\u9f13\u6572\u7ec3\u8c31 \u5b50\u5f88 \u5c11\u4e0d\u8ba1\u65f6\uff0c\u8ba1\u4e86\u624d\u53d1\u73b0\u621130\u5206\u949f\u5c31\u8170\u75bc\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u7ec3shuffle\uff0c\u4e0d\u9002\u5e94\u5e95 \u9f13\u7684\u8282 \u594f\u3002\u7136\u540e\u5c31\u6ca1\u6709\u575a\u6301\uff0c\u4e0b\u6b21\u518d\u6311\u6218\u3002\u73b0\u5728\u5230\u5904\u627eshuffle\u6b4c\uff0c\u5f97\u627e\u8001\u7684\u8001 \u9ed1\u5e03\u9c81\u65af\u3002</p> <p>\u6253\u9f13\u8981\u633a\u76f4\u8170\u4e00\u4e07\u904d\uff0c\u8981\u4e0d\u5c31\u8981\u6210\u95ea\u8170\u4fa0\u4e86\u3002</p>"},{"location":"writing/2012/#may-26-2012","title":"May 26, 2012","text":"<p>\u673a\u7f18\u5de7\u5408\u5728\u7f51\u4e0a\u770b\u4e86\u4e9b\u5173\u4e8e\u6bd2\u54c1\u7684\u4fe1\u606f\uff0c\u89c9\u5f97\u597d\u5947\u5fc3\u771f\u662f\u4e2a\u96be\u4ee5\u638c\u63a7\u7684\u4e1c\u897f\u3002\u6211 \u4e5f\u7b97\u662f\u4e2a\u597d\u5947\u5fc3\u91cd\u7684\u4eba\u4e86\uff0c\u5e78\u597d\u5bf9\u7269\u8d28\u4e0a\u7684\u5c1d\u8bd5\u6ca1\u6709\u5174\u8da3\uff0c\u4e5f\u4e0d\u5927\u5bb9\u5fcd\u4ee5\u635f\u4f24\u8eab \u4f53\u7684\u4ee3\u4ef7\u53d6\u5f97\u5fc3\u7406\u6109\u60a6\u3002\u5f53\u7136\u4e5f\u4e0d\u662f\u5b8c\u5168\u4e0d\u8fd9\u4e48\u505a\uff0c\u53ea\u662f\u6211\u80fd\u63a5\u53d7\u7684\u6781\u9650\u6bd4\u8f83 \u4f4e\u2013\u51b0\u53ef\u4e50\u914d\u65b9\u4fbf\u9762..\u6ca1\u62bd\u8fc7\u70df\u6ca1\u5438\u8fc7\u5927\u9ebb\uff0c\u5012\u7eaf\u7cb9\u662f\u56e0\u4e3a\u89c9\u5f97\u592a\u591a\u5c0f\u670b\u53cb\u7528 \u5438\u70df\u548c\u62bd\u5927\u9ebb\u6765\u8868\u793a\u53db\u9006\uff0c\u53cd\u5012\u5931\u53bb\u53db\u9006\u7684\u610f\u4e49\u4e86\uff0c\u6211\u504f\u4e0d\u78b0\u3002</p>"},{"location":"writing/2012/#may-27-2012","title":"May 27, 2012","text":"<p>\u5230\u5dde\u516c\u56ed\u73a9\u4e86\u4e00\u8d9f\uff0c\u8111\u5b50\u62bd\u7b4b\u60f3\u7740\u53ef\u4ee5\u8e29\u6c34\u6240\u4ee5\u4f20\u4e86\u51c9\u978b(\u6700\u540e\u6ca1\u8e29\u6c34\uff0c\u53ea\u8fdb\u4e86 \u4e00 \u5806\u6c99\u5b50)\uff0c\u60b2\u50ac\u7684\u8d70\u4e86\u4e24\u4e2a\u5c0f\u65f6\u4e0d\u5230\u5c31\u89c9\u5f97\u811a\u4e0a\u6709\u6c34\u6ce1\u611f\u3002\u672c\u7740\u65e0\u77e5\u662f\u798f\u7684\u539f \u5219\uff0c \u575a\u51b3\u4e0d\u67e5\u770b\uff0c\u786c\u8d70\u3002\u53c8\u8fc7\u5927\u7ea6\u4e09\u4e2a\u5c0f\u65f6\u5019\u8d70\u5b8c\u56de\u5bb6\uff0c\u5230\u5bb6\u4e00\u770b\u679c\u7136\u8d77\u4e86\u4e09 \u4e2a\u6c34 \u6ce1\uff0c\u679c\u7136\u770b\u4e86\u4e4b\u540e\u7a7f\u62d6\u978b\u90fd\u5acc\u75bc\u3002</p> <p>\u6211\u6709\u65f6\u5019\u4f1a\u62c5\u5fc3\u4e9b\u522b\u4eba\u4e0d\u4f1a\u62c5\u5fc3\uff0c\u5b8c\u5168\u65e0\u5173\u7d27\u8981\u7684\u4e8b\u60c5\u3002\u9ad8\u4e09\u8003\u8bed\u6587\uff0c\u6211\u505a\u5230\u6700 \u540e\u7684\u4f5c\u6587\u65f6\uff0c\u53d1\u73b0\u5377\u5b50\u4e0a\u5199\u8981\u7528\u9ed1\u7b14\u4f5c\u7b54\uff0c\u53ef\u6211\u7528\u7684\u662f\u84dd\u7b14\u3002\u5fc3\u4e00\u51c9\uff0c\u54ac\u7259\u7ffb\u51fa \u9ed1\u7b14\u628a\u6240\u6709\u7684\u7b54\u6848\u63cf\u4e00\u904d\u3002\u540e\u6765\u8bb2\u8bd5\u5377\u7684\u65f6\u5019\uff0c\u8001\u5e08\u7528\u6295\u5f71\u4eea\u793a\u8303\u4e00\u4e9b\u597d\u7b54\u6848/ \u574f \u7b54\u6848\uff0c\u6211\u975e\u5e38\u714e\u71ac\uff0c\u6015\u88ab\u770b\u51fa\u6211\u7528\u9ed1\u7b14\u63cf\u84dd\u7b14\u3002\u90fd\u5fd8\u8bb0\u6211\u662f\u5f53\u6b63\u9762\u8fd8\u662f\u8d1f\u9762 \u6559\u6750 \u4e86\uff0c\u6211\u53ea\u8bb0\u5f97\uff0c\u6ca1\u4eba\u53bb\u770b\u6211\u662f\u7528\u4ec0\u4e48\u7b14\u5199\u7684\u2013\u800c\u8fd9\u5c31\u591f\u4e86\u3002</p> <p>\u8def\u8fb9\u89c1\u5230\u4e00\u7f8e\u5973\u62c9\u5c0f\u63d0\u7434\u5356\u827a\uff0c\u60ca\u8273\u3002\u96be\u5f97\u89c1\u7f8e\u5973\u5356\u827a\uff0c\u5f53\u7136\u8981\u7ed9\u5c0f\u8d39\u3002\u624b\u4e2d\u7684 5\u5143\u634f\u4e86\u5f88\u4e45\u8fd8\u662f\u820d\u4e0d\u5f97\uff0c\u53bb\u65c1\u8fb9\u5c0f\u5e97\u6362\u96f6\uff0c\u51e0\u5206\u949f\u7684\u65f6\u95f4\uff0c\u51fa\u6765\u7f8e\u5973\u5c31\u6536\u5de5\u4e0d \u89c1 \u4e86\u3002\u65f6\u673a\u4e0d\u7b49\u4eba\u554a\u3002\u4e0d\u77e5\u4e3a\u5565\u60f3\u8d77\u72fc\u548c\u810f\u5c0f\u5b69\u7684\u6545\u4e8b\uff0c\u72fc\u6349\u4e86\u5c0f\u5b69\u5acc\u810f\uff0c\u56de \u5bb6\u70e7 \u597d\u6d17\u6fa1\u6c34\u56de\u6765\uff0c\u5c0f\u5b69\u5c31\u5e26\u63f4\u5175\u6765\u6293\u72fc\u4e86\u3002</p>"},{"location":"writing/2012/#may-28-2012","title":"May 28, 2012","text":"<p>\u73a9skyrim\u8d70\u5f13\u7bad\u5c0f\u6df7\u6df7\u8def\u7ebf\uff0c\u641e\u6f5c\u5165\uff0c\u6697\u6740\uff0c\u5bfb\u5b9d\u4e4b\u7c7b\u75db\u5feb\u3002\u6500\u4e86\u4e2a\u5f00\u5e97\u7684\u77ff\u5de5 \u5973\u7ed3\u5a5a\uff0c\u56e0\u4e3a\u6709\u94b1\u62ff\uff0c\u8fd8\u53ef\u4ee5\u5230\u5979\u7684\u5e97\u91cc\u5356\u4e1c\u897f\u3002\u641e\u5927\u573a\u9762\u62ef\u6551\u4eba\u7c7b\u5c31\u4e0d\u884c\u4e86\uff0c \u4e00\u628a\u5c0f\u5200\u780d\u4e0d\u52a8\uff0c\u66f4\u4e0d\u7528\u8bf4\u6740\u9f99\u5f97\u62ff\u5f13\u7bad\u9f9f\u901f\u7684\u78e8\u8840\u3002\u4e8e\u662f\u63a8\u4e0d\u52a8\u4e3b\u7ebf\uff0c\u641e\u652f\u7ebf \u641e\u592a\u4e45\u4e5f\u89c9\u5f97\u6ca1\u610f\u601d\uff0c\u5c31\u8fd9\u4e48\u653e\u5f03\u4e86\u3002\u4eba\u7b97\u4e0d\u5982\u5929\u7b97\uff0c\u8fd9\u4e2a\u6e38\u620f\u524d\u671f\u611f\u60c5\u6295\u5165\u8fd8 \u86ee\u591a\u3002</p>"},{"location":"writing/2012/#jun-11-2012","title":"Jun 11, 2012","text":"<p>\u4e3a\u4e869\u6708\u4efd\u76841600\u7c73\u7684\u4e24\u4e2a\u5b9e\u9a8c\u5ba4\u7fa4\u4f53\u8d5b\uff0c\u5f00\u59cb\u8bad\u7ec3\u8dd1\u6b65\u3002</p> <p>\u6536\u5230\u4e13\u95e8\u7528\u6765\u5f55\u97f3\uff0c\u97f3\u9891\u7f16\u8f91\u7684\u7535\u8111\uff0c\u624b\u8d31\u7b2c\u4e00\u4ef6\u4e8b\u5c31\u662f\u53bb\u88c5linux\u3002archlinux \u9996\u5148\u4e0d\u80fd\u8ba4usb\u76d8\uff0c\u653e\u5f03\uff1b\u7136\u540edebian unstable\u3002\u53d1\u73b0\u8054\u60f3\u7ed9\u88c5\u7684windows 7\uff0c \u4e00\u4e0b\u5c31\u6574\u4e86\u4e09\u4e2a\u4e3b\u5206\u533a\uff0c\u6709\u70b9\u9738\u9053\u3002\u6298\u817e\u534a\u4e2a\u4e0b\u5348\u90fd\u4e0d\u987a\u5229\uff0c\u4e2d\u6587\u8f93\u5165\u7adf\u7136\u6dfb\u52a0 \u4e0d\u4e0a\u8f93\u5165\u65b9\u5f0f\uff0c\u6ca1\u6709\u62fc\u97f3\u53ea\u80fd\u5e72\u77aa\u773c\uff0cgnome\u7ec8\u7aef\u53ea\u5728gnome\u4e0b\u6709\u7528\uff0c\u6362\u5230 openbox\u6216xfce\u4e0b\u9762\u5c31\u53ea\u6709\u5757\u5927\u9ed1\u677f\uff0c\u6253\u5b57\u6ca1\u53cd\u5e94\u3002\u7528gnome\u5427\uff0c\u65b0\u7684\u7528\u6237\u754c\u9762\u6bd4 Unity\u8fd8\u6076\u5fc3\u3002\u4e0a\u7f51\u53ea\u6709\u90e8\u5206\u7f51\u7ad9\u53ef\u4ee5\u4e0a\uff0c\u53e6\u4e00\u90e8\u5206\u4e0d\u77e5\u9053\u600e\u4e48\u56de\u4e8b\uff0c\u53ef\u80fd\u662f\u57df \u540d\u89e3\u6790\u7684\u95ee\u9898\u3002\u603b\u4e4b\uff0c\u6839\u672c\u6ca1\u6cd5\u7528\u3002\u653e\u5f03\u4e86\uff0c\u56de\u53bb\u4e56\u4e56\u7528\u636e\u8bf4\u5f88\u4e0d\u9519\u7684windows 7\u3002\u641elinux\u8fd8\u662f\u5f97\u8d81\u5e74\u8f7b\uff0c\u5f97\u4e0a\u53f0\u5f0f\u673a\uff0c\u9f20\u6807\u952e\u76d8ipad\u4e00\u5b57\u6392\u5f00\uff0c\u5bf9\u7740\u5927\u5c4f\u5e55\uff0c \u4e00\u526f\u505a\u624b\u672f\u7684\u67b6\u52bf\u6162\u6162\u6765\u3002\u5c0f\u5c4f\u5e55\u7684\u7b14\u8bb0\u672c\u592a\u618b\u5c48\u4e86\u3002</p> <p>\u97f3\u9891\u7ebf\u771f\u5751\u7239\u3002\u5f55\u97f3\u5c0f\u7535\u8111\u7684\u63a5\u53e3\u662f\u5e26\u9ea6\u514b\u98ce\u7684\u8033\u673a\u63a5\u53e3\uff0c\u5149\u63a5\u4e2a\u97f3\u9891\u8f93\u5165\u7ebf\u5b83 \u8fd8\u4e0d\u8ba4\uff0c\u5bb6\u91cc\u6742\u4e03\u6742\u516b\u7684\u8f6c\u6362\u63a5\u5934\u90fd\u8bd5\u4e86\u4e00\u904d\uff0c\u4e0d\u540c\u7684\u8f93\u51fa\u53e3\u4e5f\u8bd5\u4e86\uff0c\u6ee1\u5730\u7ebf\u6ee1 \u8eab\u6c57\u4e5f\u6ca1\u6210\u529f\u3002\u4e8e\u662f\u5f97\u641eMIDI\u8f93\u5165\u518d\u8f6f\u97f3\u6e90\uff0c\u5fc3\u60f3\u597d\u5427\uff0c\u53cd\u6b63\u7f57\u5170\u7684\u97f3\u6e90\u6211\u4e5f\u4e0d \u662f\u592a\u559c\u6b22\u3002\u63a5\u4e0b\u6765\u7684\u95ee\u9898\u662fCooledit\u4e0d\u80fd\u641eMIDI\u8f93\u5165\uff0c\u4e0b\u4e86\u4e2aCubase\uff0c\u592a\u590d\u6742\u4e86 \u8fd8\u4e0d\u4f1a\u7528\u3002\u4eca\u5929\u5f7b\u5e95\u6b47\u83dc\u3002\u9760\uff0c\u81f3\u4e8e\u4e48\uff0c\u6211\u5c31\u662f\u60f3\u7ed9\u5c0f\u53ee\u5f53\u914d\u4e2a\u7235\u58eb\u98ce\u683c\u7684\u9f13\u800c \u5df2\u3002</p> <p>\u65b0\u8fdb\u5c55\uff0cwindows 7\u4e0d\u8ba4\u6211\u7684MIDI\u7ebf\uff0clinux\u5c45\u7136\u8ba4\uff0c\u4e8e\u662f\u6211\u7684debian\u5565\u4e5f\u4e0d\u80fd\u5e72 \u5374\u5c45\u7136\u80fd\u5f55MIDI\uff0c\u7528rosegarden\u3002</p>"},{"location":"writing/2012/#jun-13-2012","title":"Jun 13, 2012","text":"<p>\u4e0b\u5348\u56de\u5bb6\uff0c\u6b63\u5728\u4e3a\u5403\u4ec0\u4e48\u5934\u75bc\uff0c\u6478\u6478\u809a\u5b50\uff0c\u7a81\u7136\u60f3\u8d77\u521a\u624d\u5728\u5b66\u6821\u5403\u8fc7\u665a\u996d\u4e86\u3002</p>"},{"location":"writing/2012/#jun-14-2012","title":"Jun 14, 2012","text":"<p>\u7ee7\u7eed\u6298\u817e\u7535\u8111\u5f55\u97f3\u3002\u5220\u5220\u88c5\u88c5\u7684\uff0cubuntu studio\u542f\u52a8\u8fdb\u4e0d\u53bb\uff0c\u91cd\u88c5\u3002</p>"},{"location":"writing/2012/#jun-16-2012","title":"Jun 16, 2012","text":"<p>\u4eca\u5929\u542c\u5230\u4e00\u53e5\u8bdd\uff0c\u8bf4\u9f13\u624b\u601d\u8003\u8282\u594f\u65f6\uff0c\u4e0d\u8981\u6309\u7167\u6570\u5b57\u60f3\uff0c\u6bd4\u5982\u4e00\u5206\u949f\u591a\u5c11\u4e0b\uff1b\u800c \u8981\u6309\u7167\u97f3\u4e50\u7684\u611f\u60c5\u8272\u5f69\u60f3\uff0c\u6bd4\u5982\u5fe7\u4f24\u6709\u591a\u6162\uff0c\u6109\u60a6\u6709\u591a\u5feb\uff0c\u4ece\u800c\u6253\u51fa\u5408\u4e4e\u66f2\u5b50\u60c5 \u611f\u7684\u8282\u62cd\u3002\u90a3\u6211\u53ef\u4e0d\u53ef\u4ee5\u8bf4\uff0c\u201c\u4eca\u5929\u6211\u6709100bpm\u7684\u5fc3\u60c5\u201d\u8fd9\u6837\u7684\u3002</p>"},{"location":"writing/2012/#jun-17-2012","title":"Jun 17, 2012","text":"<p>\u4eca\u5929\u770b\u5b9e\u9a8c\u5ba4\u7f51\u7ad9\u66f4\u65b0\uff0c\u65e0\u610f\u4e2d\u67d0\u540c\u5b66\u7684\u7231\u597d\u91cc\u6709\u4e00\u6761:\u5f00\u98de\u673a\u3002\u9760\uff0c\u6709\u94b1\u4eba\u3002</p>"},{"location":"writing/2012/#jun-25-2012","title":"Jun 25, 2012","text":"<p>\u8fd9\u4e2a\u590f\u5929\u5b9a\u4e86\u4e0d\u5c11\u5c0f\u8ba1\u5212\uff0c\u5176\u4e2d\u4e00\u4e2a\u662f\u5207\u4e09\u4e2a\u5c0f\u897f\u74dc\u5403\u3002\u539f\u4ee5\u4e3a\u6700\u7b80\u5355\uff0c\u7ed3\u679c\u8dd1 3000\u7c73\u7684\u8ba1\u5212\u5b8c\u6210\u4e86\uff0c\u5c0f\u897f\u74dc\u8fd8\u4e00\u4e2a\u90fd\u6ca1\u89e3\u51b3\u3002\u9996\u8981\u96be\u9898\u662f\uff0c\u5c0f\u897f\u74dc\u6ca1\u6709\u90a3\u4e48\u597d \u4e70\uff0c\u4ee5\u524d\u770b\u5230\u5f88\u591a\uff0c\u73b0\u5728\u60f3\u4e70\u65f6\u5c45\u7136\u4e00\u4e2a\u90fd\u4e0d\u51fa\u6765\u3002\u7b2c\u4e8c\u96be\u9898\u662f\uff0c\u4e70\u4e86\u8981\u5403\u5b8c\uff0c \u73b0\u5728\u6211\u4e70\u7684\u56db\u5206\u4e4b\u4e00\u5927\u74dc\uff0c\u90fd\u5f88\u52c9\u5f3a\u624d\u80fd\u5403\u5b8c\u3002\u590f\u5929\u60f3\u5403\u7684\u51b7\u6c34\u679c/\u96ea\u7cd5/\u679c\u6c41\u592a \u591a\u4e86\uff0c\u518d\u52a0\u4e0a\u996d\u91cf\u6ca1\u6709\u660e\u663e\u6d88\u51cf\uff0c\u897f\u74dc\u8fd9\u79cd\u5927\u5bb6\u4f19\u8fd8\u771f\u4e0d\u5bb9\u6613\u5403\u5149\u3002\u7406\u60f3\u548c\u73b0\u5b9e \u7684\u5dee\u8ddd\u771f\u5927\u3002</p>"},{"location":"writing/2012/#jun-27-2012","title":"Jun 27, 2012","text":"<p>\u6628\u5929\u7684\u68a6\uff0c\u751f\u5316\u5371\u673a\u7684\u7ecf\u5178\u7535\u952f\u7537\uff0c\u952f\u5f00\u4e86\u6211\u5bbf\u820d\u7684\u6574\u4e00\u9762\u5899\uff0c\u8bf4\u4f60\u7ec3\u9f13\u8e29\u5927\u9f13 \u9707\u7684\u6211\u5bb6\u5230\u5904\u6296\uff0c\u6211\u4e5f\u6765\u7535\u952f\u8ba9\u4f60\u5bb6\u6296\u6296\u3002\u5413\u6b7b\u4e86\uff0c\u4e0d\u81f3\u4e8e\u5427\uff0c\u6211\u4e00\u822c\u523b\u610f\u8e29\u5f88 \u8f7b\u7684\u554a\u3002\u4f4f\u5bbf\u820d\u7ec3\u9f13\u5c31\u662f\u618b\u5c48\u3002</p>"},{"location":"writing/2012/#jul-1-2012","title":"Jul 1, 2012","text":"<p>\u5012\u9709\uff0c\u78b0\u5565\u4eea\u5668\u5565\u4eea\u5668\u574f\uff0c\u64cd\u4f5c\u65f6\u95f4\u4e0d\u5230\u4e00\u4e2a\u5c0f\u65f6\u7684\u5b9e\u9a8c\uff0c\u505a\u4e86\u4e00\u5929\u8fd8\u8981\u665a\u4e0a\u8dd1 \u56de\u53bb\u4e00\u8d9f\u3002</p> <p>3DS\u4e0a\u7684\u751f\u5316\u5371\u673aresident evil - revelation\u60ca\u8273\uff0c\u60c5\u8282\u7d27\u51d1\uff0c\u591a\u89d2\u5ea6\u7684\u53d9\u4e8b \u65b9 \u5f0f\uff0c\u73a9\u5bb6\u8eab\u8fb9\u603b\u6709\u4e2a\u5e2e\u624b\u4e0d\u4f1a\u5bc2\u5bde\uff0c\u8fd8\u6709Jill\u548cChris\u8fd9\u4e00\u5bf9\uff01\u73a9\u7684\u666e\u901a\u96be\u5ea6\uff0c \u4e0d \u7b80\u5355\uff0c\u52a8\u4e0d\u52a8\u6253\u5230\u5f39\u5c3d\u7cae\u7edd\uff0c\u6b7b\u8fc7\u51e0\u6b21\u3002\u4e0d\u723d\u7684\u4e5f\u6709\uff0c\u5149\u7ebf\u592a\u6697\uff0c\u89c6\u89d2\u6ca1\u6cd5 \u8c03 \u6574\uff0c\u6bcf\u6b21\u88ab\u50f5\u5c38\u8ffd\u90fd\u4e0d\u5c0f\u5fc3\u8dd1\u5230\u5899\u89d2\u53bb\u4e86\uff1b\u8dd1\u7684\u901f\u5ea6\u592a\u6162\uff0c\u8dd1\u4e86\u534a\u5929\u56de\u5934\u6253 \u4e24\u67aa \u50f5\u5c38\u5c31\u8ffd\u5230\u8ddf\u524d\u4e86\u3002\u5728\u9634\u6697\u7684\u697c\u9053\u91cc\u88ab\u50f5\u5c38\u8ffd\uff0c\u90a3\u4e2a\u618b\u5c48\u3002\u504f\u504f\u4eca\u5929\u56de\u5bb6 \u697c\u9053\u91cc \u4e5f\u6ca1\u706f\uff0c\u641e\u5f97\u6211\u5fc3\u91cc\u53d1\u865a\u3002</p> <p>\u82b1\u4e86\u4e00\u5929\u7684\u65f6\u95f4\uff0c\u7ec8\u4e8e\u641e\u6e05\u695amidi\u5f55\u97f3\u662f\u4ec0\u4e48\u6982\u5ff5\u4e86\uff0c\u6211\u7684\u97f3\u7b26\u8d70\u7684\u8def\u7ebf\u5982\u4e0b\uff1a \u7535\u9f13\u89e6\u53d1\u5668\u2013&gt;2. MIDI\u8f93\u51fa\u2013&gt;3. USB\u8f93\u5165\u7b14\u8bb0\u672c\u7535\u8111\u2013&gt;4. MIDI\u63a5\u6536\u8f6f\u4ef6\u2013&gt;5. MIDI\u5f55\u5236\u8f6f\u4ef6\u2013&gt;6. dropbox\u2013&gt;7. \u53f0\u5f0f\u673a\u2013&gt;8. Cubase\u7684\u63d2\u4ef6 EZDrummer\u628aMIDI\u952e \u4f4d\u548c\u9f13\u7684\u96f6\u90e8\u4ef6\u5bf9\u5e94\u2013&gt;9. Cubase\u8f93\u51fa\u97f3\u9891</p> <p>\u81f3\u4e8e\u4e3a\u4ec0\u4e48\u4e0d\u5728\u7b14\u8bb0\u672c\u7535\u8111\u4e0a\u76f4\u63a5\u88c5\u97f3\u9891\u8f6f\u4ef6\u641e\u5b9a\uff1f\u56e0\u4e3a\u90a3\u53f0\u7535\u8111\u53ea\u6709\u7528linux \u624d \u8ba4MIDI\u7ebf\uff0cwindows\u4e0b\u4e0d\u8ba4\uff0c\u800c\u97f3\u9891\u8f6f\u4ef6Cubase\u7b49\u7b49\u53ea\u80fd\u76d7\u7248\u88c5win\u4e0a\u3002\u3002</p> <p>\u8fd9\u5c31\u662fMIDI\u7684\u795e\u5947\uff01\u8d70\u4e86\u90a3\u4e48\u8fdc\u7684\u8def\uff0c\u56e0\u4e3a\u662f\u6570\u5b57\u6570\u636e\uff0c\u6240\u4ee5\u8d28\u91cf\u4e0d\u4f1a\u635f\u5931\uff0c\u53ea \u8981\u6700\u540e\u4e00\u6b65\u8fd8\u539f\u5230\u5bf9\u5e94\u97f3\u8272\u5c31\u884c\u3002\u53ef\u662f\u6211\u7684\u5934\u4e5f\u5927\u7684\u4e0d\u80fd\u518d\u5927\u4e86\uff0c\u5c31\u5f55\u4e2a\u97f3\u554a\u81f3 \u4e8e\u4e48\u3002</p> <p>\u800c\u7b77\u5b50\u6572\u9505\u97f3\u8d28\u8d70\u7684\u8def\u7ebf\u662f\uff1a</p> <p>\u7535\u9f13\u97f3\u9891\u8f93\u51fa\u2013&gt;\u97f3\u9891\u8f6c\u6362\u63a5\u5934[\u635f]\u2013&gt;\u97f3\u9891\u8f93\u5165\uff0c\u97f3\u90fd\u6bc1\u5728\u4e2d\u95f4\u4e00\u6b65\u4e86\u3002\u4e4b\u524d\u5f55 \u97f3\u4e5f\u6709\u7c7b\u4f3c\u95ee\u9898\uff0c\u51e0\u5757\u94b1\u7684\u97f3\u9891\u7ebf\u679c\u7136\u4e0d\u62b5\u7528\u3002</p> <p>\u539f\u672c\u53ea\u60f3\u626f\u6839\u7ebf\u5f55\u4e2a\u97f3\u7684\uff0c\u600e\u4e48\u8fd9\u4e48\u9ebb\u70e6\u5751\u7239\u554a\u3002\u7ed9\u81ea\u5df1\u4e00\u4e2a\u5927\u8dc3\u8fdb\u76ee\u6807\uff1a\u80e1\u62fc \u4e71\u51d1\u6574\u4e2ajazz\u7684solo</p>"},{"location":"writing/2012/#jul-10-2012","title":"Jul 10, 2012","text":"<p>\u505a\u4e8b\u5931\u8d2570%\u7684\u539f\u56e0\u662f\u61d2\uff0c\u5269\u4e0b30%\u7684\u539f\u56e0\u5c31\u61d2\u5f97\u60f3\u4e86\u3002</p>"},{"location":"writing/2012/#jul-12-2012","title":"Jul 12, 2012","text":"<p>\u8001\u9769\u547d\u9047\u5230\u65b0\u95ee\u9898\u3002\u670b\u53cb\u7684\u8f66\u4f4e\u80ce\u538b\u7684\u8b66\u62a5\u706f\u4eae\u4e86\uff0c\u6211\u8bf4\u6ca1\u95ee\u9898\uff0c\u6253\u6c14\u53bb\uff0c5\u5206 \u949f \u7684\u7b80\u5355\u4e8b\u3002\u5b9e\u9645\u64cd\u4f5c\u50bb\u773c\u4e86\uff0c\u4e24\u4e2a\u8f6e\u80ce\u5f88\u96be\u91cf\uff0c\u5c31\u53ea\u6709\u4e00\u4e2a\u65b9\u5411\u53ef\u4ee5\u6d4b\u5230\uff0c \u5f97\u62ff \u80ce\u538b\u8ba1\u4e0d\u505c\u7684\u8bd5\uff0c\u6709\u4e2a\u91cf\u5230\u8fc7\u4e00\u6b21\u4e4b\u540e\u6b7b\u6d3b\u627e\u4e0d\u5230\u90a3\u4e2a\u70b9\u4e86\u3002\u7b2c\u4e8c\u4e2a\u95ee\u9898 \u662f\u8f66\u8f6e \u76d6\u5b50\u8986\u76d6\u9762\u592a\u5927\uff0c\u9001\u6c14\u7684\u7ba1\u5b50\u88ab\u6321\uff0c\u53ea\u6709\u4e00\u4e2a\u7279\u6b8a\u89d2\u5ea6\u80fd\u63a5\u4e0a\u6c14\u95e8\uff0c\u4e8e \u662f\u53c8\u662f\u627e \u89d2\u5ea6\u53c8\u662f\u632a\u8f66\uff0c\u5145\u4e86\u597d\u51e0\u6b21\u624d\u628a\u6240\u6709\u8f6e\u80ce\u90fd\u5145\u6c14\u4e86\u3002\u6700\u540e\uff0c\u56e0\u4e3a\u6389\u4e86 \u4e2a\u6c14\u95e8\u82af \uff08\u4f30\u8ba1\u56e0\u4e3a\u8fd9\u4e2a\u624d\u6f0f\u7684\u6c14\uff09\uff0c\u6700\u540e\u8fd8\u5f97\u53bb\u627e\u5356\u5bb6\u3002</p>"},{"location":"writing/2012/#jul-16-2012","title":"Jul 16, 2012","text":"<p>\u5174\u8da3\u5b88\u6052\u539f\u7406\u3002\u524d\u4e00\u9635\u5b50\u5fd9\u7740\u6572\u9f13\uff0c\u6237\u5916\uff0c\u770b\u4e66\uff0c\u5bf9\u6e38\u620f\u7684\u5174\u8da3\u964d\u4f4e\u4e3a\u96f6\uff0c\u603b\u7b97 \u6d88\u505c\u4e86\u5fc3\u763e\u3002\u5f53\u65f6\u60f3\u662f\u4e0d\u662f\u5230\u5934\u4e86\uff1a\u4eba\u4e00\u65e6\u5230\u5e74\u9f84X\u51b7\u6de1\uff0c\u5c31\u518d\u4e5f\u56de\u4e0d\u53bb\u4e86\u3002\u7ed3 \u679c \u8fd9\u5468\u7ec3\u9f13\u5403\u4e86\u82e6\u5934\uff0c\u5373\u5174\u4e5f\u4f7f\u4e0d\u4e0a\u52b2\uff0c\u5bf9\u6e38\u620f\u53c8\u71c3\u8d77\u4e86\u5174\u8da3\u3002\u5509\uff0c\u5929\u592a\u70ed\uff0c \u5bf9\u8981 \u52a8\u7684\u4e8b\u60c5\u5174\u8da3\u90fd\u8f6c\u79fb\u7ed9\u6c99\u53d1\u4e0a\u6253\u6e38\u620f\u4e86\u3002</p> <p>\u4eca\u5929\u4e00\u65e9\u770b\u8001\u677f\u90ae\u4ef6\uff0c\u95ee\u6211\u6709\u6ca1\u6709\u51c6\u5907\u597d\u5e7b\u706f\u7247\uff0c\u7b49\u4f1a\u548c\u53e6\u4e00\u4e2a\u7ec4skype\u7684\u65f6\u5019 \u7528 \u6765\u8bb2\u6211\u7684\u6570\u636e\u3002\u4e00\u60ca\uff0c\u54ea\u77e5\u9053\u8fd8\u8981\u8fd9\u4e48\u6b63\u5f0f\uff0c\u6211\u8fd8\u4ee5\u4e3a\u5c31\u53e3\u5934\u804a\u4e00\u804a\u5462\u3002\u5934 \u76ae\u53d1 \u51c9\uff0c\u8111\u5b50\u8f6c\u901f\u52a0\u5feb\uff0c\u6de1\u5b9a\u4e0b\u6765\u56de\u590d\u8bf4\u51c6\u5907\u597d\u4e86\uff0c\u7136\u540e\u72c2\u5954\u5b9e\u9a8c\u5ba4\u3002\u5e78\u597d\u4e00 \u65e9\u542c\u4e86 \u8001\u677f\u7684\u5efa\u8bae\uff0c\u4e00\u51fa\u6570\u636e\u5c31\u6574\u7406\u6210\u56fe\u7247\uff0c\u53ea\u8981\u706b\u901f\u7684\u62fc\u51d1\u6210\u4e2apdf\u5c31\u884c\u4e86\u3002 \u540e\u6765\u8001\u677f \u89c9\u5f97\u7ed9\u5bf9\u65b9\u7559\u70b9\u65f6\u95f4\uff0c\u5148\u628a\u6211\u6570\u636e\u53d1\u8fc7\u53bb\u4ed6\u4eec\u770b\u770b\u518d\u6539\u5929skype\u3002\u5f97\u6551 \u4e86\u3002</p>"},{"location":"writing/2012/#aug-2-2012","title":"Aug 2, 2012","text":"<p>\u73a9DIRT 2\u73a9\u4e86\u534a\u5929\u89c9\u5f97\u5239\u8f66\u600e\u4e48\u90a3\u4e48\u4e0d\u7075\uff0c\u8fc7\u5f2f\u8001\u662f\u5239\u4e0d\u4f4f\u8f66\u98de\u51fa\u53bb\u3002\u540e\u6765\u518d\u770b \u8bf4\u660e\u4e66\uff0c\u539f\u6765\u624b\u67c4\u4e0a\u6211\u4e00\u76f4\u6309\u7684\u662f\u624b\u5239\u3002</p>"},{"location":"writing/2012/#aug-17-2012","title":"Aug 17, 2012","text":"<p>\u5b9e\u9a8c\u5ba4-80C\u5ea6\u51b0\u7bb1\u95e8\u4e0a\u4f1a\u6162\u6162\u7ed3\u4e00\u5c42\u971c\uff0c\u4e8e\u662f\u53ef\u4ee5\u7528\u624b\u6307\u4f5c\u753b\u3002\u7b2c\u4e00\u4e2a\u4eba\u753b\u4e86\u4e2a \u8138\u4e4b\u540e\u540e\u9762\u7684\u4eba\u9646\u7eed\u52a0\u4e86\u4e1c\u897f\u3002\u6709\u5929\u65e0\u804a\u6211\u5c31\u7ed9\u4e2a\u5356\u840c\u7684\u5c0f\u4eba\u52a0\u4e86\u4e2a\u4e2d\u6307\u3002\u5b9e\u9a8c \u5ba4\u7684\u4eba\u542c\u8bf4\u4f5c\u8005\u662f\u6211\u4e4b\u540e\u90fd\u60ca\u8bb6\u7684\u7b11\u7684\u4e0d\u884c\uff0c\u4e00\u4e2amm\u8fd8\u4ece\u51f3\u5b50\u4e0a\u6389\u4e86\u4e0b\u6765\u3002\u4e8e\u662f \u6211\u5728\u6574\u5c42\u697c\u90fd\u706b\u4e86\u3002</p>"},{"location":"writing/2012/#aug-27-2012","title":"Aug 27, 2012","text":"<p>\u4eca\u5929\u53c2\u52a0\u5b9e\u9a8c\u5ba4\u7684\u5403\u8fa3\u6912\u5927\u8d5b\uff0c\u5403\u58a8\u897f\u54e5\u8fa3\u6912\u3002\u6211\u5403\u4e86\u4e00\u4e2a\u5c31\u5012\u4e0b\u4e86\uff0c\u62ff\u4e86\u7b2c\u4e09 \u540d\u3002\u603b\u5171\u56db\u4eba\uff0c\u7b2c\u56db\u540d\u5403\u4e863/4\u4e2a\u3002\u7b2c\u4e00\u540d\u5e72\u6389\u7ea6\u4e09\u4e2a\u5c31\u75db\u82e6\u7684\u8981\u547d\u3002\u4e4b\u540e\u72c2\u559d \u725b \u5976\uff0c\u5403\u9999\u8549\uff0c\u5564\u9152\u3002</p>"},{"location":"writing/2012/#sep-11-2012","title":"Sep 11, 2012","text":"<p>\u6628\u5929\u68a6\u5230\u56de\u56fd\uff0c\u5feb\u56de\u6765\u7684\u65f6\u5019\u60f3\u8d77\u8fd9\u6b21\u4f11\u5047\u90fd\u6ca1\u8ddf\u8001\u677f\u8bf4\uff0c\u56de\u53bb\u600e\u4e48\u4ea4\u4ee3\u3002\u518d\u4e00 \u7ec6\u60f3\uff0c\u600e\u4e48\u6ca1\u6709\u5173\u4e8e\u56de\u56fd\u524d\u5728\u7f8e\u56fd\u5e72\u4e86\u5565\u7684\u8bb0\u5fc6\uff0c\u65f6\u95f4\u8f74\u6709\u65ad\u5c42\u4e86\u3002\u800c\u4e14\u56de\u56fd\u5446 \u90a3\u4e48\u4e45\uff0c\u4e00\u662f\u4ece\u6765\u6ca1\u6709\u770b\u89c1\u8fc7\u663e\u793a\u65f6\u95f4\u7684\u4e1c\u897f\uff0c\u6bd4\u5982\u949f\u8868\uff0c\u7535\u89c6\uff0c\u7535\u8111\u4e0a\u7684\u65f6 \u95f4\uff0c\u4e8c\u662f\u4ece\u6765\u6ca1\u6709\u8ddf\u5916\u754c\u53d1\u751f\u8fc7\u4ea4\u6d41\u3002\u4e8e\u662f\u65ad\u5b9a\u662f\u5728\u505a\u68a6\uff0c\u65e2\u7136\u662f\u505a\u68a6\uff0c\u653e\u5047\u5c31 \u968f\u4fbf\u653e\u4e86\u3002</p> <p>\u5f00\u7ec4\u4f1a\u7684\u65f6\u5019\u8bb2\u4e24\u5f20\u56fe\uff0c\u5de6\u8fb9\u653e\u4e00\u5f20\u53f3\u8fb9\u653e\u4e00\u5f20\u3002\u8bb2\u5230\u4e00\u534a\u6709\u4eba\u95ee\u5de6\u8fb9\u662f\u4ec0\u4e48\uff0c \u6211\u8111\u5b50\u77ed\u8def\u7406\u89e3\u9519\u4e86\u95ee\u9898\uff0c\u6307\u7740\u5de6\u8fb9\u7684\u56fe\u8bf4\uff0c\u8fd9\u662f\u5de6\u8fb9\u3002</p>"},{"location":"writing/2012/#sep-28-2012","title":"Sep 28, 2012","text":"<p>\u6700\u8fd1\u6709\u4e9b\u7d2f\uff0c\u4f11\u606f\u4e00\u4e0b\uff0c\u8ddf\u81ea\u5df1\u8bf4\u4f1a\u8bdd\u3002\u6709\u79cd\u60f3\u628a\u4e8b\u60c5\u505a\u597d\uff0c\u53c8\u7f3a\u4e4f\u81ea\u4fe1\u7684\u7126\u8651\u3002 \u5c24\u5176\u8868\u73b0\u5728\u6253\u9f13\u4e0a\u3002\u81ea\u4ece\u7ec4\u4e86\u961f\u610f\u8bc6\u5230\u81ea\u5df1\u8bf8\u591a\u6bdb\u75c5\uff0c\u5c31\u60f3\u6765\u4e2a\u5927\u8dc3\u8fdb\uff0c\u62fc\u6b7b\u7ec3\u3002 \u4f46\u5374\u5e26\u6765\u4e86\u795e\u7ecf\u7d27\u5f20\u7b49\u7b49\u8d1f\u60c5\u7eea\uff0c\u4e3a\u4e86\u7f13\u89e3\u6216\u62d6\u5ef6\u8fd9\u4e9b\u60c5\u7eea\uff0c\u53cd\u5012\u803d\u8bef\u4e86\u7ec3\u4e60\u65f6 \u95f4\uff0c\u4e22\u4e86\u4e9b\u4e50\u611f\uff0c\u66f4\u6709\u6f5c\u5728\u7684\u78e8\u706d\u5174\u8da3\u7684\u574f\u5904\u3002\u8981\u8bb0\u4f4f\uff0c\u9996\u8981\u76ee\u7684\u662f\u5a31\u4e50\u3002\u8981\u653e \u677e\u3002</p> <p>\u53c8\u6bd4\u5982\u8bf4\u4f11\u606f\u65f6\u95f4\u3002\u76ee\u6807\u662f\u65e9\u7761\uff0c\u4f46\u662f\u6ca1\u52309\u70b9\u5c31\u5f00\u59cb\u7126\u8651\uff0c\u89c9\u5f97\u665a\u4e0a\u5269\u4e0b\u7684\u65f6 \u95f4 \u4e0d\u591a\uff0c\u8fd8\u6ca1\u6709\u5f00\u59cb\u7ec3\u9f13\uff0c\u7126\u8651\uff0c\u90a3\u5c31\u4e0a\u4f1a\u7f51\u5427\u3002\u7136\u540e\u62d6\u52301\u70b9\u591a\u7761\u89c9\uff0c\u7b2c\u4e8c\u5929 \u5f00\u59cb \u6076\u6027\u5faa\u73af\u3002</p>"},{"location":"writing/2012/#oct-25-2012","title":"Oct 25, 2012","text":"<p>\u7528\u5b9e\u9645\u884c\u52a8\u8bc1\u660e\u4e86\uff0c\u5750\u7535\u68af\u6bd4\u770b\u4e0a\u53bb\u8981\u6162\uff0c\u56e0\u4e3a\u8981\u7b49\u7535\u68af\uff0c\u7535\u68af\u4e2d\u9014\u8fd8\u53ef\u80fd\u505c\u4e0b \u653e\u4eba\u7b49\u7b49\u3002\u540c\u65f6\u51fa\u53d1\uff0c\u6211\u8dd1\u697c\u68af\u51b2\u4e0a9\u697c\uff0c\u5c45\u7136\u6bd4\u4e24\u4e2a\u7535\u68af\u91cc\u7684\u4eba\u90fd\u5148\u5230\uff0c\u771f\u957f \u9762 \u5b50\u3002</p> <p>\u4ece\u4eca\u5929\u8d77\uff0c\u8c01\u63d0\u6bd5\u4e1a\u8ddf\u8c01\u6025\u3002\u6211\u5176\u5b9e\u8fd8\u60f3\u6df7\u4e0b\u53bb\uff0c\u53ef\u8001\u677f\u89c9\u5f97\u6211\u660e\u5e74\u4e2d\u4e0b\u65ec\u5c31\u53ef \u4ee5\u6bd5\u4e1a\u4e86\u3002\u4eca\u5929\u627e\u5c0f\u79d8\u8c08\u4e86\u8ba1\u5212\uff0c\u65f6\u95f4\u5b89\u6392\uff0c\u8fdb\u5165\u6050\u614c\u72b6\u6001\uff0c\u9879\u76ee\u8981\u505a\u5b8c\uff0c\u4e0b\u5bb6 \u8981\u627e\u597d\uff0c\u8fd9\u4e9b\u6211\u4ee5\u4e3a\u9065\u8fdc\u7684\u4e8b\uff0c\u73b0\u5728\u5c31\u8981\u5f00\u5de5\u4e86\u3002\u800c\u4e14\u6211\u8fd8\u60f3\u5728\u5f53\u524d\u5e08\u5085\u4e0b\u591a\u5b66 \u70b9\u9f13\u5462\uff0c\u8fd9\u4e0b\u4e0d\u5230\u4e00\u5e74\u4e86\uff0c\u548b\u529e\u3002</p>"},{"location":"writing/2019/","title":"2019\u968f\u8bb0","text":""},{"location":"writing/2019/#_1","title":"\u5de5\u4f5c/\u793e\u4ea4","text":"<p>\u5de5\u4f5c\u7684\u7b2c\u4e00\u5e74\uff1a\u65e0\u6bd4\u60f3\u5ff5phd\u540c\u5b66\u3002\u89c9\u5f97\u8ddf\u540c\u4e8b\u6ca1\u6709\u4ec0\u4e48\u4ea4\u96c6\uff0c\u65e0\u5fc3\u793e\u4ea4\u3002\u4ece\u5b9e \u9a8c\u5ba4\u53d8\u5316\u5230\u4e00\u6392\u673a\u5668\u5e73\u94fa\u7684\u5f00\u653e\u5f0f\u529e\u516c\u5ba4\uff0c\u6211\u53d7\u5230\u6700\u5927\u7684\u6587\u5316\u51b2\u51fb\u5c45\u7136\u662f\uff1a\u540c\u4e8b \u4e0a\u73ed\u600e\u4e48\u4e0d\u8fb9\u4e0a\u8fb9\u804a\u5929\uff0c\u800c\u662f\u5403\u996d\u518d\u804a\u3002\u6211\u867d\u7136\u4e0d\u662f\u4e2a\u7231\u8bf4\u8bdd\u7684\u4eba\uff0c\u4f46\u542c\u591a\u4e86\u4e5f \u4f1a\u65f6\u4e0d\u65f6\u5410\u69fd\u5e76\u4ece\u4e2d\u83b7\u5f97\u4e50\u8da3\uff0c\u5e76\u4ee5\u6b64\u5efa\u7acb\u793e\u4f1a\u5173\u7cfb\u3002\u4e0a\u73ed\u5927\u5bb6\u4e0d\u804a\u5929\u600e\u4e48\u6574\u3002 \u5176\u5b9e\u8fd9\u662f\u5199\u4ee3\u7801\u548c\u505a\u5b9e\u9a8c\u7684\u533a\u522b\u3002</p> <p>\u5de5\u4f5c\u7684\u7b2c\u4e8c\uff0c\u4e09\u5e74\uff1a\u4ece\u4e0d\u81ea\u5728\u5730\u505a\u5c40\u5916\u4eba\u6210\u529f\u8fc7\u6e21\u5230\u4e0d\u4ecb\u610f\u505a\u5c40\u5916\u4eba\u3002</p> <p>\u5de5\u4f5c\u7684\u7b2c\u4e09\u5e74\uff1a\u627e\u5230\u4e86\u4efd\u79bb\u5bb6\u8fd1\uff0c\u5f85\u9047\u66f4\u597d\u7684\u5de5\u4f5c\u3002</p> <p>\u79bb\u804c\u7684\u524d\u4e24\u5468\uff1a\u6b63\u5f0f\u63d0\u4ea4\u8f9e\u804c\u7533\u8bf7\uff0c\u5927\u8001\u677f\u63d0\u51fa\u4e86\u51e0\u4e2a\u633d\u7559\u65b9\u6848\uff0c\u6211\u62d2\u7edd\u4e86\uff0c\u6211 \u53ea\u60f3\u7ffb\u65b0\u7bc7\uff0c\u6bcf\u5929\u65e9\u70b9\u56de\u5bb6\u3002</p> <p>\u79bb\u804c\u7684\u524d\u4e00\u5468\uff1a\u6570\u7740\u65e5\u5b50\u76fc\u671b\u65b0\u751f\u6d3b\uff0c\u5728\u8001\u677f\u5141\u8bb8\u540e\u5f00\u5fc3\u7684\u5411\u7ec4\u91cc\u5ba3\u5e03\u79bb\u804c\uff0c\u5e76 \u793c\u8c8c\u6027\u8868\u793a\u9057\u61be\u3002</p> <p>\u79bb\u804c\u7684\u4e0b\u5348\uff1a\u60ca\u559c\u86cb\u7cd5\uff0c\u544a\u522b\u805a\u4f1a\uff0c\u63e1\u624b\uff0c\u62e5\u62b1\uff0c\u539f\u6765\u5373\u4f7f\u662f\u793e\u4ea4\u6050\u60e7\u7684\u6211\uff0c\u4e5f \u662f\u88ab \u60e6\u8bb0\u7740\u7684\u3002\u8fd9\u548c\u6211\u8ba1\u5212\u7684\u4f4e\u8c03\u79bb\u53bb\u7684\u5267\u672c\u4e0d\u4e00\u6837\uff0c\u7adf\u6709\u4e00\u4e1d\u4f24\u611f\u3002\u4e0d\u80fd\u7f3a \u5c11\u6211\u4e00 \u8d2f\u7684\u5634\u8d31\u7279\u8272\uff0c\u6211\u201c\u79bb\u804c\u6f14\u8bb2\u201d\u8bf4\uff0c\u6211\u4eec\u7ec4\u6700\u7279\u522b\u7684\u662f\u6bcf\u4e2a\u4eba\u90fd\u6765\u81ea\u4e0d\u540c \u7684\u80cc\u666f\uff0c \u4e0d\u540c\u7684\u77ed\u5904\uff0c\u6240\u4ee5\u6211\u8ddf\u6bcf\u4e2a\u4eba\u6bd4\u90fd\u6709\u6bd4\u4ed6\u5f3a\u7684\u5730\u65b9\uff0c\u611f\u89c9\u53ef\u597d\u4e86\u3002\u6709 \u4eba\u7b11\u51fa\u4e86\u773c\u6cea\u3002</p> <p>\u79bb\u804c\u540e\u7684\u7b2c\u4e8c\u5929\uff1a\u7adf\u7136\u5f00\u59cb\u65e0\u6bd4\u6000\u5ff5\u540c\u4e8b\uff0c\u5934\u4e00\u5929\u620f\u8bf4\u7684\u201c\u79bb\u804c\u540e\u6211\u4f1a\u7a81\u7136\u53d1\u73b0 \u4f60\u4eec\u90fd\u5f88\u597d\u201d\u6210\u771f\u4e86\uff0c\u539f\u6765\u6211\u771f\u7684\u820d\u4e0d\u5f97\u6bcf\u4e00\u4e2a\u4eba\u3002</p> <p>\u79bb\u804c\u540e\u7684\u7b2c\u4e00\u5468\uff1a\u4f9d\u7136\u5728\u6000\u5ff5\u6bcf\u4e00\u4f4d\u65e7\u540c\u4e8b\uff0c\u4f9d\u7136\u5929\u5929\u68a6\u5230\u4ed6\u4eec\u3002</p> <p>\u79bb\u804c\u540e\u7684N\u5929\uff1a\u6211\u95ee\u961f\u53cb\uff0c\u6211\u8fd9\u89c1\u4eba\u8eb2\u4e3a\u5148\u7684\u6027\u683c\uff0c\u4e3a\u4ec0\u4e48\u79bb\u804c\u4e5f\u4f1a\u6709\u4eba\u6000\u5ff5\uff0c \u6211\u54ea\u91cc\u50cf\u4e2a\u597d\u540c\u4e8b\u4e86\uff1f\u961f\u53cb\u8bf4\uff0c\u8eb2\u907f\u6280\u80fd\u4e5f\u662f\u4eba\u7269\u7279\u8272\u3002\u6211\u8bf4\uff0c\u90a3\u4f60\u770b\u4e0a\u6211\u5565\u4e86\uff0c \u6211\u4e0d\u4ecb\u610f\u591a\u8bf4\u70b9\u3002\u961f\u53cb\u8bf4\uff0c\u770b\u4e0a\u4f60\u8fd9\u9493\u9c7c\u6c42\u5938\u7684\u539a\u8138\u76ae\u3002</p>"},{"location":"writing/2019/#_2","title":"\u95fa\u5973","text":"<p>\u74dc\u7c7d\u513f\u51fa\u6765\u7684\u4e00\u523b\uff0c\u6211\u4eff\u4f5b\u542c\u5230\u4e86\u4e00\u4e2a\u8ba1\u65f6\u5668\u88ab\u5b89\u88c5\uff0c\u5ba3\u5e03\u201c\u8ba1\u65f6\u5f00\u59cb\uff0c\u4ece\u73b0\u5728 \u5f00\u59cb\u768418\u5e74\uff0c\u628a\u8fd9\u4e2a\u4eba\u7c7b\u5e7c\u5d3d\u517b\u597d\uff0c\u6ca1\u6709\u6682\u505c\u6ca1\u6709\u5b58\u76d8\uff01\u201d\u4ece\u6b64\u751f\u6d3b\u5c31\u8fdb\u5165\u4e86\u4e00 \u4e2a\u5b8c\u5168\u4e0d\u540c\u7684\u6b21\u5143\u3002</p>"},{"location":"writing/2019/#_3","title":"\u9634\u9633\u5e08","text":"<p>\u6700\u5145\u5b9e\u7684\u83ab\u8fc7\u4e8e\u8bf7\u5047\u5237\u8d85\u9b3c\u738b</p>"},{"location":"writing/celeste_death/","title":"Celeste\u91cc\u7684N\u79cd\u6b7b\u6cd5","text":"<p>\u636e\u6211\u6240\u77e5\u5168\u4e16\u754c\u53ea\u6709\u4e00\u4e2a\u4eba\u4e00\u547d\u5168\u6536\u96c6\uff0c\u4e5f\u662f\u6211\u5934\u4e00\u6b21\u770b\u89c1\u9ad8\u624b\u76f4\u64ad\u901a\u5173\u540e\u6fc0\u52a8 \u54ed\u3002\u6211\u5361\u57287c\u4e86\uff0c8c\u3001\u6708\u8393\u3001b\u548cc\u9762\u91d1\u8349\u8393\u4e0d\u6562\u60f3\u3002</p> <ul> <li>\u638c\u673a\u6a21\u5f0f\u6447\u6746\u504f\u79fb</li> <li>wall bounce\u6307\u4ee4\u8fc7\u65e9\u649e\u5899</li> <li>wall bounce\u6307\u4ee4\u8fc7\u665a\u649e\u7a7a\u6c14</li> <li>\u88abbadeline\u6355\u83b7</li> <li>extended super\u53d8super</li> <li>farewell\u6700\u540e\u4e00\u5c4f\u6c34\u6bcd\u6ca1\u6293\u7a33</li> <li>\u4f53\u529b\u4e0d\u652f</li> <li>\u51fa\u6c57\u624b\u6ed1</li> <li>\u7206\u70b8\u9c7c\u6ca1\u8eb2\u8fc7\u53bb</li> <li>\u9519\u8bef\u65b9\u5411\u649e\u7206\u70b8\u9c7c\u5f39\u98de</li> <li>\u8d77\u98ce\u4e86</li> <li>\u5343\u7b97\u4e07\u7b97\u7cbe\u51c6\u649e\u4e0a\u79fb\u52a8\u7684\u654c\u4eba</li> <li>\u8d34\u8eab\u649e\u5c16\u523a</li> <li>\u5de6\u53f3\u5de6\u53f3\u72b9\u8c6b\u5b9a\u4f4d\u6700\u540e\u9519\u5931\u5c0f\u5e73\u53f0</li> <li>\u51b2\u523a\u8ddd\u79bb\u8fc7\u957f\u649e\u523a</li> <li>\u7c89\u4e91\u6ca1\u4e86\u8fd8\u6ca1\u8d77\u8df3</li> </ul>"},{"location":"writing/fallout/","title":"\u9886\u4e3b\u7b14\u8bb0","text":"<p>\u524d\u8a00\uff1a\u6709\u6bb5\u65f6\u95f4\u7cbe\u795e\u538b\u529b\u5de8\u5927\uff0c\u6c89\u8ff7\u6e38\u620f\u90fd\u4e0d\u591f\u4f7f\uff0c\u90c1\u95f7\u5230\u7834\u5929\u8352\u5199\u5c0f\u8bf4\u3002\u80cc\u666f \u4e3a\u8f90\u5c04-\u907f\u96be\u6240\uff08Fallout-Shelter\uff09\uff0c\u4e00\u6b3e\u8f90\u5c04\u4e16\u754c\u8bbe\u5b9a\u7684\u624b\u6e38\u3002\u9760\u6361\u5783\u573e\u548c\u505a \u4efb\u52a1\u8fdb\u884c\u751f\u5b58\u4e0e\u6269\u5f20\u3002</p> <p>293\u57ce\u7b2c\u96f6\u6cd5\u5219\uff1a\u4e0d\u8981\u76f8\u4fe1\u9886\u4e3b\u7684\u65b9\u9488\uff0c\u56e0\u4e3a\u9886\u4e3b\u7f3a\u4e4f\u7ba1\u7406\u7ecf\u9a8c\uff0c\u968f\u65f6\u53ef\u80fd\u6539\u53d8\u65b9\u9488\u3002</p> <p>\u76ee\u5f55</p> <ol> <li>\u798f\u7984\u50e7\u5bb6\u65cf\u7684\u5973\u6027</li> <li>\u798f\u7984\u50e7\u5bb6\u65cf\u7684\u5d1b\u8d77</li> <li>\u8a79\u59ae\u4f69\u6069</li> <li>\u8001\u4eba\u83b1\u6069</li> <li>\u5f00\u8352\u707e\u96be</li> <li>\u98ce\u6c34</li> <li>\u6b22\u8fce\u6536\u542c293\u7535\u53f0</li> <li>\u6b66\u5668\u5382\u7684\u5c0f\u738b</li> </ol>"},{"location":"writing/fallout/#_2","title":"\u798f\u7984\u50e7\u5bb6\u65cf\u7684\u5973\u6027","text":"<p>293\u57ce\u7b2c\u4e00\u6cd5\u5219\uff1a\u9886\u4e3b\u8bf4\u4f60\u5feb\u4e50\uff0c\u4f60\u5c31\u5feb\u4e50\u3002</p> <p>\u798f\u7984\u50e7\u5bb6\u65cf\u7684\u5973\u6027\uff0c\u751f\u6d3b\u5927\u591a\u65e0\u8da3\u3002\u5979\u4eec\u7684\u4e3b\u8981\u804c\u8d23\u662f\u6dfb\u65b0\u4e01\uff0c\u76f4\u5230\u4eba\u53e3\u8fbe\u5230\u4e0a \u9650\u3002\u4e4b\u540e\u4f1a\u600e\u4e48\u6837\uff0c\u8bf4\u5b9e\u8bdd\u9886\u4e3b\u6211\u5f88\u614c\u3002</p> <p>\u5728293\u57ce\uff0c\u4eba\u751f\u800c\u5e73\u7b49\uff1a\u6bcf\u4e2a\u4eba\u751f\u6765\u5feb\u4e50\u503c\u90fd\u662f50%\u8d77\u6b65\u3002\u5feb\u4e50\u503c\u662f\u8bc4\u5b9a\u9886\u4e3b\u7684\u552f \u4e00\u6807\u51c6\uff0c\u800c\u604b\u7231\u751f\u5b50\uff0c\u662f\u6da8\u5feb\u4e50\u503c\u7684\u6377\u5f84\u3002\u798f\u7984\u50e7\u5bb6\u65cf\u7684\u5973\u6027\uff0c\u521a\u6210\u5e74\u5c31\u88ab\u5e26\u53bb \u5bdd\u5ba4\uff0c\u548c\u4e00\u4f4d\u975e\u76f4\u7cfb\u4eb2\u5c5e\u7537\u5b50\u914d\u5bf9\uff0c\u9886\u4e3b\u4e0d\u61d2\u7684\u65f6\u5019\u4f1a\u7ec6\u6311\u4e2a\u6027\u611f\u8001\u9c9c\u8089\uff0c\u61d2\u7684 \u65f6\u5019\u5219\u968f\u624b\u62d6\u4e2a\u6027\u522b\u7537\u3002\u4e00\u6000\u5b55\uff0c\u798f\u7984\u50e7\u59d1\u5a18\u5feb\u4e50\u503c\u76f4\u5954100\uff0c\u9886\u4e3b\u8bf4\u4f60\u5feb\u4e50\u4f60 \u5c31\u5feb\u4e50\u3002</p> <p>\u80f8\u5927\u65e0\u8111\u7684\uff0c\u6000\u5b55\u4e4b\u540e\u53bb\u5e7f\u64ad\u7535\u53f0\u5750\u73ed\uff0c\u7ed9\u5916\u9762\u7684\u5e9f\u571f\u4e16\u754c\u53d1\u7535\u62db\u52df\u8def\u4eba\uff0c\u867d\u7136 \u5e76\u4e0d\u4f1a\u6709\u4eba\u6765\u3002\u8d44\u8d28\u5e73\u5e73\u7684\uff0c\u5219\u9001\u53bb\u8bad\u7ec3\uff0c\u5b66\u4e2a\u7e41\u884d\u4e4b\u5916\u7684\u6280\u80fd\u3002\u8d44\u8d28\u7edd\u4f73\u7684\uff0c \u624d\u6709\u673a\u4f1a\u53bb\u4e00\u7ebf\u5355\u4f4d\u4e0a\u73ed\uff0c\u636e\u8bf4\u8fd9\u662f\u9003\u8131\u751f\u80b2\u8f6e\u56de\u7684\u552f\u4e00\u8def\u5f84\uff0c\u53ea\u6709\u8fdb\u4e86\u4e00\u7ebf\u5355 \u4f4d\uff0c\u624d\u6709\u51e0\u7387\u88ab\u9886\u4e3b\u6293\u58ee\u4e01\u5916\u6d3e\u53bb\u5e9f\u571f\u63a2\u7d22\u3002</p> <p>\u7b49\u5a03\u5471\u5471\u5760\u5730\uff0c\u82e5\u6025\u9700\u7ee7\u7eed\u6269\u5145\u4eba\u53e3\uff0c\u6bcd\u4eb2\u5219\u7acb\u9a6c\u88ab\u9001\u56de\u5bdd\u5ba4\uff0c\u8fce\u63a5\u4e0b\u4e00\u4f4d\u7537\u5b50 \uff08\u53ef\u80fd\u662f\u4e4b\u524d\u540c\u4e00\u4f4d\uff09\u3002\u82e5\u65e0\u4eba\u53e3\u538b\u529b\uff0c\u6bcd\u4eb2\u5219\u56de\u5230\u539f\u5355\u4f4d\uff0c\u5e76\u6709\u673a\u4f1a\u53c2\u4e0e293 \u57ce\u5185\u5916\u7684\u4e30\u5bcc\u751f\u6d3b\u3002</p> <p>\u4f60\u53ef\u80fd\u60f3\u95ee\u90a3\u4e2a\u5bdd\u5ba4\u662f\u4ec0\u4e48\u73a9\u610f\u513f\uff0c\u4e0d\u662f\u4f11\u606f\u7528\u7684\u5417\uff0c\u600e\u4e48\u90a3\u4e48\u90aa\u6076\uff1f\u8fd9\u662f\u4e00\u4e2a \u975e\u76f4\u5c5e\u5173\u7cfb\u7537\u5973\u5fc5\u7136\u4ea4\u5f80\u7684\u5730\u65b9\uff0c\u5305\u6000\u5b55\u3002\u6700\u5927\u7684\u623f\u95f4\u6709\u4e09\u4e2a\u5e8a\u4f4d\uff0c\u4e00\u5339\u79cd\u9a6c\u53ef \u4ee5\u4e00\u6b21\u914d\u4e09\u4e2a\u798f\u7984\u50e7\u59d1\u5a18\u3002\u542c\u5230\u8fd9\u91cc\uff0c\u4f60\u53ef\u80fd\u53c8\u4f1a\u95ee\uff0c\u662f\u4e0d\u662f\u597d\u597d\u5b66\u4e60\u8d70\u4e0a\u4eba\u751f \u5dc5\u5cf0\u5c31\u80fd\u5f53\u4e00\u603c\u4e09\u7684\u79cd\u9a6c\u4e86\u9e2d\uff1f\u9519\uff0c\u8d44\u8d28\u6700\u597d\u7684\u65e0\u8bba\u7537\u5973\uff0c\u90fd\u5728\u5e9f\u571f\u4f53\u9a8c\u751f\u6d3b\uff0c \u5f88\u5c11\u8fdb\u57ce\uff0c\u5373\u4fbf\u8fdb\u57ce\uff0c\u4e5f\u662f\u5fd9\u7740\u4e0a\u73ed\u6216\u8005\u8bad\u7ec3\u3002\u5f53\u79cd\u9a6c\u7684\uff0c\u5168\u662f\u8d44\u8d28\u5e73\u5e73\u7684\u4e3b\u3002</p> <p>\u4f60\u53ef\u80fd\u53c8\u60f3\u95ee\uff0c\u4f60\u5c31\u4e0d\u80fd\u505a\u4e2a\u5c0a\u91cd\u81ea\u7531\u604b\u7231\u3001\u5c0a\u91cd\u5973\u6027\u610f\u5fd7\u3001\u4e0d\u5f3a\u5236\u57f9\u517b\u751f\u80b2\u673a \u5668\u7684\u5584\u826f\u9886\u4e3b\u5417\uff1f\u8fd9\u4e8b\u513f\u4e0d\u80fd\u602a\u6211\uff0c\u5f97\u8d56\u8fd9\u6e38\u620f\u7684\u7834\u6570\u503c\u8bbe\u5b9a\u3002\u7406\u8bba\u4e0a\u6269\u5f20\u4eba\u53e3 \u6709\u4e24\u4e2a\u9014\u5f84\uff0c\u4e00\u662f\u751f\uff0c\u4e8c\u662f\u5e7f\u64ad\u62db\u52df\uff0c\u4f46\u62db\u52df\u7684\u6210\u529f\u7387\u592a\u4f4e\u4e86\uff0c\u4e00\u5929\u6491\u6b7b\u62db\u5230\u4e00 \u4e2a\u83dc\u9e21\u3002\u8981\u662f\u4f5b\u7cfb\u6269\u5f20\uff0c\u5168\u57ce\u90fd\u8981\u559d\u63ba\u7740\u8f90\u5c04\u7684\u897f\u5317\u98ce\u3002</p>"},{"location":"writing/fallout/#_3","title":"\u798f\u7984\u50e7\u5bb6\u65cf\u7684\u5d1b\u8d77","text":"<p>293\u57ce\u7b2c\u4e8c\u6cd5\u5219\uff1a\u4e00\u5bb6\u4eba\u5c31\u8981\u6574\u6574\u9f50\u9f50\uff0c\u4e0d\u6574\u9f50\u7684\u90fd\u662f\u5916\u4eba</p> <p>\u798f\u7984\u50e7\u539f\u672c\u5e76\u4e0d\u662f\u6700\u5927\u7684\u5bb6\u65cf\uff0c\u540c\u671f\u5165\u4f4f293\u57ce\u5f00\u8352\u7684\u8fd8\u6709\u5e03\u6717\u3001\u8def\u6613\u65af\u3001\u54c8\u7279 \u7b49\u7b49\u5bb6\u65cf\u7684\u4eba\u3002\u90a3\u65f6\u5019\u9886\u4e3b\u8fd8\u5728\u770b\u65b0\u624b\u518c\u5b50\uff0c\u6765\u8005\u4e0d\u62d2\u3002\u4f46\u662f\u540e\u6765\u9886\u4e3b\u4e0a\u7f51\u770b\u4e86 \u4e71\u4e03\u516b\u7cdf\u73a9\u6cd5\uff0c\u5927\u624b\u4e00\u6325\u60f3\u641e\u6210\u5355\u4e00\u5927\u5bb6\u65cf\u6a21\u5f0f\uff0c\u5e76\u76f8\u4e2d\u4e86\u798f\u7984\u50e7\u8fd9\u4e2a\u59d3\uff0c\u4e8e\u662f \u5f00\u59cb\u4e86\u9a71\u9010\uff0c\u6240\u6709\u975e\u798f\u7984\u50e7\u5bb6\u65cf\u7684\uff1a</p> <p>\u8d44\u8d28\u5f88\u5dee\uff1a\u9a71\u9010</p> <p>\u8d44\u8d28\u9876\u7ea7\uff1a\u7559\u4e0b\u6765\u505a\u82e6\u529b\uff0c\u4e0d\u5bf9\uff0c\u4f53\u9a8c\u751f\u6d3b</p> <p>\u5176\u4ed6\uff1a\u5973\u6027\u88ab\u9a71\u9010\uff0c\u7537\u6027\u968f\u673a\u9009\u51e0\u4e2a\u4f5c\u4e13\u804c\u79cd\u9a6c\uff0c\u5176\u4ed6\u9a71\u9010</p> <p>\u968f\u540e\u7684\u51e0\u5929\uff0c\u4e3a\u4e86\u8fc5\u901f\u8865\u5145\u6d41\u5931\u7684\u4eba\u53e3\uff0c\u798f\u7984\u50e7\u5bb6\u65cf\u7684\u5973\u6027\u5168\u4f53\u8fde\u751f\u4e86\u51e0\u8f6e\u3002\u7531 \u4e8e\u52b3\u52a8\u529b\u77ed\u7f3a\uff0c\u5979\u4eec\u5927\u7740\u809a\u5b50\u5728\u4e00\u7ebf\u5c97\u4f4d\u4e0a\u5162\u5162\u4e1a\u4e1a\uff0c\u5e78\u798f\u611f\u6ee1\u6ee1\u3002\u4e3a\u4e86\u8d76\u8fdb\u5ea6\uff0c \u51e0\u4e2a\u975e\u798f\u7984\u50e7\u5bb6\u65cf\u7684\u5973\u6027\uff0c\u7834\u4f8b\u8ddf\u798f\u7984\u50e7\u5bb6\u65cf\u7684\u7537\u6027\u4e0d\u53ef\u63cf\u8ff0\u4e00\u665a\uff0c\u7ed9\u5927\u5bb6\u65cf\u6dfb \u4e86\u4e00\u8f6e\u65b0\u4e01\u3002</p> <p>\u4e00\u5929\u8fc7\u53bb\uff0c\u5c31\u904d\u5730\u662f\u798f\u7984\u50e7\u4e86\u3002 </p>"},{"location":"writing/fallout/#_4","title":"\u8a79\u59ae\u4f69\u6069","text":"<p>293\u57ce\u7b2c\u4e09\u6cd5\u5219\uff1a\u5982\u4f55\u5e73\u8861\u5bb6\u5ead\u548c\u5de5\u4f5c\uff1f\u7b49\u6211\u628a\u8fd9\u4efd\u62a5\u544a\u4ea4\u7ed9\u8001\u677f\u4e86\u5c31\u544a\u8bc9\u4f60\u3002</p> <p>\u8a79\u59ae\u4f69\u6069\u662f\u4e2a\u6709\u7279\u6b8a\u5f85\u9047\u7684\u59d1\u5a18\u3002\u6709\u4e2a\u63a2\u7d22\u5206\u961f\u5728\u6267\u884c\u4efb\u52a1\u7684\u65f6\u5019\u6551\u4e0b\u4e86\u5979\u3002\u4e00 \u822c\u6765\u8bf4\u8def\u4e0a\u7684NPC\u6551\u5b8c\u5c31\u539f\u5730\u4e0d\u52a8\u4e86\uff0c\u53ef\u4f69\u6069\u59d1\u5a18\u4e3b\u52a8\u8ddf\u56de\u6765\u5728293\u57ce\u624e\u4e86\u6839\u3002\u6211 \u6253\u91cf\u4e86\u4e00\u756a\uff0c\u7b49\u7ea71\uff0c\u6280\u80fd\u6570\u503c\u2026\u9760\u8fd9\u4e48\u9ad8\u3002\u8fd9\u4e48\u5389\u5bb3\u7684\u59d1\u5a18\uff0c\u53ef\u4e0d\u80fd\u88ab\u751f\u80b2\u91cd \u4efb\u803d\u8bef\u4e86\u3002\u8865\u5145\u8bf4\u660e\uff0c\u5927\u7740\u809a\u5b50\u7684\uff0c\u80fd\u4e0a\u73ed\u80fd\u8bad\u7ec3\uff0c\u552f\u72ec\u4e0d\u80fd\u5916\u51fa\u505a\u4efb\u52a1\u3002\u4ee5\u4f69 \u6069\u7684\u80fd\u529b\uff0c\u5e94\u8be5\u591a\u5728\u5916\u9762\u4f53\u9a8c\u5e9f\u571f\u751f\u6d3b\uff0c\u65e0\u8bba\u662f\u5355\u72ec\u63a2\u7d22\u8fd8\u662f\u52a0\u5165\u4efb\u52a1\u5c0f\u961f\u3002</p> <p>\u4f69\u6069\u59d1\u5a18\u6ca1\u6709\u8f9c\u8d1f\u6211\u7684\u671f\u671b\uff0c\u7a81\u7a81\u7a81\uff0c\u5996\u602a\u5012\u4e0b\u4e86\uff0c\u88c5\u5907\u6361\u5230\u4e86\uff0c\u6750\u6599\u88c5\u5305\u4e86\u3002 \u4e30\u5bcc\u7684\u6218\u6597\u7ecf\u5386\u8ba9\u5979\u8fc5\u901f\u5347\u7ea7\uff0c\u73b0\u5df2\u662f\u63a2\u7d22\u4e3b\u961f\u7684\u56fa\u5b9a\u6210\u5458\u3002</p> <p>\u7136\u800c\u6211\u6709\u4e2a\u5fc3\u75c5\uff0c\u5c31\u662f\u4f69\u6069\u59d1\u5a18\u7684\u5feb\u4e50\u503c\u603b\u662f\u4e0d\u5b8c\u7f8e\u3002\u524d\u9762\u8bf4\u4e86\uff0c\u5feb\u4e50\u901f\u6210\u9760\u556a \u556a\uff0c\u7136\u800c293\u57ce\u91cc\u6ca1\u6709\u907f\u5b55\u7684\u9009\u9879\uff0c\u6ca1\u6709\u529e\u6cd5\u7528\u4e2a\u9c9c\u8089\u6765\u5bf9\u4f69\u6069\u59d1\u5a18\u5b89\u5168\u7684\u8fdb\u884c \u4e00\u987f\u758f\u901a\u3002\u63d0\u5347\u5feb\u4e50\u7684\u5176\u5b83\u65b9\u6cd5\u6211\u90fd\u8bd5\u8fc7\u4e86\uff0c\u5305\u62ec\u5728\u7b26\u5408\u80fd\u529b\u7684\u5c97\u4f4d\u4e0a\u73ed\uff0c\u8bad\u7ec3\uff0c \u548c\u53bb\u5e7f\u64ad\u7535\u53f0\u5750\u5750\u3002\u4e5f\u5c31\u5e7f\u64ad\u7535\u53f0\u80fd\u8ba9\u4f69\u6069\u59d1\u5a18\u7684\u5feb\u4e50\u503c\u7f13\u6162\u4e0a\u5347\u523092\uff0c\u4e0d\u80fd\u518d \u9ad8\u4e86\u3002</p> <p>\u4e5f\u8bb8\u5979\u5c31\u662f\u60f3\u8bd5\u4e00\u6b21\u4e91\u96e8\u7684\u6ecb\u5473\uff0c\u4e0d\u8fc7\u5979\u4e0d\u662f\u798f\u7984\u50e7\u5bb6\u65cf\u7684\u4eba\uff0c\u6015\u662f\u6ca1\u8fd9\u673a\u4f1a\u4e86\u3002 \u4e8e\u662f\u6211\u7684\u63a2\u7d22\u5c0f\u961f\u91cc\uff0c\u5c31\u8fd9\u4e48\u5e38\u9a7b\u4e86\u4e00\u4f4d\u6b32\u6c42\u4e0d\u6ee1\u7684\u59d1\u5a18\u3002</p>"},{"location":"writing/fallout/#_5","title":"\u8001\u4eba\u83b1\u6069","text":"<p>293\u57ce\u7b2c\u56db\u6cd5\u5219\uff1a\u5f00\u8352\u4fdd\u59c6\u5f97\u6c38\u751f</p> <p>\u7279\u522b\u63d0\u4e00\u4e0b\u8fd9\u4f4d\u5f00\u8352\u4fdd\u59c6\u3002\u5982\u679c\u4f60\u73a9\u8fc7\u706b\u7130\u4e4b\u7eb9\u7ae0\uff0c\u4ed6\u5c31\u7c7b\u4f3c\u4e8e\u9a6c\u5361\u65af\u4e4b\u7c7b\u51fa\u573a \u914d\u5907\u7684\u9ad8\u6570\u503c\u62a4\u9a7e\u89d2\u8272\u3002\u8001\u4eba\u83b1\u6069\u662f\u6211\u4ece\u65b0\u624b\u5b9d\u7bb1\u91cc\u5f00\u51fa\u6765\u7684\uff0c\u524d\u671f\u63a2\u7d22\u6361\u5783\u573e \u5168\u9760\u4ed6\u3002</p> <p>\u83b1\u6069\u6ca1\u6709\u8fdb\u8fc7\u5bdd\u5ba4\uff0c\u4e5f\u8bb8\u56e0\u4e3a\u592a\u5fd9\uff0c\u4e5f\u8bb8\u56e0\u4e3a\u4e0d\u4f1a\u8bf4\u751c\u8bdd\u3002\u4ee5\u4e0b\u6458\u6284\u4e00\u6bb5\uff1a</p> <p>\u4f69\u6069\u59d1\u5a18\uff1a\u201c\u8fd9\u8eab\u63a2\u7d22\u88c5\u5907\u662f\u4e0d\u662f\u8ba9\u6211\u663e\u80d6\uff1f\u201d</p> <p>\u8001\u4eba\u83b1\u6069\uff1a\u201c\u4e0d\uff0c\u8ba9\u4f60\u663e\u80d6\u7684\u662f\u4f60\u8eab\u4e0a\u7684\u8102\u80aa\u201d </p>"},{"location":"writing/fallout/#_6","title":"\u5f00\u8352\u707e\u96be","text":"<p>293\u57ce\u7b2c\u4e94\u6cd5\u5219\uff1a\u6ca1\u6709\u7528\u94b1\u89e3\u51b3\u4e0d\u4e86\u7684\u4e8b\uff0c\u5982\u679c\u6709\uff0c\u90a3\u662f\u4f60\u94b1\u4e0d\u591f</p> <p>293\u57ce\u7b2c\u516d\u6cd5\u5219\uff1a\u6b7b\u4ea1\u4e0d\u662f\u7ec8\u70b9\uff0c\u9886\u4e3b\u6ca1\u94b1\u8ba9\u4f60\u590d\u6d3b\u624d\u662f\u7ec8\u70b9\u3002</p> <p>\u8fd9\u4e2a\u4e16\u754c\u91cc\uff0c\u4eba\u6b7b\u4e86\u4f1a\u6709\u4e24\u4e2a\u9009\u9879\uff0c\u79fb\u9664\u6216\u8005\u590d\u6d3b\u3002\u590d\u6d3b\u6bd4\u8f83\u8d35\u3002</p> <p>\u5f00\u8352\u65f6\u671f\u6709\u8fc7\u4e00\u573a\u761f\u75ab\uff0c\u6a2a\u626b\u4e86\u6574\u4e2a\u57fa\u5730\uff0c\u6b7b\u4e86\u4e94\u4e2a\u5c45\u6c11\uff0c\u8d44\u6e90\u4e5f\u6d88\u8017\u7684\u89c1\u5e95\u4e86\u3002 \u672c\u4e16\u754c\u7684\u751f\u4ea7\u57fa\u7840\u662f\u80fd\u6e90\uff0c\u4f46\u662f\u4e24\u4e2a\u80fd\u6e90\u5382\u90fd\u6ca1\u6709\u4eba\u613f\u610f\u53bb\uff0c\u56e0\u4e3a\u91cc\u9762\u90fd\u6709\u5c38\u4f53\uff0c \u6362\u6211\u4e5f\u4e0d\u60f3\u53bb\u3002\u4e8e\u662f\u6574\u4e2a293\u57ce\u4e00\u7247\u9ed1\uff0c\u8981\u4e0d\u662f\u8003\u8651\u5230\u6709\u8001\u4eba\u83b1\u6069\u5728\uff0c\u6211\u90fd\u8981\u5f03 \u6863\u91cd\u5f00\u4e86\u3002</p> <p>\u4e00\u54ac\u7259\uff0c\u6162\u6162\u628a\u8fd9\u51e0\u4e2a\u7ed9\u590d\u6d3b\u4e86\u5427\u3002\u590d\u6d3b\u8981\u94b1\uff0c\u6ca1\u94b1\u600e\u4e48\u529e\uff1f\u51fa\u95e8\u6361\u5783\u573e\u3002\u90a3\u51e0 \u4e2a\u5acc\u6709\u5c38\u4f53\u4e0d\u613f\u610f\u5728\u80fd\u6e90\u5382\u5c31\u804c\u7684\uff0c\u7edf\u7edf\u6d3e\u51fa\u53bb\u5e9f\u571f\u6361\u5783\u573e\uff0c\u6361\u5230\u94b1\u5c31\u56de\u6765\uff0c\u653e \u4e0b\u94b1\u518d\u51fa\u53bb\uff0c\u5982\u6b64\u8f6e\u56de\u3002\u4e00\u6ce2\u64cd\u4f5c\u731b\u5982\u864e\uff0c\u7ec8\u4e8e\u590d\u6d3b\u4e86\u6b7b\u5728\u5176\u4e2d\u4e00\u4e2a\u80fd\u6e90\u5382\u7684\u6240 \u6709\u4eba\uff0c\u751f\u4ea7\u6062\u590d\u4e86\u3002\u6709\u4e86\u7535\uff0c\u6162\u6162\u4e5f\u5c31\u6062\u590d\u4e86\u6c34\u548c\u98df\u7269\u3002</p> <p>\u65e2\u7136\u5df2\u7ecf\u6062\u590d\u8fd0\u4f5c\uff0c\u770b\u4e86\u770b\u53e6\u4e00\u4e2a\u80fd\u6e90\u5382\u7684\u70c8\u58eb\uff0c\u597d\u50cf\u6570\u503c\u4e5f\u6ca1\u6709\u60ca\u8273\u7684\uff0c\u4e8e\u662f \u4e00\u72e0\u5fc3\u70b9\u4e86\u201c\u79fb\u9664\u201d\uff0c\u5bf9\u4e0d\u8d77\u9886\u4e3b\u6ca1\u94b1\u590d\u6d3b\u4f60\u4eec\u4e86\u3002\u597d\u4e86\uff0c\u5c38\u4f53\u6ca1\u4e86\uff0c\u5927\u5bb6\u6b22\u8fce\u6765 \u80fd\u6e90\u4e8c\u5382\u5c31\u804c\u3002</p> <p>\u53e6\u5916\uff0c\u590d\u6d3b\u82b1\u7684\u94b1\u8ddf\u7b49\u7ea7\u6709\u5173\u3002\u524d\u9762\u8bf4\u7684\u5f00\u8352\u4eba\u58eb\u5927\u90fd\u662f\u5341\u7ea7\u4e0d\u5230\u3002\u540e\u6765\u6709\u4e00\u6b21\uff0c \u6211\u628a\u8001\u4eba\u83b1\u6069\u5f04\u6b7b\u4e86\uff0c\u8d64\u8d2b\u7684\u6211\u8bb0\u4f4f\u4e86\u4ed6\u7684\u7b49\u7ea7\uff1a43\u3002</p>"},{"location":"writing/fallout/#_7","title":"\u98ce\u6c34","text":"<p>293\u907f\u96be\u6240\u7b2c\u516b\u6cd5\u5219\uff1a\u80fd\u591f\u8fde\u7eed\u62b5\u5fa1\u4e24\u6b21\u6b7b\u4ea1\u722a\u5165\u4fb5\u7684\u907f\u96be\u6240\uff0c\u624d\u662f\u5b89\u5168\u7684\u907f\u96be\u6240\u3002</p> <p>\u6b7b\u4ea1\u722a\uff1a\u907f\u96be\u6240\u5165\u4fb5\u8005\u91cc\u6700\u6050\u6016\u7684\u4e00\u79cd\uff0c\u5728\u67d0\u6b21\u66f4\u65b0\u5305\u91cc\u51fa\u73b0\u540e\uff0c\u8f7b\u5ea6\u73a9\u5bb6\u82e6\u4e0d \u582a\u8a00\uff0c\u786c\u6838\u73a9\u5bb6\u5927\u547c\u8fc7\u763e\u3002\u6b7b\u4ea1\u722a\u8840\u539a\uff0c\u80fd\u8fc5\u901f\u9020\u6210\u4f24\u5bb3\uff0c\u9020\u6210\u8f90\u5c04\uff0c\u751a\u81f3\u8fd8\u80fd \u541e\u566c\u7535\u529b\u8d44\u6e90\u3002\u5982\u679c\u6ca1\u6709\u51c6\u5907\u597d\uff0c\u6b7b\u4ea1\u722a\u8086\u8650\u8fc7\uff0c\u5c31\u8981\u7d27\u5df4\u5df4\u7684\u8fc7\u4e00\u6bb5\u9ed1\u6697\u65e5\u5b50 \u624d\u80fd\u7f13\u8fc7\u6765\u3002</p> <p>\u65b0\u624b\u518c\u5b50\u91cc\u6ca1\u6709\u63d0\u5230\u8fdb\u9636\u57ce\u5e02\u89c4\u5212\uff0c\u6240\u4ee5\u90a3\u573a\u5dee\u70b9\u5c60\u57ce\u7684\u761f\u75ab\u4e4b\u540e\uff0c\u6211\u706b\u901f\u8bf7\u4e86 \u4e00\u4e2a\u98ce\u6c34\u5927\u5e08\u3002\u6211\u95ee\u5927\u5e08\uff0c\u600e\u6837\u5b89\u7f6e\u5404\u5f0f\u5efa\u7b51\uff0c\u53ef\u4ee5\u6700\u5927\u7a0b\u5ea6\u7684\u62b5\u6297\u5404\u79cd\u4fb5\u7565\uff1f \u98ce\u6c34\u5927\u5e08\u7f13\u7f13\u9053\u6765\uff0c\u6218\u6597\u65f6\u51b3\u5b9a\u4f60\u6b66\u5668\u6263\u52a8\u9891\u7387\u7684\uff0c\u662f\u7075\u6d3b\u6027\uff08Agility\uff09\uff0c\u8d8a \u7075\u6d3b\u7684\u8d8a\u80fd\u7a81\u7a81\u7a81\u5c31\u628a\u654c\u4eba\u6253\u6b7b\u4e86\u3002\u800c\u53a8\u623f\u5de5\u4f5c\u7684\u4eba\uff0c\u6709\u7740\u6700\u9ad8\u7684\u7075\u6d3b\u6027\uff0c\u6bd5\u7adf \u8981\u8fc5\u901f\u5207\u83dc\u7206\u7092\u88c5\u76d8\u4ec0\u4e48\u7684\u3002</p> <p>\u4e8e\u662f\u6211\u5728\u907f\u96be\u6240\u5927\u95e8\u540e\u9762\u653e\u4e86\u4e00\u6e9c\u7684\u53a8\u623f\uff0c\u7ed9\u53a8\u5b50\u4eec\u7a7f\u4e0a\u6700\u7ed3\u5b9e\u7684\u8863\u670d\uff0c\u914d\u4e0a\u6700 \u731b\u529b\u7684\u6b66\u5668\uff0c\u8dc3\u8dc3\u6b32\u8bd5\u3002\u63a0\u593a\u8005\u6765\uff0c\u8d70\u4e0d\u4e86\u51e0\u6b65\u5c31\u5012\u4e0b\uff0c\u8001\u9f20\u87d1\u8782\u6765\uff0c\u51fa\u4e0d\u4e86\u5927 \u95e8\u5c31\u5168\u88ab\u706d\u56db\u5bb3\u3002\u6b7b\u4ea1\u722a\u6765\u2026\u5927\u4e8b\u4e0d\u5999\u4e86\uff0c\u67aa\u6253\u4e0a\u53bb\u5c31\u662f\u6320\u75d2\u75d2\uff0c\u8fb9\u6253\u6211\u8fb9\u8981 \u7ed9 \u6cbb\u7597\uff0c\u58a8\u8ff9\u4e00\u4f1a\u513f\uff0c\u6b7b\u4ea1\u722a\u5c31\u53bb\u4e0b\u4e00\u4e2a\u623f\u95f4\u4e86\uff0c\u7535\u529b\u4e5f\u635f\u5931\u60e8\u91cd\u3002\u7b2c\u4e00\u9053\u9632\u7ebf\u5931 \u8d25\u3002</p> <p>\u6211\u8dd1\u53bb\u8d28\u95ee\u98ce\u6c34\u5148\u751f\uff0c\u4ed6\u559d\u4e86\u53e3\u8336\u8bf4\uff0c\u7075\u6d3b\u662f\u7075\u6d3b\uff0c\u53ef\u662f\u529b\u91cf\u4e5f\u5f88\u91cd\u8981\u3002\u5c11\u800c\u7cbe\uff0c \u6253\u4e24\u4e0b\u5c31\u6253\u6b7b\u7684\u8bdd\uff0c\u5c31\u4e0d\u9700\u8981\u7a81\u7a81\u7a81\u7528\u6570\u91cf\u53d6\u80dc\u4e86\u3002\u98ce\u6c34\u5148\u751f\u63a5\u7740\u8bf4\uff0c\u5f88\u591a\u4eba\u628a \u7535\u529b\u5382\u653e\u7b2c\u4e00\u5c42\uff0c\u56e0\u4e3a\u7535\u529b\u5382\u4e13\u95e8\u62db\u5927\u529b\u58eb\u3002\u6211\u8bf4\uff0c\u4f60\u5012\u662f\u65e9\u8bf4\u554a\uff0c\u6211\u53a8\u623f\u90fd\u5f00 \u4e1a\u90a3\u4e48\u4e45\u4e86\u3002\u98ce\u6c34\u5148\u751f\u7b54\uff0c\u6211\u5f53\u65f6\u8bdd\u8bf4\u4e86\u4e00\u534a\u4f60\u5c31\u8dd1\u4e86\uff0c\u62e6\u4e5f\u62e6\u4e0d\u4f4f\u3002</p> <p>\u73b0\u5728\u600e\u4e48\u529e\uff1f\u62c6\u8fc1\uff1f\u522b\u60f3\u4e86\uff0c\u54ea\u91cc\u4ed8\u5f97\u8d77\u62c6\u8fc1\u8d39\u3002\u53ea\u80fd\u628a\u53a8\u5b50\u4eec\u7edf\u7edf\u9001\u53bb\u65b0\u4e1c\u65b9 \u529b\u91cf\u5b66\u6821\u6df1\u9020\u3002\u4e8e\u662f\u8fd9\u6bb5\u65f6\u95f4\uff0c\u6211\u6e38\u620f\u90fd\u4e0d\u6562\u4e45\u5f00\uff0c\u751f\u6015\u770b\u4e45\u5c31\u628a\u6b7b\u4ea1\u722a\u7ed9\u770b\u8fc7 \u6765\u4e86\u3002\u4e0a\u7ebf\u5c31\u6536\u4e00\u6ce2\u6280\u80fd\u70b9\uff0c\u628a\u53a8\u5b50\u4eec\u4ece\u53a8\u623f\u5230\u529b\u91cf\u5b66\u6821\u6765\u56de\u642c\u8fd0\u3002</p> <p>\u8fc7\u4e86\u597d\u4e00\u9635\u5b50\uff0c\u6211\u7684\u53a8\u5b50\u4eec\u5df2\u7ecf\u6bd4\u7535\u529b\u5382\u7684\u5de5\u4eba\u8fd8\u529b\u5927\u5982\u725b\u4e86\uff0c\u76d8\u5b50\u7aef\u7684\u53c8\u5feb\u53c8\u7a33\u3002 </p>"},{"location":"writing/fallout/#293","title":"\u6b22\u8fce\u6536\u542c293\u7535\u53f0","text":"<p>293\u57ce\u7b2c\u4e03\u6cd5\u5219\uff1a\u8d60\u4eba\u73ab\u7470\uff0c\u4e0d\u5982\u76f4\u63a5\u7ed9\u94b1</p> <p>\u74f6\u76d6\u513f\u5144\u5f1f\uff1a\u74f6\u76d6\u513f\u5144\u5f1f\u4e00\u4e2a\u53eb\u74f6\u513f\uff0c\u4e00\u4e2a\u53eb\u76d6\u513f\uff0c\u66fe\u7ecf\u88ab\u7ed1\u67b6\uff0c\u540e\u6765\u88ab\u4e00\u4e2a\u4efb \u52a1\u5206\u961f\u6551\u4e0b\u3002\u4ece\u90a3\u4e4b\u540e\uff0c\u4ed6\u4eec\u65f6\u4e0d\u65f6\u5149\u987e293\u907f\u96be\u6240\uff0c\u9644\u52a0\u4e34\u65f6\u6548\u679c\uff1a\u5168\u4f53\u5feb\u4e50 \u503c\u52a020\u3002\u56e0\u4e3a\u4ec0\u4e48\u5462\uff1f\u56e0\u4e3a\u4ed6\u4eec\u6492\u94b1\u554a\uff0c\u4ed6\u4eec\u6bcf\u5230\u4e00\u4e2a\u623f\u95f4\u90fd\u6709\u51e0\u7387\u8df3\u821e\uff0c\u4e00\u822c \u4eba\u8df3\u821e\u662f\u5411\u4f60\u8981\u8868\u6f14\u8d39\uff0c\u74f6\u76d6\u513f\u5144\u5f1f\u4e0d\uff0c\u4ed6\u4eec\u8d4f\u94b1\u4e70\u89c2\u4f17\u53bb\u770b\u3002\u6240\u4ee5\u4ed6\u4eec\u4e00\u6765\uff0c \u9886\u4e3b\u5c31\u8ffd\u7740\u4ed6\u4eec\u8dd1\uff0c\u4e00\u770b\u8df3\u821e\u4e86\u5c31\u8d76\u7d27\u70b9\u51fb\uff0c\u4e00\u70b9\u51fb\u94b1\u5c31\u54d7\u54d7\u7684\u5165\u8d26\u3002</p> <p>\u524d\u9762\u8bf4\u8fc7\u4e86\uff0c\u7535\u53f0\u5de5\u4f5c\u662f\u4e2a\u95f2\u804c\uff0c\u989c\u4e8c\u4ee3\u7684\u94c1\u996d\u7897\u3002\u798f\u7984\u50e7\u5bb6\u65cf\u7684\u5973\u6027\uff0c\u82e5\u9664\u4e86 \u9b45\u529b\u503c\u9ad8\u5916\u6beb\u65e0\u4eae\u70b9\uff0c\u5c31\u4f1a\u5e38\u5e74\u5728\u7535\u53f0\u5750\u73ed\u3002\u7535\u53f0\u6709\u5185\u5916\u4e24\u4e2a\u804c\u80fd\uff0c\u5bf9\u5916\u662f\u5411\u5e9f \u571f\u53d1\u7535\u62a5\u5438\u5f15\u4eba\u6765\u5165\u4f4f293\u907f\u96be\u6240\uff0c\u5bf9\u5185\u662f\u7ed9\u907f\u96be\u6240\u5c45\u6c11\u8bb2\u6545\u4e8b\u63d0\u9ad8\u6574\u4f53\u5feb\u4e50\u503c\u3002</p> <p>\u201c\u5e9f\u571f\u91cc\u8dcb\u6d89\u7684\u5e05\u54e5\u7f8e\u5973\u4f60\u4eec\u597d\uff0c\u6b22\u8fce\u6765\u770b\u770b293\u907f\u96be\u6240\uff0c\u987a\u4f4d\u5bee\u5168\u5929\u4e09buff\u4e0d\u65ad\uff0c \u6bcf\u5468\u793c\u5305\u53d1\u5230\u624b\u8f6f\uff0c\u8001\u53f8\u673a\u5e26\u4f60\u98de\u6e21\u771f\u86c7\uff0c\u5750\u6807\u662f45.62\uff0c23.44\u201d\u591c\u591c\u59d0\u5bf9\u7740\u5587 \u53ed\u543c\u3002</p> <p>\u201c\u522b\u4e22\u4eba\u4e86\uff0c\u81ea\u4ece\u6b7b\u4ea1\u722a\u5165\u4fb5\uff0c\u9886\u4e3b\u542c\u8bf4\u5bf9\u5916\u5e7f\u64ad\u7279\u522b\u62db\u6b7b\u4ea1\u722a\uff0c\u5c31\u5173\u4e86\u6574\u4e2a\u5bf9 \u5916\u9891\u6bb5\uff0c\u73b0\u5728\u53ea\u6709\u907f\u96be\u6240\u81ea\u5df1\u4eba\u80fd\u542c\u5230\u201d\u65e5\u65e5\u54e5\u4e0d\u8010\u70e6\u9053\u3002</p> <p>\u591c\u591c\u59d0\u548c\u65e5\u65e5\u54e5\u6253\u6210\u5e74\u8d77\u5c31\u5728\u7535\u53f0\u4e0a\u73ed\uff0c\u56e0\u4e3a\u4ed6\u4eec\u9664\u4e86\u9b45\u529b\u4e00\u65e0\u662f\u5904\uff0c\u6570\u503c\u592a\u5bd2 \u789c\u4e86\u3002\u90a3\u65f6\u5019\uff0c\u9886\u4e3b\u53f9\u7740\u6c14\u7ed9\u4ed6\u4eec\u4e00\u4eba\u914d\u4e86\u4e00\u5957\u6da85\u70b9\u9b45\u529b\u503c\u7684\u6781\u4e0a\u591c\u5e97\u88c5\uff0c\u5c31 \u6253\u53d1\u53bb\u7535\u53f0\u4e86\u3002</p> <p>-\u2014</p> <p>\u591c\u591c\u59d0\u548c\u65e5\u65e5\u54e5\u6700\u8fd1\u5f88\u70e6\u607c\uff0c\u56e0\u4e3a\u907f\u96be\u6240\u7684\u5feb\u4e50\u503c\u964d\u7684\u5389\u5bb3\u3002\u6700\u8fd1\u6b7b\u4ea1\u722a\u6765\u7684\u6709 \u70b9\u9891\u7e41\uff0c\u53a8\u623f\u7684\u5927\u529b\u53a8\u5b50\u6709\u4e9b\u62db\u67b6\u4e0d\u4f4f\uff0c\u6bcf\u5929\u8fc7\u7740\u6218\u6597\uff0c\u8865\u8840\uff0c\u6218\u6597\u7684\u65e5\u5b50\uff0c\u5076 \u5c14\u8fd8\u8981\u770b\u7740\u5c38\u4f53\u7b49\u9886\u4e3b\u6512\u94b1\u590d\u6d3b\u3002\u60f3\u60f3\u5c31\u53ef\u6015\uff0c\u6211\u90fd\u4e0d\u6562\u767b\u6e38\u620f\u4e86\u3002\u8fd9\u4fe9\u4eba\u667a\u529b \u867d\u7136\u4e0d\u9ad8\uff0c\u5374\u61c2\u5f97\u57fa\u672c\u4eba\u6027\uff1a\u770b\u4f60\u8fc7\u7684\u6bd4\u6211\u8fd8\u60e8\uff0c\u6211\u6574\u4e2a\u4eba\u5c31\u90fd\u597d\u4e86\u3002\u4e8e\u662f\u7814\u53d1 \u51fa\u4e86\u5e7f\u64ad\u65b0\u5957\u8def\uff1a\u7ed9\u53a8\u5b50\u4eec\u8bb2\u8bb2\u53a8\u623f\u5916\u9762\u7684\u60e8\u4e8b\u3002</p> <p>\u201c\u4ee5\u524d\u6709\u4e2a\u4efb\u52a1\u5206\u961f\u53eb\u2018\u5389\u5bb3\u4e86\u6211\u7684\u961f\u2019\uff0c\u6709\u6b21\u51fa\u53bb\u505a\u4efb\u52a1\u5fd8\u4e86\u5e26\u8840\u74f6\uff0c\u5f02\u60f3\u5929\u5f00 \u9760\u5347\u7ea7\u81ea\u52a8\u8865\u8840\u6765\u7eed\u547d\u3002\u7ed3\u679c\u626b\u8361\u4e86\u4e09\u56db\u4e2a\u5c4b\u5b50\u540e\u6491\u4e0d\u4f4f\u56e2\u706d\u4e86\uff0c\u9886\u4e3b\u5728\u907f\u96be\u6240 \u96c6\u8d44\u96c6\u4e86\u51e0\u5929\u624d\u591f\u94b1\u6551\u56de\u6765\uff0c\u5927\u5bb6\u8bf4\u4e22\u4eba\u4e0d\uff1f\u201d</p> <p>\u201c\u7136\u540e\u961f\u957f\u6211\u88ab\u514d\u804c\uff0c\u73b0\u5728\u5c31\u662f\u4e2a\u53a8\u5b50\u3002\u201d\u4e00\u4e2a\u6b63\u5728\u5207\u83dc\u7684\u53a8\u5b50\u6cae\u4e27\u5730\u8bf4\u3002</p> <p>\u201c\u4ee5\u524d\u6709\u8fc7\u4e00\u6b21\u761f\u75ab\uff0c\u4e24\u4e2a\u7535\u5382\u6b7b\u4e86\u4e94\u4e2a\u7535\u5de5\uff0c\u9886\u4e3b\u51d1\u94b1\u6551\u6d3b\u4e86\u4e00\u4e2a\u5382\u7684\uff0c\u53e6\u4e00 \u4e2a\u5382\u7684\u636e\u8bf4\u592a\u83dc\uff0c\u4e0d\u6551\u4e86\uff01\u201d</p> <p>\u201c\u5176\u4e2d\u4e00\u4e2a\u662f\u6211\u54e5\uff01\u201d\u4e00\u4e2a\u6b63\u5728\u7aef\u76d8\u5b50\u7684\u53a8\u5b50\u54ed\u5f97\u628a\u76d8\u5b50\u6454\u4e86</p> <p>\u4e00\u5929\u5929\u8fc7\u53bb\uff0c\u591c\u591c\u59d0\u770b\u4e86\u4e00\u773c\u5feb\u4e50\u699c\uff0c\u5fc3\u865a\u7684\u8bf4\uff1a\u201c\u8fd9\u4e2a\u8def\u5b50\u597d\u50cf\u4e0d\u53d7\u6b22\u8fce\uff0c\u8981 \u4e0d\u6211\u4eec\u8fd8\u662f\u56de\u5f52\u98ce\u82b1\u96ea\u6708\u8def\u7ebf\u5427\uff1f\u201d\u65e5\u65e5\u54e5\u4e0d\u7518\u5fc3\uff0c\u76ef\u7740\u770b\u4e86\u597d\u4e00\u4f1a\u513f\uff0c\u7a81\u7136\u5927 \u53eb\uff1a\u201c\u5947\u8ff9\u5237\u65b0\u4e86\uff01\u4f60\u770b\u8fd9\u4e0d\u662f\u90a3\u4e2a\u4e0d\u7231\u7b11\u7684\u8001\u5927\u96be\u4f69\u6069\u59d1\u5a18\u5417\uff1f\u5979\u5feb\u4e50\u503c\u77ac\u95f4 98\u4e86\uff01\u201d</p> <p>\u201c\u522b\u4e22\u4eba\u4e86\uff0c\u90a3\u662f\u56e0\u4e3a\u74f6\u76d6\u513f\u5144\u5f1f\u6765\u4e86\uff01\u5168\u4f53\u4e34\u65f6\u589e\u52a020\u5feb\u4e50\u503c!\u201d</p>"},{"location":"writing/fallout/#_8","title":"\u6b66\u5668\u5382\u7684\u5c0f\u738b","text":"<p>293\u907f\u96be\u6240\u7b2c\u4e5d\u6cd5\u5219\uff1a\u5987\u5973\u513f\u7ae5\u514d\u75ab\u4e00\u5207\u4f24\u5bb3\uff0c\u7537\u4eba\u8bf7\u81ea\u5f3a\u3002</p> <p>\u5c0f\u738b\u662f\u6b66\u5668\u5382\u7684\u8001\u677f\uff0c\u4e5f\u662f\u552f\u4e00\u5458\u5de5\u3002\u5c0f\u738b\u559c\u6b22\u5b89\u9759\u3002</p> <p>\u6b66\u5668\u5382\u5728293\u907f\u96be\u6240\u66fe\u7ecf\u662f\u4e2a\u53ef\u6709\u53ef\u65e0\u7684\u5730\u65b9\u3002\u6b66\u5668\u5382\u5efa\u597d\u7684\u51e0\u5929\u91cc\uff0c\u662f\u6ca1\u6709\u4eba \u5728\u91cc\u9762\u7684\uff0c\u51b7\u6e05\u5f97\u8fde\u8001\u9f20\u90fd\u8dd1\u5149\u4e86\u3002\u6b66\u5668\u5382\u4e4b\u6240\u4ee5\u5efa\u7acb\uff0c\u53ea\u662f\u56e0\u4e3a\u6709\u7740\u5f3a\u8feb\u75c7\u7684 \u9886\u4e3b\u521a\u89e3\u9501\u6b66\u5668\u5382\uff0c\u8981\u5145\u95e8\u9762\u5fc5\u987b\u9020\u4e00\u4e2a\u3002\u5efa\u5b8c\u53d1\u73b0\u53ea\u80fd\u4ea7\u666e\u901a\u6b66\u5668\uff0c\u90a3\u79cd\u5728\u5e9f \u571f\u6643\u4e00\u5708\u5c31\u80fd\u6361\u5230\u7684\u3002\u8981\u5347\u7ea7\u624d\u80fd\u9020\u7279\u6b8a\u6b66\u5668\uff0c\u53ef\u662f\u6ca1\u94b1\u5347\u7ea7\u3002\u5c0f\u738b\u5c31\u5728\u90a3\u65f6\u5019 \u9012\u4ea4\u4e86\u7b80\u5386\u3002</p> <p>\u540e\u6765\u6b66\u5668\u5382\u5347\u7ea7\u4e86\uff0c\u80fd\u9020\u5f3a\u529b\u6b66\u5668\u4e86\uff0c\u9886\u4e3b\u5374\u56e0\u4e3a\u4eba\u624b\u7d27\u5f20\uff0c\u6ca1\u6709\u7ed9\u6b66\u5668\u5382\u6269\u5458\uff0c \u4efb\u7531\u5c0f\u738b\u4e00\u4e2a\u4eba\u6162\u5de5\u51fa\u7ec6\u6d3b\u3002\u5c0f\u738b\u5077\u7740\u4e50\u7684\u6e05\u95f2\u3002</p> <p>\u6700\u8fd1\u9886\u4e3b\u5f97\u77e5\u6700\u9ad8\u7ea7\u7684\u5efa\u7b51\u8981\u4eba\u53e3\u5230100\u624d\u89e3\u9501\uff0c\u5f53\u524d\u4eba\u53e380\uff0c\u5c31\u62db\u52df\u4e86\u4e00\u6279\u798f \u7984\u50e7\u5bb6\u65cf\u7684\u5973\u6027\u53bb\u51fa\u751f\u4ea7\u529b\u3002\u4ee5\u524d\u8bf4\u8fc7\uff0c\u9886\u4e3b\u5e73\u65f6\u559c\u6b22\u628a\u5b55\u5987\u6d3e\u53bb\u7535\u53f0\u5de5\u4f5c\u3002\u5927 \u8dc3\u8fdb\u9020\u5c31\u4e86\u5f88\u591a\u5b55\u5987\uff0c\u7535\u53f0\u88c5\u4e0d\u4e0b\uff0c\u9886\u4e3b\u5c31\u628a\u76ee\u5149\u6295\u5411\u4e86\u6b66\u5668\u5382\u3002</p> <p>\u6709\u4e00\u5929\u5c0f\u738b\u4e0a\u73ed\uff0c\u770b\u89c1\u4e94\u4e2a\u65b0\u5458\u5de5\uff0c\u5927\u7740\u809a\u5b50\u8c08\u751f\u4ea7\u7ecf\uff0c\u80b2\u513f\u7ecf\uff0c\u5c31\u79d2\u61c2\u5916\u9762\u7684 \u65b0\u653f\u7b56\u65b0\u9700\u6c42\u4e86\u3002\u6b66\u5668\u5382\u9700\u8981\u7684\u80fd\u529b\u662f\u667a\u529b\uff0c\u5c0f\u738b\u624b\u65e0\u7f1a\u9e21\u4e4b\u529b\uff0c\u4f46\u8111\u5b50\u5f88\u597d\u4f7f\u3002 \u8111\u5b50\u597d\u4f7f\u7684\u5c0f\u738b\u7a81\u7136\u60f3\u8d77\u81ea\u5df1\u4e0d\u662f\u798f\u7984\u50e7\u5bb6\u65cf\u7684\u4eba\uff0c\u8ddf\u8fd9\u4e94\u4e2a\u59d1\u5a18\u90fd\u53ef\u4ee5\u5408\u6cd5\u914d \u5bf9\uff0c\u4e8e\u662f\u5fc3\u91cc\u4e00\u559c\uff0c\u4e0b\u73ed\u540e\u53c8\u7ed9\u9886\u4e3b\u9012\u4e86\u5c01\u81ea\u544a\u594b\u52c7\u6dfb\u65b0\u4e01\u7684\u4fe1\u3002</p> <p>\u201c\u4e00\u7ea7\u8b66\u62a5\uff01\u53d8\u5f02\u87d1\u8782\u6765\u88ad\uff0c\u8fd9\u4e0d\u662f\u6f14\u4e60\uff01\u53d8\u5f02\u87d1\u8782\u6765\u88ad\uff0c\u8fd9\u4e0d\u662f\u6f14\u4e60\uff01\u201d\u7b2c\u4e8c\u5929 \u5c0f\u738b\u521a\u5230\u5355\u4f4d\uff0c\u5c31\u542c\u5230\u5587\u53ed\u91cc\u653e\u8fd9\u4e2a\u3002</p> <p>\u201c\u8fd9\u600e\u4e48\u56de\u4e8b\uff0c\u6b66\u5668\u5382\u4e00\u5411\u5f88\u5b89\u5168\u554a\uff1f\u201d\u5c0f\u738b\u614c\u4e86\u3002</p> <p>\u5b55\u5987\u5c0f\u7ea2\u7b11\u7740\u8bf4\uff1a\u201c\u87d1\u8782\u8001\u9f20\u4ec0\u4e48\u7684\uff0c\u968f\u673a\u51fa\u73b0\u51e0\u7387\u8ddf\u623f\u95f4\u4eba\u6570\u6210\u6b63\u6bd4\uff0c\u4f60\u770b\u6211 \u4eec\u5382\u7ea2\u7ea2\u706b\u706b\u7684\u3002\u201d</p> <p>\u9ad8\u667a\u529b\u4f4e\u6b66\u529b\u7684\u5c0f\u738b\u5b9a\u4e86\u5b9a\u795e\uff0c\u5927\u558a\u4e00\u58f0\u201c\u4e0d\u614c\uff0c\u6211\u4eec\u4eba\u591a\u529b\u91cf\u2026\"</p> <p>\u7136\u540e\u5c0f\u738b\u611f\u53d7\u5230\u4e86\u4e00\u9635\u98ce\uff0c\u98ce\u91cc\u98d8\u7740\u4e94\u4e2a\u5b55\u5987\uff0c\u5411\u5927\u95e8\u65b9\u5411\u98d8\u53bb\u3002</p> <p>\u8fd8\u662f\u90a3\u4e2a\u59a9\u5a9a\u7684\u5c0f\u7ea2\uff0c\u7b11\u7740\u8bf4\u9053\uff0c\u201c\u4f60\u65b0\u53f8\u673a\u5427\uff0c\u5b55\u5987\u548c\u513f\u7ae5\u514d\u75ab\u4e00\u5207\u4f24\u5bb3\uff0c\u4f1a \u88ab\u7cfb\u7edf\u81ea\u52a8\u8fd0\u8f93\u5230\u5b89\u5168\u7684\u5730\u65b9\u3002\u4f60\u522b\u6015\uff0c\u7b49\u6211\u4eec\u4e94\u4e2a\u56de\u6765\uff0c\u4e00\u8d77\u96c6\u8d44\u628a\u4f60\u590d\u6d3b\u3002\u201d</p> <p>\u540e\u6765\u5fc3\u60c5\u597d\u4e86\uff0c\u5199\u4e0d\u4e0b\u53bb\u4e86</p>"},{"location":"writing/game_ost/","title":"\u6e38\u620fOST\u6d41\u6c34\u8d26","text":"<p>\u4ee5\u4e0b\u4e13\u8f91\u90fd\u5728bandcamp\u4e70\u4e86\uff0c \u652f\u6301\u5c0f\u4f5c\u574a\u97f3\u4e50\u4eba\u3002\u6211\u517b\u6210\u4e86\u4e00\u4e2a\u5c0f\u4e60\u60ef\uff0c\u5c31\u662f\u542c\u6b4c\u542c\u6574\u4e2a\u4e13\u8f91\uff0c\u4e0d\u90a3\u4e48\u559c\u6b22\u7684 \u4e5f\u4e0d\u8df3\u8fc7\u53bb\uff08\u9664\u975e\u592a\u5435\uff09\u7ee7\u7eed\u542c\u5b8c\uff0c\u4e3a\u4e86\u589e\u52a0\u5bf9\u4f5c\u8005\u7684\u7406\u89e3\u3002</p>"},{"location":"writing/game_ost/#chicory-a-colorful-tale-original-soundtrack-lena-raine","title":"Chicory: A Colorful Tale (Original Soundtrack) - Lena Raine","text":"<p>\u795e\u7b14\u72d7\u826f\uff08\u6c11\u95f4\u7ffb\u8bd1\uff09\u7684\u539f\u58f0\u3002\u542c\u4e86\u539f\u58f0\u53bb\u73a9\u6e38\u620f\uff0c\u53d1\u73b0\u6e38\u620f\u4e0a\u624b\u4e0d\u5bb9\u6613\uff0c\u9065\u611f \u64cd\u4f5c\u753b\u7b14\u529d\u9000\u3002\u6cbb\u6108\u7cfb\uff0c\u5bf9\u6211\u6765\u8bf4\u6e38\u620f\u8282\u594f\u592a\u6162\u4e86\uff0c\u4efb\u52a1\u63d0\u793a\u53c8\u4e0d\u8db3\u3002\u5c31\u5f53\u4f5c\u82b1 \u94b1\u8868\u652f\u6301\u597d\u4e86\u3002\u8fd9\u4e2a\u4e13\u8f91\u5947\u5999\u4e4b\u5904\u5728\u4e8e\uff0c\u6211\u4e00\u822c\u7f3a\u4e4f\u8010\u5fc3\u6b23\u8d4f\u6162\u8282\u594f\u7684\u97f3\u4e50\uff0c\u4f46 \u8fd9\u91cc\u9762\u7684\u6bcf\u4e00\u9996\u6211\u90fd\u559c\u6b22\uff0c\u53ef\u80fd\u662f\u56e0\u4e3a\u8282\u594f\u867d\u6162\uff0c\u5143\u7d20\u5374\u5f88\u591a\u3002</p> <p>Eyes in the Darkness: \u6253boss\u7684\u914d\u4e50\uff0cceleste\u91cc\u5408\u6210\u5668\u7684\u97f3\u8272\uff0c\u4e00\u542c\u5c31\u662f Lena\u7279\u8272\uff0c\u50cfconfronting myself\u548cfarewell\u7684\u7ed3\u5408\u3002\u5c42\u5c42\u53e0\u52a0\uff0c\u8ddf\u6218\u6597\u8fdb\u5ea6\u5173 \u8054\uff0c\u53ef\u60dc\u6211\u6ca1\u73a9\u5230\u8fd9\u90e8\u5206\u3002\u9057\u61be\u8fd9\u9996\u592a\u77ed\u4e86\uff0c\u611f\u89c9\u8fd9\u4e2a\u6545\u4e8b\u53ef\u4ee5\u8bb2\u4e45\u4e00\u70b9\u3002\u542c\u7740 \u6211\u8fd8\u5192\u51fa\u4e86\u4e2a\u60f3\u6cd5\uff0c\u8fd9\u4e2aboss\u6211\u4e0d\u60f3\u6253\uff0c\u8fd9\u97f3\u8272\u4e00\u542c\u5c31\u5f88\u96be\u6253\uff0c\u81f3\u5c11\u5728Celeste \u90e8\u5206\u8fd9\u4e2a\u97f3\u8272\u5bf9\u5e94\u7684\u90e8\u5206\u6b7b\u4ea1\u7387\u90fd\u5f88\u9ad8\u3002</p>"},{"location":"writing/game_ost/#chicory-the-sounds-of-picnic-province-lena-raine","title":"Chicory: The Sounds of Picnic Province - Lena Raine","text":"<p>\u5f88\u6e05\u6de1\uff0c\u5f88\u6162\uff0c\u518d\u6162\u5c31\u6ca1\u6cd5\u6b23\u8d4f\u4e86\u3002</p>"},{"location":"writing/game_ost/#celeste-original-soundtrack-lena-raine","title":"Celeste Original Soundtrack - Lena Raine","text":"<p>\u6e38\u620f\u8fdb\u5ea6\u8ddf\u97f3\u4e50\u7684\u7ed1\u5b9a\u82b1\u4e86\u5f88\u591a\u5fc3\u601d\uff0c\u4e3a\u4e86\u6c89\u6d78\u5f0f\u6b23\u8d4f\uff0c\u5efa\u8bae\u6253\u6162\u70b9\uff0c\u591a\u6b7b\u51e0\u767e \u51e0\u5343\u6b21\u3002</p> <p>Resurrection\uff1a9\u5206\u949f\u7684\u957f\u5ea6\uff0c\u65e2\u4fdd\u7559\u4e86\u6545\u4e8b\u7684\u5b8c\u6574\u6027\u53c8\u5168\u7a0b\u6709\u8db3\u591f\u800c\u53d8\u5316\u4e0d\u81f3 \u4e8e\u5ba1\u7f8e\u75b2\u52b3\u3002</p> <p>Scatter and lost\uff1a\u6e38\u620f\u8fdb\u5ea6\u63a8\u4e00\u4e2a\u8282\u70b9\uff0c\u5c31\u591a\u52a0\u4e00\u5c42\u9f13\u70b9\u3002</p>"},{"location":"writing/game_ost/#celeste-farewell-original-soundtrack-lena-raine","title":"Celeste: Farewell (Original Soundtrack) - Lena Raine","text":"<p>\u514d\u8d39DLC\uff0c\u826f\u5fc3\u5236\u4f5c\u3002OST\u5982\u6b64\u4f18\u7f8e\uff0c\u5982\u6b64\u4f24\u611f\u3002\u68a6\u91cc\u5bfb\u627e\u53bb\u4e16\u7684\u8001\u5976\u5976\uff0c\u5bf9\u6297\u56e0 \u4e3a\u8001\u5976\u5976\u53bb\u4e16\u800c\u89e6\u53d1\u7684\u6291\u90c1\u75c7\u3002Farewell\u7684\u6545\u4e8b\u6211\u5176\u5b9e\u5e76\u4e0d\u5171\u60c5\uff0c\u4f46\u4e0d\u77e5\u9053\u4e3a\u4ec0 \u4e48\u542cOST\u53c8\u89c9\u5f97\u8fd9\u79cd\u60c5\u7eea\u6709\u719f\u6089\u7684\u5473\u9053\uff0c\u5b83\u7ed3\u7ed3\u5b9e\u5b9e\u5730\u6253\u52a8\u4e86\u6211\u3002\u8fd9\u4e00\u5b9a\uff0c\u662f\u88ab \u8fd9\u4e2aDLC\u96be\u5ea6\u6298\u78e8\u51fa\u6765\u7684\u4f24\u611f\u3002\u8fd9\u79cd\u4f24\u611f\u662f\u4f18\u96c5\u7684\uff0c\u4e0d\u662f\u90a3\u79cd\u4e0d\u5f00\u5fc3\u4e86\u5c31\u4e0d\u60f3\u8d77 \u5e8a\u7684\u60b2\u4f24\uff0c\u800c\u662f\u542c\u5b8c\u60f3\u8ba9\u4eba\u53bb\u627f\u8f7d\u7740\u9057\u61be\u8bb0\u5fc6\u53bb\u52aa\u529b\u751f\u6d3b\u7684\u60b2\u4f24\u3002</p> <p>Farewell\uff08\u6b4c\uff09\uff1a\u6211\u5386\u7ecf\u5343\u8f9b\u4e07\u82e6\u5265\u8783\u87f9\u5c31\u662f\u4e3a\u4e86\u8fd9\u53e3\u918b\u3002\u8fd9\u9996\u914d\u5408\u6e38\u620f\uff0c\u6bcf\u6b21 \u542c\u90fd\u88ab\u4f1a\u5fc3\u4e00\u51fb\uff0c\u5c24\u5176\u662f\u5c0f\u63d0\u7434\u90e8\u5206\u3002\u901a\u5173\u524d\u540e\u542c\uff0c\u4f53\u9a8c\u5b8c\u5168\u4e0d\u4e00\u6837\u3002\u7531\u4e8e\u6b4c\u968f \u7740\u6e38\u620f\u63a8\u8fdb\uff0c\u6bcf\u4e2a\u7247\u6bb5\u6211\u4e00\u542c\u5c31\u77e5\u9053\u6709\u54ea\u4e9b\u6b7b\u6cd5\uff0c\u6bd5\u7adf\u6b7b\u4e867000\u6b21\u3002\u6bd4\u5982\u8fd9\u91cc\u56e0 \u4e3a\u6c34\u6bcd\u6ca1\u62ff\u597d\u6454\u6b7b\u4e86\uff0c\u8fd9\u91cc\u89e6\u7535\u4e86\uff0c\u8fd9\u91cc\u6ca1\u7eed\u4e0a\u4f53\u529b\uff0c\u8fd9\u91cc\u7ec8\u4e8e\u8d77\u98de\u53ef\u4ee5\u5598\u53e3\u6c14\uff0c \u5feb\u5230\u4e86\u592a\u6fc0\u52a8\u4eba\u5fc3\u4e86\uff0c\u624b\u5fc3\u5168\u662f\u6c57\u4e86\uff0c\u7ec8\u4e8e\u89c1\u5230\u8001\u5976\u5976\u4e86\uff0c\u6bcf\u4e2a\u6b4c\u66f2\u53d8\u5316\u7684\u8282\u70b9 \u90fd\u80fd\u5bf9\u5e94\u4e0a\u3002\u4e00\u5207\u90fd\u7ed3\u675f\u540e\uff0c\uff08\u5927\u610f\uff09\uff1a</p> <p>Madeline\uff1a\u7ec8\u4e8e\u89c1\u5230\u4f60\u4e86\uff0c\u53ef\u60dc\u662f\u5728\u68a6\u91cc\u3002</p> <p>\u8001\u5976\u5976\uff1a\u4f46\u4f60\u8fd8\u662f\u8d39\u8001\u5927\u52b2\u6765\u4e86\uff0c\u6709\u8bdd\u5feb\u8bf4\u3002</p> <p>Madeline \u5bf9\u4e0d\u8d77\u6ca1\u6709\u53c2\u52a0\u4f60\u7684\u846c\u793c\uff0c\u4f60\u7684\u6b7b\u8ba9\u6211\u5d29\u6e83\u4e86\u3002</p> <p>\u8001\u5976\u5976\uff1a\u6211\u90fd\u6b7b\u4e86\u8fd8\u5728\u4e4e\u8fd9\u4e2a\uff1f\u846c\u793c\u662f\u7ed9\u60f3\u5ff5\u6211\u7684\u7b28\u86cb\u4eec\u51c6\u5907\u7684\u3002</p> <p>Madeline\uff1a\u8c22\u8c22\u4f60\uff0c\u6211\u8be5\u600e\u4e48\u529e\uff0c\u50cf\u4ec0\u4e48\u4e8b\u4e5f\u6ca1\u53d1\u751f\u8fc7\u4e00\u6837\u7ee7\u7eed\u6d3b\u4e0b\u53bb\u5417\uff1f</p> <p>\u7136\u540e\u5c31\u9192\u4e86\u3002\u8fd9\u4e2a\u68a6\u8ba9\u5979\u9f13\u8d77\u52c7\u6c14\u8d70\u4e86\u51fa\u6765\uff0c\u8ddfTheo\u804a\u8fc7\u4e4b\u540e\u7ee7\u7eed\u6d3b\u4e0b\u53bb\u4e86\u3002 \uff08\u756a\u5916\uff09\u751a\u81f3\u53d1\u5c55\u4e86\u65b0\u7684\u7231\u597d\uff0c\u8fd8\u88abtheo\u64ae\u5408\u7740\u8ddf\u53bb\u4ed6\u59b9\u59b9\u7ea6\u4f1a\u4e86\u3002</p> <p>\u582a\u79f0\u5b8c\u7f8e\u7684\u53e5\u53f7\uff0c\u6df1\u5165\u63a2\u8ba8\u6291\u90c1\u75c7\u4f46\u662f\u6ca1\u6709\u8bf4\u6559\uff0c\u6ca1\u6709\u9760\u7c89\u9970\u5374\u8fbe\u6210\u4e86\u87ba\u65cb\u5f0f\u4e0a \u5347\u7684\u52b1\u5fd7\uff0c\u65e0\u8bba\u662f\u6b63\u7247\u8fd8\u662fdlc\uff0c\u5973\u4e3b\u6700\u540e\u90fd\u8d70\u51fa\u6765\u4e86\u3002\u4e00\u4e2a\u6291\u90c1\u75c7\u5199\u7684\u7528\u6765\u6cbb \u6108\u6291\u90c1\u75c7\u7684\u6e38\u620f\uff0c\u592a\u7ed9\u529b\u4e86\u3002</p>"},{"location":"writing/game_ost/#oneknowing-lena-raine","title":"Oneknowing - Lena Raine","text":"<p>\u521b\u4f5c\u80cc\u666f\uff1a\u7531\u4e8eCeleste\u9884\u6599\u4e4b\u5916\u800c\u7206\u706b\uff0c\u8ba9Lena\u8fdb\u5165\u4e86\u805a\u5149\u706f\u4e4b\u4e0b\u5f88\u4e45\u7f13\u4e0d\u8fc7 \u6765\uff0c\u4e8e\u662f\u5e72\u8106\u7ed9\u81ea\u5df1\u5199\u6cbb\u6108\u6b4c\uff0c\u6512\u8d77\u6765\u5c31\u662f\u8fd9\u5f20\u4e13\u8f91\uff0c\u6240\u4ee5\u5979\u5bf9\u5b83\u5f88\u6709\u7279\u6b8a\u60c5\u611f\u3002 \u8fd9\u4e5f\u662f\u5979\u51fa\u7248\u7684\u7b2c\u4e00\u5f20\u8ddf\u6e38\u620f\u65e0\u5173\u7684\u4e13\u8f91\uff0c\u6240\u4ee5\u5979\u628a\u5b83\u770b\u4f5c\u662f\u51fa\u9053\u4f5c\u3002\u8282\u594f\u771f \u7684...\u8d85\u7ea7\u6162\uff0c\u6cbb\u6108\u7cfb\u662f\u6ca1\u9519\u4e86\uff0c\u5bf9\u6211\u6765\u8bf4\u518d\u6162\u4e00\u70b9\u5c31\u542c\u4e0d\u4e0b\u53bb\u4e86\u3002\u6211\u7ecf\u5e38\u53bb\u542c Tsukuyomi\uff0c\u662f\u7528Vocaloid\u6a21\u62df\u51fa\u6765\u7684Lena\u5e74\u5c11\u65f6\u671f\u7684\u58f0\u97f3\u3002\u6211\u5f88\u671f\u5f85\u5979\u5565\u65f6\u5019 \u80fd\u771f\u7684\u5531\u4e00\u4e2a\u3002</p>"},{"location":"writing/game_ost/#celeste-b-sides-various-artists","title":"Celeste B-Sides - Various Artists","text":"<p>\u6e38\u620fb\u9762\u7684\u80cc\u666f\u97f3\u4e50\u6765\u81ea\u4e8e\u7c89\u4e1d\u4eec\u5728\u539f\u58f0\u57fa\u7840\u4e0a\u7684\u91cd\u65b0\u7f16\u6392\uff0c\u4e5f\u6709\u4e24\u9996\u662fLena\u81ea \u5df1\u5199\u7684\u3002</p> <p>Mirror Temple\uff1a\u7235\u58eb\u98ce\uff0c\u6700\u53d7\u597d\u8bc4\u7684\u4e00\u9996\u3002\u5728\u6e38\u620f\u91cc\u642d\u914d\u897f\u5965\u7684\u6c99\u96d5\u53f0\u8bcd\uff0c \u5c31\u66f4\u6709\u610f\u601d\u4e86\u3002\u5728C\u9762\u91cc\uff0c\u5219\u662f\u4e3a\u83dc\u9e21\u51cf\u8f7b\u4e86wallbounce\u4e0d\u505c\u5931\u8d25\u7684\u75db\u82e6\u3002</p> <p>Area 1 Demo: \u5047\u88c5Celeste\u662f\u4e2a\u6109\u60a6\u7684\u6e38\u620f\u3002</p> <p>Summit\uff1aLena\u62ab\u7740\u9a6c\u7532\u5199\u7684\u3002\u6211\u6700\u4e0d\u559c\u6b22\u7684\u4e00\u9996\uff0c\u975e\u5e38\u5355\u8c03\u3002\u8fd9\u4e2a\u53cd\u9762\u4f8b\u5b50\u8bf4\u660e \u4e86\u5bf9\u4e8e\u6b7b\u5f88\u591a\u6b21\u7684\u6e38\u620f\u6765\u8bf4\uff0c\u597d\u7684\u97f3\u4e50\u6709\u591a\u91cd\u8981\u3002\u6211\u5dee\u70b9\u653e\u5f037c\u6709\u5927\u90e8\u5206\u539f\u56e0\u662f \u56e0\u4e3a\u8fd9\u9996\u6b4c\u4e0d\u597d\u542c\uff0c\u6211\u4e0d\u60f3\u65e0\u9650\u6b7b\u5faa\u73af\u3002</p>"},{"location":"writing/game_ost/#celeste-madelines-grab-bag-lena-raine","title":"Celeste - Madeline's Grab Bag - Lena Raine","text":"<p>B,C\u9762\u8282\u594f\u65b9\u5757\u5173\u5361\u7684\u80cc\u666f\u97f3\u4e50\u3002\u5355\u72ec\u62ff\u51fa\u6765\u542c\u4e2d\u89c4\u4e2d\u77e9\uff0c\u4e0d\u63a8\u8350\u3002</p>"},{"location":"writing/game_ost/#beastieball-original-sound-track-lena-raine","title":"BEASTIEBALL Original Sound Track - Lena Raine","text":"<p>\u6211\u6709\u70b9\u6000\u7591\u8fd9\u624d\u662fLena\u7684\u672c\u8272\u6f14\u51fa\u3002\u5f88\u591a\u9996\u90fd\u662f\u53ef\u4ee5\u54fc\u51fa\u6765\u7684\u5c0f\u66f2\u513f\u3002</p> <p>VS. The Rutile All-Stars\uff1a\u5982\u679c\u8bf4\u54ea\u9996\u6b4c\u80fd\u8ba9\u6211\u53bb\u4e70\u8fd9\u4e2a\u6e38\u620f\uff0c\u90a3\u5c31\u662f\u8fd9\u9996\u3002 \u7b80\u5355\u4f46\u662f\u597d\u542c\u3002\u4e2d\u95f4\u94a2\u7434\u7684\u5c0f\u66f2\u513f\u5f88\u597d\u542c\u3002</p>"},{"location":"writing/game_ost/#into-the-breach-soundtrack-ben-prunty","title":"Into the Breach Soundtrack - Ben Prunty","text":"<p>\u8eab\u4e34\u5176\u5883\u611f\u5341\u8db3\uff0c\u542c\u5230\u6000\u7591\u5730\u7403</p>"},{"location":"writing/game_ost/#crypt-of-the-necrodancer-ost-danny-baranowsky","title":"Crypt of the Necrodancer OST - Danny Baranowsky","text":"<p>\u8282\u594f\u5730\u7262\u539f\u58f0\uff0c\u975e\u5e38\u5bf9\u5f97\u8d77\u8282\u594f\u6e38\u620f\u7684\u540d\u53f7\u3002\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u6211\u4e0d\u559c\u6b22disco\u98ce\u683c \u7684\u4ece\u5934\u5230\u5c3e\u5361\u7b2c\u4e00\u62cd\u7684\u5047\u9f13\u70b9\uff0c\u53ef\u662f\u5728\u8fd9\u5f20\u4e13\u8f91\u91cc\u7adf\u7136\u6070\u5230\u597d\u5904\u3002\u76f4\u63a5\u5f53\u8282\u62cd\u5668 \u505a\u6253\u9f13\u57fa\u672c\u7ec3\u4e60\u4e86\u3002\u5f88\u7535\u5b50\u4f46\u662f\u97f3\u8272\u4e30\u5bcc\uff0c\u4e0b\u4e86\u529f\u592b\u3002</p> <p>\u5eb7\u52a0\u98ce\u683c\u7684\u90a3\u9996\u662f\u795e\u6765\u4e4b\u7b14\uff0c\u5728\u4e00\u8282\u594f\u6e38\u620f\u91cc\u52a0\u4e86\u5927\u91cf\u9759\u6b62\u7b26\u5374\u4e0d\u8fdd\u548c\u3002</p>"},{"location":"writing/game_ost/#yodel-eclipse-of-the-heart-nicolas-daoust-danny-baranowsky","title":"Yodel Eclipse of the Heart - Nicolas Daoust, Danny Baranowsky","text":"<p>\u4e13\u8f91\u540d\u5b57\u4e0d\u660e\u663e\uff0c\u8fd9\u662f\u8282\u594f\u5730\u7262\u539f\u58f0\u7684\u4e8c\u521b\u3002\u8282\u594f\u5730\u7262\u6709\u4e2a\u8bbe\u5b9a\uff0c\u5e97\u4e3b\u4f1a\u554a\u554a\u54e6 \u54e6\u56af\u56af\u7684\u8ddf\u7740\u97f3\u4e50\u5531\u6b4c\u5267\u98ce\uff0c\u79bb\u5546\u5e97\u8d8a\u8fd1\u542c\u5f97\u8d8a\u6e05\u695a\uff0c\u6240\u4ee5\u6bcf\u9996\u5173\u5361\u97f3\u4e50\u90fd\u6709\u5e97 \u4e3b\u7248\u672c\u3002\u539f\u58f0\u91cc\u7f8e\u4e2d\u4e0d\u8db3\u7684\u662f\uff0c\u5e97\u4e3b\u7684\u97f3\u6548\u592a\u5047\u4e86\uff0c\u4e00\u5230\u9ad8\u97f3\u5c31\u4e27\u5931\u4eba\u58f0\u6548\u679c\u3002 \u8fd9\u4e2a\u4e13\u8f91\u5f25\u8865\u4e86\u8fd9\u4e2a\u9057\u61be\uff0cNicolas\u5927\u4f6c\u628a\u5e97\u4e3b\u90e8\u5206\u81ea\u5df1\u7ed9\u5531\u4e86\uff01\u5f88\u4f4e\u5f88\u9ad8\u7684\u97f3 \u90fdhold\u4f4f\u4e86\u3002\u5176\u4e2d\u51e0\u9996\u6b4c\uff0c\u4e0d\u77e5\u9053\u4e3a\u5565\u542c\u5f97\u6b62\u4e0d\u4f4f\u5730\u6b22\u4e50\u3002</p>"},{"location":"writing/game_ost/#undertale-soundtrack-toby-fox","title":"UNDERTALE Soundtrack - Toby Fox","text":"<p>\u6211\u542c\u8fd9\u9996\u4e13\u8f91\u7684\u521d\u8877\u662f\uff0c\u53bb\u4e86\u89e3\u4e3a\u5565Toby Fox\u6709\u5f88\u591a\u7c89\u4e1d\uff0c\u5230\u5e95\u6709\u5565\u597d\u542c\u3002\u6162\u6162 \u542c\u51fa\u5473\u513f\u4e86\uff0c\u4e5f\u4e0d\u77e5\u9053\u662f\u771f\u7684\u597d\u542c\u8fd8\u662f\u81ea\u6211\u6697\u793a\u4e86\u3002\u8fd9\u5f20\u4e13\u8f91101\u9996\u6b4c\uff0c\u771f\u662f\u5565 \u90fd\u5f80\u91cc\u653e\u3002\u5145\u6ee1\u4e86\u7535\u5b50\u5473\u3001\u516b\u4f4d\u673a\u5473\uff0c\u8ba9\u4eba\u6000\u7591\u4e0b\u8f7d\u9ad8\u4fdd\u771f\u7248\u672c\u6709\u5565\u610f\u4e49\u3002</p>"},{"location":"writing/game_ost/#castlevania-kareshi","title":"Castlevania - kareshi","text":"<p>\u4e00\u5f20\u6734\u7d20\u7684\u4e13\u8f91\uff0c\u6076\u9b54\u57ce\u94a2\u7434\u4e8c\u521b\u3002\u4e0d\u6127\u662f\u94a2\u7434\uff0c\u594f\u51fa\u4e86\u5c42\u6b21\u611f\u3002\u6076\u9b54\u57ce\u7684\u5473\u513f \u6f14\u7ece\u5f97\u5f88\u5230\u4f4d\u3002\u4f5c\u8005\u6709\u4e24\u628a\u5237\u5b50\uff0c\u5728\u67d0\u6b21\u901f\u901a\u8868\u6f14\u65f6\u73b0\u573a\u914d\u4e50\uff0c\u6839\u636e\u901f\u901a\u73a9\u5bb6\u7684 \u6e38\u620f\u8fdb\u5ea6\u6f14\u594f\uff0c\u5173\u5361\u4e4b\u95f4\u5207\u6362\u4e1d\u6ed1\u3002\u6b64\u4e13\u8f91\u88ab\u4f5c\u8005\u4eb2\u624b\u8d60\u9001\u539f\u4f5c\u66f2\u5c71\u4e0b\u7d79\u4ee3\uff0c\u4f5c \u8005\u751a\u81f3\u5728\u8857\u5934\u6355\u6349\u5230\u4e86\u5728\u73a9\u8857\u8fb9\u94a2\u7434\u7684\u5c71\u4e0b\u7d79\u4ee3\uff0c\u5e76\u9080\u8bf7\u5979\u4eb2\u81ea\u6f14\u594fwicked child\u3002</p>"},{"location":"writing/game_ost/#hollow-knight-original-soundtrack-christopher-larkin","title":"Hollow Knight (Original Soundtrack) - Christopher Larkin","text":"<p>\u6b7bN\u6b21\u7684\u6e38\u620f\uff0c\u539f\u58f0\u5fc5\u987b\u8fc7\u786c\u3002Boss\u6218\u5c24\u5176\u5904\u7406\u7684\u5f88\u597d\uff0c\u6253\u51fb\u611f\u5341\u8db3\uff0c\u6211\u751a\u81f3\u89c9 \u5f97\u6218\u6597\u97f3\u4e50\u6bd4\u573a\u666f\u80cc\u666f\u4e50\u4f18\u79c0\u3002</p> <p>False Knight\uff1a\u521d\u89c1\u6740\u7b2c\u4e00\u4e2aboss\uff0c\u8fd9\u9996\u6b4c\u5f88\u6709\u52b2\u9053\uff0c\u8f85\u52a9\u627e\u5230\u51fa\u62db\u7684\u8282\u594f\u611f\u3002</p> <p>\u87b3\u8782\uff1a\u4f18\u96c5</p> <p>City of Tears\uff1a\u7528\u4e86\u771f\u8428\u514b\u65af\u98ce\u6f14\u594f\u3002\u6e38\u620f\u91cc\u542c\u7740\u5f88\u60ca\u8273\uff0c\u5355\u72ec\u62ff\u51fa\u6765\u53cd\u800c\u7f3a \u4e86\u70b9\u513f\u611f\u89c9\u3002\u914d\u5408\u8df3\u8df3\u780d\u780d\u4e50\u66f4\u4f73\u3002</p>"},{"location":"writing/indie_games/","title":"\u72ec\u7acb\u6e38\u620f\u6d41\u6c34\u8d26","text":"<p>\u57fa\u672c\u53ea\u8bb0\u5f55\u8001\u6e38\u620f\u6216\u8005\u72ec\u7acb\u6e38\u620f\u3002Celeste\u91cd\u65b0\u5851\u9020\u4e86\u6211\u5bf9\u6e38\u620f\u7684\u770b\u6cd5\uff1a3A\u5927\u4f5c \u7528\u8111\u7528\u6280\u672f\u505a\uff0c\u72ec\u7acb\u6e38\u620f\u638f\u5fc3\u505a\u3002\u786c\u5e7f\u544a\uff1aOST\u63a8\u8350\u53bbbandcamp\u4e70\u3002</p>"},{"location":"writing/indie_games/#into-the-breach","title":"Into the breach","text":"<p>\u975e\u5e38\u91cd\u6570\u503c\u7684\u672b\u4e16\u80cc\u666f\u6218\u65d7\u6e38\u620f\uff0c\u53cc\u53a8\u72c2\u559c\u3002\u6bcf\u4e2a\u5173\u5361\u4e00\u5b9a\u6709\u89e3\uff0c\u4f46\u540c\u65f6\u53ef\u4ee5\u4e00 \u6b65\u9519\u6b65\u6b65\u9519\uff0c\u5f88\u9002\u5408\u68cb\u7c7b\u5927\u4f6c\u6311\u6218\u3002\u5f31\u9e21\u6211\u4ece\u6765\u6ca1\u901a\u5173\u8fc7\u6700\u9ad8\u96be\u5ea6\uff0c\u4f46\u662f\u770b\u522b\u4eba \u73a9\u6700\u9ad8\u96be\u5ea6\u6ee1\u5206\u5c31\u6b63\u597d\u3002\u6e38\u620f\u5e76\u6ca1\u6709\u8be6\u7ec6\u7684\u60c5\u8282\uff0c\u53f0\u8bcd\u5f88\u5c11\u3002\u4f46\u662f\u4ece\u673a\u5e08\u7684\u53f0\u8bcd \u788e\u7247\u53ef\u4ee5\u62fc\u51d1\u51fa\u4ed6\u4eec\u7684\u8fc7\u5f80\u548c\u66fe\u7ecf\u7684\u5730\u7403\u3002\u6709\u4e9b\u5bf9\u8bdd\u8fd8\u633a\u6bd2\u820c\u3002OST\u597d\u8bc4\uff0c\u6bcf\u4e00 \u9996\u90fd\u8ba9\u4eba\u7f6e\u8eab\u672b\u4e16\u4e4b\u4e2d\uff0c\u8fde\u7eed\u542c\u5b8c\u4e00\u6574\u5f20\u4e13\u8f91\u5f97\u7f13\u534a\u5929\u3002</p>"},{"location":"writing/indie_games/#celeste","title":"Celeste\uff08\u851a\u84dd\uff09","text":"<p>\u9ad8\u96be\u5ea62D\u5e73\u53f0\u6e38\u620f\uff0c\u6b64\u751f\u6700\u7231\uff0c\u54ea\u6015\u6b7b\u4e86\u51e0\u5343\u6b21\u3002\u826f\u5fc3DLC\u767d\u9001\uff0c\u914d\u4e0aOST\u751a\u81f3\u7b49 \u4e8e\u9001\u4e86\u4e2a\u5168\u65b0\u6e38\u620f\uff0c\u9644\u8d60\u518d\u51e0\u5343\u6b21\u6b7b\u4ea1\u3002\u4e3b\u9898\u662f\u514b\u670d\u6291\u90c1\u75c7\uff0c\u57fa\u4e8e\u4e3b\u521b\u4e2a\u4eba\u7ecf\u5386\uff0c \u53d9\u4e8b\u771f\u8bda\u800c\u987a\u6ed1\u3002\u6d41\u4f20\u7684\u4e00\u4e2a\u6897\u662f\uff0c\u73a9celeste\u901a\u5173\u4f1a\u8ba9\u4f60\u53d8\u6210\u8de8\u6027\u522b\uff08\u5927\u8bef\uff09\uff0c \u867d\u8bf4\u662f\u8c03\u4f83\uff0c\u4f46\u5982\u679c\u4f1a\u56e0\u4e3a\u8de8\u6027\u522b\u5143\u7d20\u800c\u96be\u53d7\u7684\u8bdd\u786e\u5b9e\u4e0d\u63a8\u8350\u73a9\uff0c\u6bd5\u7adf\u4e3b\u521b\u548c\u97f3 \u4e50\u5236\u4f5c\u90fd\u662fmtf\u3002Celeste\u7684\u97f3\u4e50\u6211\u542c\u4e86\u65e0\u6570\u904d\uff0c\u5e76\u628a\u767d\u6bdb\u5c0f\u59d0\u59d0Lena Raine\u5c01\u795e \u4e86\uff0c\u591a\u91cd\u610f\u4e49\u4e0a\u7684\u3002\u6211\u552f\u4e00\u7684\u4e0d\u6ee1\u662f\uff0c\u5b83\u628a\u540c\u5e74\uff082018\uff09\u53d1\u5e03\u7684into the breach\u6bd4\u4e0b\u53bb\u4e86\uff0c\u5bfc\u81f4\u540e\u8005\u9519\u8fc7\u6700\u4f73\u72ec\u7acb\u6e38\u620f\u5956\u3002</p> <p>\u8fd9\u4e2a\u6e38\u620f\u7684\u8df3\u8df3\u4e50\u662f\u6211\u4f53\u9a8c\u8fc7\u7684\u6e38\u620f\u91cc\u6700\u7cbe\u81f4\u7684\uff0c\u6e38\u620f\u7ec4\u5bf9\u6e38\u620f\u673a\u5236\u7684\u7406\u89e3\u975e\u5e38 \u6df1\u523b\u3002\u624b\u6b8b\u515a\u6709\u624b\u6b8b\u515a\u7684\u73a9\u6cd5\uff0c\u901f\u901a\u6709\u901f\u901a\u7684\u73a9\u6cd5\uff0c\u901f\u901a\u751a\u81f3\u6709\u7279\u522b\u7684\u6280\u672f\u3002\u5176 \u5b83\u6e38\u620f\u6b7b\u4ea1\u662f\u60e9\u7f5a\uff0c\u851a\u84dd\u91cc\u6b7b\u4ea1\u662f\u722c\u5b66\u4e60\u66f2\u7ebf\uff0c\u5237\u808c\u8089\u8bb0\u5fc6\u3002\u62537c\u6700\u540e\u4e00\u5c4f \u7684\u65f6\u5019\uff0c\u6211\u672c\u6765\u60f3\u7740\u7528\u8f85\u52a9\u6a21\u5f0f\u5148\u7ec3\u4e60\uff0c\u4f46\u662f\u53d1\u73b0\u6ca1\u6cd5\u7ec3\uff0c\u78b0\u5c16\u523a\u4e0d\u6302\u6389\u5c31\u6ca1\u6cd5 \u81ea\u6211\u5fae\u8c03\u3002\u808c\u8089\u8bb0\u5fc6\u4e00\u65e6\u7ec3\u6210\uff0c\u518d\u5237\u540c\u4e00\u5173\u5c31\u5bb9\u6613\u591a\u4e86\uff0c\u6bd4\u5982farewell\u6700\u540e\u4e00\u5c4f \u6211\u6b7b\u4e86\u4e0a\u767e\u6b21\u624d\u8fc7\uff0c\u4f46\u662f\u8fc7\u5b8c\u4e4b\u540e\u518d\u53bb\u91cd\u590d\u4e00\u904d\u62ff\u6708\u8393\uff0c\u5c31\u53d1\u73b0\u90a3\u4e00\u5c4f\u6211\u6210\u529f\u7387 \u5df2\u7ecf\u8d85\u8fc710%\u4e86\u3002</p> <p>\u8bbe\u8ba1\u7684\u8fd9\u4e48\u96be\uff0c\u5236\u4f5c\u7ec4\u81ea\u5df1\u73a9\u5f97\u8fc7\u53bb\u5417\uff1f\u770b\u8fc7\u4ed6\u4eec\u73a9\u8349\u8393\u9171mod\u4e4b\u540e\u6211\u5c31\u653e\u4e0b\u8fd9 \u4e2a\u7591\u95ee\u4e86\uff0c\u597d\u5427\u786e\u5b9e\u90fd\u662f\u9ad8\u624b\u3002</p>"},{"location":"writing/indie_games/#advance-wars-by-web","title":"Advance wars by web","text":"<p>\u4e25\u683c\u6765\u8bf4\u4e0d\u7b97\u4e8c\u521b\uff0c\u4f46\u5b83\u628a\u9ad8\u7ea7\u6218\u4e89\u62d4\u9ad8\u6210\u4e86\u7535\u5b50\u7ade\u6280\uff0c\u524d\u6392\u9009\u624b\u91cc\u9762\u8c61\u68cb\u9ad8\u624b \u6d53\u5ea6\u5f88\u9ad8\u3002\u517b\u6d3b\u4e86\u597d\u51e0\u4e2a\u4e3b\u64ad\uff0c\u6709\u6280\u672f\u578b\u7684\u4e5f\u6709\u6574\u6d3b\u578b\u7684\u3002\u8fd9\u4e2a\u7f51\u9875\u7248\u7eafpvp\uff0c \u5185\u5bb9\u5f88\u4e30\u5bcc\uff0c\u6709\u5e38\u89c4\u6392\u4f4d\u8d5b\uff0c\u9526\u6807\u8d5b\uff0c\u591a\u4eba\u5408\u4f5c\uff0c\u591a\u4eba\u4e71\u6597\u3002\u65f6\u95f4\u8bbe\u7f6e\u5f88\u7075\u6d3b\uff0c \u53ef\u4ee5\u8bbe\u6210\u4e00\u5c40\u6162\u6162\u60f3\u51e0\u5929\uff0c\u4e5f\u53ef\u4ee5\u641e\u5373\u65f6\u6218\u3002\u591a\u4eba\u6218\u5f79\u6253\u534a\u5e74\u624d\u4e86\u7ed3\u90fd\u6b63\u5e38\uff0c\u7ecf \u5e38\u6253\u7740\u6253\u7740\u5c31\u6709\u4eba\u6ca1\u8010\u5fc3\u6295\u964d\u4e86\uff0c\u5269\u4e0b\u7684\u4eba\u62a2\u5360\u7a7a\u51fa\u6765\u7684\u8d44\u6e90\u7ee7\u7eed\u6253\u3002\u7531\u4e8e\u662f\u771f \u4eba\u6597\uff0c\u53ef\u4ee5\u8fb9\u6253\u8fb9\u53d1\u6d88\u606f\uff0c\u4e00\u822c\u6765\u8bf4\u7d20\u8d28\u90fd\u4e0d\u9519\uff0c\u4f46\u4e5f\u6709\u6c14\u6025\u8d25\u574f\u9a82\u4eba\u7684\u3002\u591a\u4eba \u8d5b\u91cc\u7684\u4e92\u52a8\u6bd4\u8f83\u590d\u6742\uff0c\u751a\u81f3\u6709\u6295\u673a\u5206\u5b50\u5047\u88c5\u7ed3\u76df\uff0c\u4e2d\u9014\u53db\u53d8\uff08\u6709\u62c9\u9ed1\u529f\u80fd\uff09\u3002</p>"},{"location":"writing/indie_games/#crypt-of-the-necrodancer","title":"Crypt of the Necrodancer \u8282\u594f\u5730\u7262","text":"<p>\u6bcf\u6b21\u770b\u5230\u8fd9\u4e2a\u7ffb\u8bd1\u5c31\u60f3\u5230\u5bf9\u6807\u8282\u594f\u5929\u56fd\u3002OST\u5f88\u5f3a\u5927\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u5065\u8eab\u80cc\u666f\u97f3\u4e50\u3002</p>"},{"location":"writing/indie_games/#_2","title":"\u7a7a\u6d1e\u9a91\u58eb\uff0c\u4e1d\u4e4b\u6b4c","text":"<p>\u7a7a\u6d1e\u9a91\u58eb\u4e0d\u8bf4\u4e86\uff0c\u706b\u51fa\u5708\u4e86\uff0c\u5e76\u4e14\u5e26\u52a8\u4e86\u8bb8\u591a\u65b0\u5165\u5751\u73a9\u5bb6\uff0c\u5bfc\u81f4\u4e1d\u4e4b\u6b4c\u5f97\u5230\u5f88\u591a \u6028\u6c14\u5dee\u8bc4\u3002</p> <p>\u4e1d\u4e4b\u6b4c\u603b\u4f53\u4f53\u9a8c\u975e\u5e38\u597d\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u6211\u7684\u671f\u5f85\uff0c\u4e0d\u8fc7\u5e76\u4e0d\u662f\u8bf4\u6ca1\u6709\u69fd\u70b9\u3002</p> <p>\u4e00\u6d41\u7684\u56fe\u50cf\u3002\u573a\u666f\u7ec6\u817b\uff0c\u866b\u5b50\u7684\u8bbe\u8ba1\u7b80\u6d01\u3001\u6d41\u5229\u3001\u53c8\u5bcc\u542b\u7279\u8272\u4e0d\u91cd\u6837\u3002</p> <p>\u4e00\u6d41\u7684\u6218\u6597\u3002\u8282\u594f\u548c\u6253\u51fb\u611f\u5341\u8db3</p> <p>\u4e00\u6d41\u7684\u97f3\u4e50\u3002\u6070\u5230\u597d\u5904</p> <p>\u4e00\u6d41\u7684\u97f3\u6548\u3002\u9759\u97f3\u6ca1\u6cd5\u73a9\uff0c\u53ef\u89c1\u97f3\u6548\u5bf9\u89e3\u8bfb\u654c\u65b9\u52a8\u4f5c\u7684\u91cd\u8981\u6027\u3002\u6253\u866b\u5b50\u560e\u5623\u8106\u7684 \u6548\u679c\u542c\u7740\u723d\uff0c\u866b\u9053\u91cc\u866b\u5b50\u94bb\u6765\u94bb\u53bb\u7684\u58f0\u97f3\u542c\u7740\u5934\u76ae\u53d1\u9ebb\u3002</p> <p>\u53ca\u516b\uff08\u4e0d\u662f\u9a82\u4eba\uff0c\u662f\u539f\u53f0\u8bcd\uff09\u6d41\u7684\u4e2d\u6587\u7ffb\u8bd1\uff0c\u53ea\u80fd\u8bf4\u6a31\u6843\u793e\u4e5f\u4e0d\u5bb9\u6613\u3002\u7b2c\u4e00\u4e2a\u8ba9 \u5207\u56de\u82f1\u6587\u7684\u6e38\u620f\u3002</p> <p>\u5c41\u6eda\u5c3f\u6d41\u7684\u5730\u56fe\u3002\u6a31\u6843\u793e\u8ddf\u5730\u56fe\u6709\u4ec7\uff0c\u81f4\u529b\u4e8e\u8ba9\u73a9\u5bb6\u80cc\u677f\u522b\u770b\u5730\u56fe\u3002\u6bcf\u4e2a\u533a\u90fd\u8981 \u5148\u627e\u5730\u56fe\u5546\u4eba\u4e70\u5730\u56fe\uff0c\u6307\u5357\u9488\u9700\u8981\u5355\u72ec\u5360\u4e00\u4e2a\u88c5\u5907\u4f4d\uff0c\u5426\u5219\u4e0d\u8ba9\u770b\u5f53\u524d\u4f4d\u7f6e\u3002\u5546 \u4eba\u4f4d\u7f6e\u5f88\u9690\u79d8\uff0c\u6211\u533a\u57dfboss\u90fd\u6253\u5b8c\u4e86\u624d\u4e70\u5230\u5730\u56fe\u3002\u6e38\u620f\u4e3a\u4ec0\u4e48\u8981\u8ba9\u6211\u590d\u73b0\u8def\u75f4\u4f53 \u9a8c\uff1f</p> <p>\u60ca\u8273  * \u5730\u56fe\u5927\uff0c\u7ba1\u9971  * \u591a\u79cd\u6218\u6597\u7eb9\u7ae0\uff0c\u5b8c\u5168\u4e0d\u4e00\u6837\u7684\u6218\u6597\u98ce\u683c\u548c\u4f53\u9a8c  * \u8dd1\u56fe\u7eb9\u7ae0\u4e0b\u5288\u5bb9\u9519\u5ea6\u975e\u5e38\u9ad8  * \u8df3\u8df3\u4e50\u7684\u591a\u6837\u6027  * \u4e00\u5982\u65e2\u5f80\u7684\u7ec6\u8282\uff0c\u770b\u5730\u56fe\u5f97\u6709\u62ff\u56fe\u7684\u52a8\u753b\uff0c\u6240\u4ee5\u6c34\u91cc\u6ca1\u6cd5\u770b\u5730\u56fe  * \u8682\u8681\u4f1a\u642c\u8fd0\u5c38\u4f53\u548c\u5730\u4e0a\u7684\u96f6\u788e  * \u79fb\u52a8\u94f6\u884c\u5ff5\u73e0\u5236\u4f5c\u673a\uff0c\u867d\u7136\u63d0\u6210\u5f88\u9ed1\u4f46\u786e\u5b9e\u89e3\u51b3\u4e86\u957f\u9014\u8df3\u8df3\u4e50\u8dd1\u5c38\u7684\u75db\u70b9  * \u5c0f\u59d0\u59d0\u8d8a\u72f1\u540e\u628a\u73a9\u5f04\u5979\u62ab\u98ce\u7684\u72f1\u5352\u7ede\u6740\u4e86\uff0c\u597d\u6837\u7684  * \u9ec4\u8702\u5c0f\u59d0\u59d0\u4f1a\u6247\u5c0f\u5077\u5df4\u638c\uff0c\u8fd8\u53ef\u4ee5\u62bd\u5077\u770b\u5979\u6ce1\u6e29\u6cc9\u7684\u4eba\u3002</p> <p>\u60ca\u5413  * \u5750\u6905\u5b50\u5b58\u6863\u7136\u540e\u88ab\u6905\u5b50\u5403\u4e86  * \u6253\u5b8cboss\uff0c\u5b83\u70b8\u4e86\uff0c\u6211\u88c2\u4e86  * \u4ee5\u4e3a\u6253\u5b8cboss\uff0c\u5b83\u4e8c\u9636\u6bb5\u4e86  * \u597d\u4e0d\u5bb9\u6613\u4ece\u5723\u5802\u8d5a\u4e86\u70b9\u8f9b\u82e6\u94b1\u5e76\u5f00\u4e86\u5730\u94c1\u51fa\u6765\u60f3\u62a5\u590d\u6027\u6d88\u8d39\uff0c\u534a\u9014\u88ab\u4e00\u4e2a\u5929    \u964d\u94c1\u7b3c\u5b50\u6293\u8fdb\u5730\u7262\uff0c\u5f52\u96f6\uff0c\u8fd8\u628a\u9ec4\u8702\u5c0f\u59d0\u59d0\u7684\u62ab\u98ce\u4e5f\u6252\u5149\u4e86\uff0c\u592a\u6da9\u4e86  * \u8dd1\u6b65\u5e26\u60ef\u6027\u4e00\u5934\u649e\u654c\u4eba\u6389\u8840\u4e86  * \u6709\u4e9b\u6905\u5b50\u6bcf\u6b21\u5750\u90fd\u8981\u6536\u94b1\uff0c\u5b58\u5b8c\u6905\u5b50\u7ed9\u6536\u8d77\u6765\u4e86\uff0c\u5978\u5546  * \u98ce\u6c99\u592a\u9ed1\u4e86\u770b\u4e0d\u89c1  * \u4ece\u5723\u5821\u4e66\u5e93\u90a3\u4e48\u9ad8\u6863\u7684\u5730\u65b9\u5f80\u4e0b\u98d8\u7740\u5f00\u5730\u56fe\uff0c\u4e00\u8def\u5f80\u4e0b\u6389\u8fdb\u8150\u6c41\u6cfd\u7684\u7caa\u5751\uff0c    \u4e00\u8eab\u86c6\u3002  * \u7f6a\u9014\u3001\u8150\u6c41\u6cfd\u6389\u8fdb\u53bb\u5c31\u67d3\u4e00\u8eab\u86c6\u7684\u86c6\u6c60\uff0c\u6211\u7684\u665a\u996d\u8c22\u8c22\u4f60\u4e86</p>"},{"location":"writing/lena_raine/","title":"\u5199\u70b9\u5173\u4e8eLena Raine\u5927\u795e","text":"<p>\u56e0\u4e3aCeleste\u800c\u5f00\u59cb\u4e86\u89e3\u5979\uff0c\u4f46\u662f\u5979\u98ce\u683c\u7edd\u5bf9\u4e0d\u9650\u4e8eCeleste\u3002\u6240\u6709\u4f5c\u54c1\u5728\u5979\u4e2a\u4eba \u7f51\u7ad9\u4e0a\uff1a</p> <p>lena.fyi</p> <p>Lena Raine\u662f\u4e2a\u8bf4\u8bdd\u5f97\u4f53\uff0c\u975e\u5e38\u7231\u7b11\uff0c\u6280\u672f\u8fc7\u786c\u7684\u5927\u59d0\u59b9\uff0c\u6781\u5176\u6709\u624d\u3002\u5979\u751f\u4e8e\u827a \u672f\u4e4b\u5bb6\uff0c\u5f88\u5c0f\u5c31\u5728\u7236\u4eb2\u7684\u6df7\u97f3\u5ba4\u73a9\uff0c\u4e0a\u5b66\u540e\u5219\u5e38\u9a7b\u5531\u8bd7\u73ed\u3002\u548c\u6e38\u620f\u3001\u97f3\u4e50\u4e00\u8d77\u957f \u5927\u7684\u5979\uff0c\u4e00\u76f4\u6709\u4e2a\u575a\u5b9a\u7684\u76ee\u6807\uff1a\u6210\u4e3a\u6e38\u620f\u97f3\u4e50\u4f5c\u66f2\u5bb6\u3002\u5979\u5927\u5b66\u5ff5\u97f3\u4e50\u5b66\u9662\uff0c\u53e4\u5178 \u548c\u7235\u58eb\u53cc\u4fee\uff0c\u8fd9\u5728\u5979\u7684\u98ce\u683c\u91cc\u80fd\u542c\u51fa\u6765\u3002Lena\u6bd5\u4e1a\u540e\u867d\u7136\u56db\u5904\u6295\u7a3f\uff0c\u4f46\u4e00\u76f4\u6ca1\u6709 \u4f2f\u4e50\u3002\u773c\u770b\u4e00\u5e74\u8fc7\u53bb\u4e86\uff0c\u5bb6\u91cc\u7ed9\u7684\u7ecf\u6d4e\u652f\u6301\u89c1\u5e95\uff0c\u5979\u53ea\u80fd\u5148\u5e72\u6d4b\u8bd5\u65b9\u9762\u7684\u5de5\u4f5c\u3002 \u540e\u6765\u673a\u7f18\u5de7\u5408\u7533\u8bf7\u5230\u4e86\u8bbe\u8ba1\u7684\u5185\u90e8\u804c\u4f4d\uff0c\u5728\u4e00\u6b21\u5c0f\u5723\u8bde\u6d3b\u52a8\u6e38\u620f\u4e2d\u5199\u4e86\u97f3\u4e50\uff0c\u5f97 \u5230\u4e86\u97f3\u4e50\u65b9\u9762\u7684\u8ba4\u53ef\uff0c\u4ece\u6b64\u5f00\u59cb\u7ed9Guild Wars 2\u5199\u6b4c\u3002\u6709\u4e86\u8fd9\u4e2a\u5951\u673a\uff0c\u5979\u5f00\u59cb\u4e1a \u4f59\u65f6\u95f4\u51fa\u4e13\u8f91\uff0c\u6b63\u5de7\u88abMaddy Thorson\u76f8\u4e2d\uff0c\u4f5c\u4e3a\u4e50\u5e08\u52a0\u5165Celeste\u505a\u97f3\u4e50\uff0c\u4e00\u4e3e \u6210\u540d\u3002\u5979\u7684\u91c7\u8bbf\u6211\u5168\u90e8\u770b\u4e86\uff0c\u6700\u5927\u7684\u611f\u53d7\u5c31\u662f\uff0c\u4e00\u8bf4\u5230\u97f3\u4e50\u5979\u5c31\u6ed4\u6ed4\u4e0d\u7edd\u4e86\uff0c\u4eff \u4f5b\u804a\u6280\u672f\u80fd\u804a\u5230\u5730\u8001\u5929\u8352\u3002</p> <p>\u98ce\u683c\u65b9\u9762\uff0cLena\u559c\u6b22\u6311\u6218\u542c\u4f17\u7684\u8033\u6735\uff0c\u4e00\u65b9\u9762\u5979\u7684\u53e4\u5178/\u7235\u58eb\u53cc\u6301\u80cc\u666f\u8ba9\u5979\u80fd\u719f \u7ec3\u4f7f\u7528\u97f3\u4e50\u7406\u8bba\uff0c\u53e6\u4e00\u65b9\u9762\u5979\u4f1a\u65f6\u4e0d\u65f6\u6254\u4e00\u4e9b\u53cd\u76f4\u89c9\u7684\u4e1c\u897f\u51fa\u6765\u3002\u4e00\u4e2a\u6709\u8da3\u7684\u4f8b \u5b50\u662fCeleste\u91cc\u7684confronting myself\uff0c\u6709\u51e0\u4e2a\u58f0\u8c03\u53d8\u5316\u7565\u610f\u5916\uff0c\u5979\u81ea\u5df1\u7684\u73b0\u573a \u4e0a\u90fd\u591a\u6b21\u5f39\u9519\uff0c\u54c8\u54c8\uff0c\u96be\u602a\u5979\u8bf4\u81ea\u5df1\u4e0d\u64c5\u957f\u6f14\u594f\u3002\u5979\u98ce\u683c\u8de8\u5ea6\u4e5f\u5f88\u5927\uff0c\u6839\u636e\u6e38\u620f \u98ce\u683c\u8c03\u6574\uff0c\u6240\u4ee5\u5728Celeste\u91cc\u80fd\u5199mirror, anxiety\u8fd9\u6837\u7684\u81f4\u90c1\u66f2\uff0c\u5728 beastieball\u91cc\u80fd\u5199\u51fa\u5168\u70ed\u8840\u4e13\u8f91\u3002\u4e00\u542cbeastieball\uff0c\u70ed\u8840\u98ce\u683c\uff0c\u8fd9\u5c45\u7136\u662f\u540c\u4e00 \u4e2a\u4eba\u5199\u51fa\u6765\u7684\uff1f</p> <p>\u521b\u4f5c\u5143\u7d20\u65b9\u9762\uff0c\u5927\u6982\u662f\uff1a\u7535\u97f3\u3001\u5408\u6210\u5668\u3001\u7235\u58eb\u3001\u4e2d\u53e4\u6e38\u620flow fidelity\u3002\u5979\u64c5\u957f \u7528\u6b4c\u5199\u6545\u4e8b\uff0c\u4e00\u9996\u6b4c\u91cc\u5305\u542b\u77db\u76fe\u7684\u60c5\u7eea\uff0c\u6bd4\u5982minecraft\u91cc\u7684otherside\uff0c\u4ece\u8212\u7f13 \u653e\u677e\u8fc7\u6e21\u5230\u65e0\u6bd4\u4f24\u611f\u3002Celeste\u91cc\u7684resurrection\u5219\u662f\u8bb2\u6545\u4e8b\u6700\u597d\u7684\u4f8b\u5b50\uff0c9\u5206\u949f \u91cc\u5305\u542b\u4e86\u6574\u4e2a\u5173\u5361\u7684\u6545\u4e8b\u3002\u53e6\u4e00\u4e2a\u662fscattered and lost\u8282\u594f\u4e0a\u80fd\u542c\u51fa\u7235\u58eb\u4e50\u5f71 \u54cd\uff0c\u7f16\u66f2\u4e0a\u6709\u53e4\u5178/\u897f\u5f0f\u6e38\u620f\u7684\u6c1b\u56f4\u611f\uff0c\u8bb2\u7a76\u4e00\u4e2a\u4e30\u5bcc\uff0c\u897f\u5f0f\u53e4\u5178\u4e50\u5668\u7528\u5f88\u591a\u3002 minecraft,creator\u91cc\u4e2d\u9014\u780d\u6389\u91cd\u70bc\u7a81\u7136\u6362\u4e50\u5668\u4e5f\u5f88\u60ca\u8273\u3002</p> <p>\u6df7\u54cd(reverb)\uff0c\u6df7\u54cd\uff0c\u6df7\u54cd\uff01\u5979\u5f88\u559c\u6b22\u7528\u6df7\u54cd\uff0c\u751a\u81f3\u5927\u5b66\u91cc\u56e0\u4e3a\u65e0\u5dee\u522b\u6ee5\u7528\u6df7\u54cd \u88ab\u8001\u5e08\u6279\u8bc4\u8fc7\u3002\u5979\u5199\u8fc7\u8be6\u7ec6\u7684resurrection\u521b\u4f5c\u8fc7\u7a0b\uff0c\u6bcf\u4e2a\u7247\u6bb5\u7684\u6b65\u9aa4\u90fd\u662f1\uff0c2\uff0c 3\uff0c....\u52a0\u6df7\u54cd\uff01</p> <p>\u6211\u60f3\u4e86\u4e0b\u5230\u5e95\u4ec0\u4e48\u662fceleste\u5473\u513f\uff1fminecraft\u5979\u7684\u51e0\u9996\u6b4c\uff0c\u65f6\u4e0d\u65f6\u80fd\u542c\u51fa celeste\u5473\u513f\u3002\u5927\u7ea6\u662f\u5408\u6210\u5668\u97f3\u8272\u3001\u5927\u8bda\u65c5\u5e97\u90a3\u7ae0\u91cc\u7684\u9b3c\u58f0\uff0cLena\u597d\u50cf\u633a\u559c\u6b22\u52a0 \u60a0\u60a0\u7684\u9b3c\u53eb\uff0cbeastieball\u91cc\u4e5f\u51fa\u73b0\u4e86\uff0c\u4e00\u542c\u6211\u5c31\u4e50\u4e86\uff0c\u563fCeleste\u4f60\u597d\u53c8\u89c1\u9762\u4e86\u3002</p> <p>\u5c0f\u63d2\u66f2\uff0cLena\u4e3a\u4e86\u5728\u81f4\u90c1\u7684mirror temple\u91cc\u52a0\u9ed1\u5f69\u86cb\uff0c\u628a\u81ea\u5df1\u5173\u67dc\u5b50\u91cc\u5bf9\u7740\u9ea6 \u514b\u98ce\u6765\u4e86\u4e00\u6bb5\u72ec\u767d\uff0c\u8fd8\u628a\u54ed\u90fd\u7ed9\u5f55\u8fdb\u53bb\u4e86\u3002\u7136\u540e\u5979\u628a\u8fd9\u6bb5\u97f3\u9891\u53cd\u7740\u653e\u8fdb\u4e86\u6b4c\u91cc\uff0c \u5236\u9020\u51fa\u4eba\u58f0\u6548\u679c\u3002\u540e\u6765\u88ab\u7c89\u4e1d\u7ffb\u8f6c\u56de\u6765\u7834\u8bd1\u51fa\u4e86\u72ec\u767d\uff0c\u4f46\u51e1\u4e0d\u5b8c\u5168\u987a\u76f4\u7684\u4eba\u90fd\u4f1a \u88ab\u8fd9\u6bb5\u72ec\u767d\u6574\u7834\u9632\u3002\u4e0d\u8fc7\u867d\u7136\u5979\u7684\u81f4\u90c1\u66f2\u5f88\u80fd\u6253\u52a8\u4eba\uff0c\u5979\u672c\u4eba\u5e76\u4e0d\u60f3\u5199\u592a\u591a\u3002\u6709 \u7c89\u4e1d\u95ee\u5979\u8fd8\u80fd\u4e0d\u80fd\u518d\u5199\u4e2aresurrection \u8fd9\u6837\u5145\u6ee1\u60c5\u7eea\u7684\uff0c\u5979\u8bf4\u4e0d\u60f3\u5199\u3002</p> <p>\u5979\u91c7\u8bbf\u91cc\u51e0\u6b21\u8bf4\u8fc7\uff08\u56e0\u4e3a\u6709\u4eba\u95ee\uff09\uff0c\u5e0c\u671b\u81ea\u5df1\u662f\u901a\u8fc7\u4f5c\u54c1\u88ab\u8ba4\u53ef\uff0c\u800c\u4e0d\u662f\u5979\u7684 mtf\u8eab\u4efd\u3002\u5979\u786e\u5b9e\u505a\u5230\u4e86\uff0c\u5728\u8eab\u4efd\u4e0a\u5f88\u4f4e\u8c03\uff0c\u4e0d\u5351\u4e0d\u4ea2\uff0c\u9760\u4f5c\u54c1\u8bc1\u660e\u81ea\u5df1\u3002\u4f46\u6211 \u4e2a\u4eba\u8ba4\u4e3aCeleste\u91cc\u6709\u51e0\u9996\u6b4c\uff0c\u4e0d\u4f46\u76f4\u4eba\u5199\u4e0d\u51fa\u6765\uff0c\u8fdelgb\u4e5f\u5199\u4e0d\u51fa\u6765\uff0c\u53ea\u6709t\u80fd \u5199\u51fa\u6765\u3002\u7ecf\u5e38\u6027\u8f6c\u7684\u4eba\u90fd\u77e5\u9053\uff0cmirror\u91cc\u7684\u72ec\u767d\u7b80\u76f4\u5c31\u662f\u8de8\u7684\u81ea\u767d\u4e66\u3002</p> <p>\u5c0f\u63d2\u66f2\uff1a\u5979\u5199farewell\u7684\u65f6\u5019\u4e13\u95e8\u8003\u8651\u5230\u4e86\u901f\u901a\u8df3\u5173\u3002\u6240\u4ee5\u7f16\u6392\u6210\u4e86\u54ea\u6015\u8df3\u5173\u4e5f \u80fd\u987a\u6ed1\u8fc7\u6e21\u7684\u6837\u5b50\u3002\u5979\u7684\u5de5\u4f5c\u6001\u5ea6\u5c31\u662f\u201c\u7528\u529b\u8fc7\u731b\u201d\uff0c\u4efb\u4f55\u6d3b\u90fd\u5c3d\u529b\u505a\u5230\u6700\u597d\uff0c\u54ea \u6015\u4e0d\u662f\u5fc5\u987b\u3002</p> <p>\u5c0f\u63d2\u66f2\uff1aceleste b side\u4e13\u8f91\u6536\u5f55\u4e86\u5979\u5199\u7684\u7b2c\u4e00\u7248\u7b2c\u4e00\u5173\u3002\u5979\u5728\u91c7\u8bbf\u91cc\u8bf4\u8fc7\uff0c\u662f \u6309\u7167\u661f\u4e4b\u5361\u6bd4\u98ce\u683c\u5199\u7684\uff0c\u6b22\u5feb\u800c\u8df3\u8dc3\uff0c\u56e0\u4e3a\u4ee5\u4e3a\u662f\u8d85\u7ea7\u739b\u4e3d\u98ce\u683c\u6e38\u620f\u3002Maddy\u542c \u4e86\u4e4b\u540e\u8bf4\u4f60\u8fd9\u4e0d\u884c\uff0c\u722c\u5c71\u8981\u6709\u505c\u987f\u4e0b\u6765\u601d\u8003\u7684\u611f\u89c9\uff0c\u624d\u6539\u6210\u4e86\u540e\u6765\u7684\u6837\u5b50\u3002\u7b2c\u4e00 \u7248\u786e\u5b9e\u592a\u6109\u60a6\u4e86\uff0c\u542c\u4e86\u5c31\u60f3\u8bf4\uff0cmaddy\u662f\u4e0d\u662f\u4e00\u5f00\u59cb\u5fd8\u8bb0\u544a\u8bc9lena\u6545\u4e8b\u8bb2\u5565\u4e86\uff0c \u5f53\u7eaf\u8df3\u8df3\u4e50\u5462\u3002</p> <p>\u5979Medium\u4e0a\u5199\u4e86\u51e0\u7bc7\u6280\u672f\u578b\u6587\u7ae0\uff0c\u5206\u4eab\u5199\u6b4c\u7684\u8fc7\u7a0b\uff0c\u53d7\u76ca\u532a\u6d45\u3002\u53ef\u60dc\u5f03\u5751\u4e86\u3002</p>"},{"location":"writing/link_cook/","title":"\u820c\u5c16\u4e0a\u7684\u6d77\u62c9\u9c81\uff0c\u5927\u53a8\u6797\u514b\u7684\u65c5\u884c","text":"<p>\u738b\u56fd\u4e4b\u6cea\u4e0d\u662f\u4e0d\u597d\uff0c\u4f46\u91ce\u708a\u624d\u662f\u767d\u6708\u5149\u3002\u6211\u6015\u662f\u6709\u5e7d\u95ed\u6050\u60e7\u75c7\uff0c\u738b\u6cea\u7684\u5730\u4e0b\u4e16\u754c \u4e00\u8fdb\u53bb\u5c31\u7cbe\u795e\u4e0d\u9002\u3002\u91ce\u708a\u5927\u5e08\u6a21\u5f0f\uff0c\u795e\u5e99\u5168\u5f00\uff0c\u5440\u54c8\u54c8\u6ca1\u6709\u6293\u5b8c\u4f46\u662f\u4e0d\u60f3\u518d\u6293\uff0c \u6765\u4e00\u6b21\u5e73\u6c11\u5f0f\u65c5\u884c\u6765\u4f5c\u4e3a\u4e86\u7ed3\u3002</p> <p>\uff08\u4e5f\u8bb8\u8fd9\u7bc7\u5199\u5b8c\u6211\u4f1a\u518d\u53bb\u63a2\u5bfb\u4e00\u756a\u738b\u6cea\u3002\uff09</p> <p>\u89c4\u5219\uff1a  * \u4e0d\u80fd\u6218\u6597\uff0c\u53ea\u80fd\u9003\u8dd1\uff0c\u53a8\u5e08\u4e0d\u6253\u4ed7\u3002\u552f\u4e00\u4f8b\u5916\u662f\u88ab\u91ce\u751f\u76d6\u4f0a\u961f\u5077\u88ad\u3002  * \u6b66\u5668\u53ef\u4ee5\u7528\u6765\u6316\u77ff</p> <p>\u4f5c\u4e3a\u5e73\u6c11\u5927\u53a8\u7684\u6211\uff0c\u4ece\u91cd\u751f\u4e4b\u5730\u51fa\u53d1\uff0c\u62fc\u51d1\u98df\u6750\u91cd\u73b0\u6d77\u62c9\u9c81\u7684\u8f89\u714c</p> <p>\u70e4\u82f9\u679c</p> <p>\u70e4\u8fa3\u6912\u679c</p>"},{"location":"writing/link_travel_wind/","title":"\u5e73\u6c11\u6797\u514b\u7684\u5192\u9669","text":"<p>\u6211\u662f\u4e2a\u4f5c\u4e3a\u5e73\u6c11\u7684\u6797\u514b</p>"},{"location":"writing/shenzhen/","title":"\u6df1\u5733\u56de\u5fc6","text":"<p>\u6211\u5728\u6df1\u5733\u8bfb\u4e66\u751f\u6d3b\u7684\u5341\u591a\u5e74\u91cc\uff0c\u4ece\u6765\u6ca1\u6709\u559c\u6b22\u8fc7\u6df1\u5733\uff0c\u6025\u5207\u76fc\u671b\u7740\u9003\u79bb\u3002\u73b0\u5728\u53c8 \u89c9\u5f97\u5b83\u5e76\u6ca1\u6709\u90a3\u4e48\u4e0d\u582a\uff0c\u51b5\u4e14\u662f\u4e2a\u4f4f\u4e60\u60ef\u4e86\u7684\u57ce\u5e02\u3002\u4e8e\u662f\u628a\u8bb0\u5fc6\u91cc\u7684\u4e1c\u897f\u6253\u635e \u51fa\u6765\u3002</p>"},{"location":"writing/shenzhen/#_2","title":"\u9ad8\u538b\u8bbe\u5907","text":"<p>\u6211\u6700\u6015\u7684\u4e1c\u897f\u4e4b\u4e00\u662f\u5e26\u9ad8\u538b\u7684\u673a\u5668\u3002\u8fd9\u4e2a\u53ef\u80fd\u8981\u8ffd\u6eaf\u5230\u5c0f\u5b66\u5427\uff0c\u5f53\u65f6\u5e02\u533a\u67d0\u5382\u53d1 \u751f\u4e86\u7164\u6c14\u5927\u7206\u70b8\uff08\u6df1\u5733199X\u5e74\u76848.5\u5927\u7206\u70b8\uff09\uff0c\u6211\u5bb6\u7684\u7535\u706f\u4e5f\u8ddf\u7740\u9707\u788e\u4e86\uff0c\u8fd8\u4ee5 \u4e3a\u5730\u9707\uff0c\u77e5\u9053\u540e\u5bf9\u7164\u6c14\u5f88\u6050\u60e7\u3002\u90a3\u65f6\u4faf\u5bb6\u5bb6\u90fd\u7528\u7164\u6c14\u6876\uff0c\u5927\u4eba\u5631\u5490\u4e0d\u8981\u4e71\u52a8\u65f6\u603b \u4f1a\u5938\u5f20\u4e00\u70b9\uff0c\u8bf4\u54ea\u91cc\u53c8\u7206\u70b8\u4e86\u3002\u90a3\u4e2a\u5e74\u4ee3\u9ad8\u538b\u9505\u7206\u70b8\u7684\u65b0\u95fb\u4e5f\u4e0d\u5c11\uff0c\u6240\u4ee5\u5bf9\u4e00\u5207 \u9ad8\u538b \u6c14\u4f53\u90fd\u63d0\u5fc3\u540a\u80c6\uff0c\u62c5\u5fc3\u81ea\u5df1\u6216\u90bb\u5c45\u5bb6\u7684\u5374\u51fa\u95ee\u9898\uff0c\u9632\u4e0d\u80dc\u9632\u3002 \u6bcf\u5f53\u8def\u4e0a\u770b\u89c1\u7528\u4e09\u8f6e\u8f66\u9001\u7164\u6c14\u7f50\u7684\u9001\u8d27\u5de5\uff0c\u90fd\u89c9\u5f97\u4ed6\u4eec\u771f\u52c7\u6562\u3002\u73b0\u5728\u6bcf\u6b21 \u7528\u9ad8\u538b\u706d \u83cc\u673a\u5668\u90fd\u6709\u70b9\u4e0d\u81ea\u5728\uff0c\u867d\u7136\u77e5\u9053\u5b89\u5168\u7cfb\u6570\u9ad8\u3002</p>"},{"location":"writing/shenzhen/#_3","title":"\u8f66","text":"<p>\u6216\u8005\u8bf4\u8f66\u7978\u3002\u6df1\u5733\u5f00\u59cb\u6709\u79c1\u5bb6\u8f66\u7684\u65f6\u5019\uff0c\u65b0\u53f8\u673a\u591a\uff0c\u8f66\u7978\u4e5f\u591a\u3002\u7b2c\u4e00\u6b21\u7684\u95f4\u63a5\u4e86 \u89e3\u6765\u81ea\u4e8e\u8001\u7238\u4f5c\u4e3a\u76ee\u51fb\u8005\u7684\u63cf\u8ff0\u3002\u8bf4\u662f\u76ee\u51fb\u8005\uff0c\u5176\u5b9e\u770b\u5230\u7684\u4e5f\u662f\u4e8b\u540e\u4e86\u3002\u65f6\u95f4\u662f \u5927\u5e74\u4e09\u5341\uff0c\u5730\u70b9\u662f\u4e00\u6761\u7e41\u5fd9\u7684\u6591\u9a6c\u7ebf\uff0c\u8001\u7238\u770b\u5230\u7684\u65f6\u5019\u5904\u7406\u7684\u5dee\u4e0d\u591a\u4e86\uff0c\u4f46\u73b0\u573a \u7559\u4e86\u4e00\u53ea\u9ad8\u8ddf\u978b\u8c01\u4e5f\u6ca1\u6709\u53bb\u52a8\u3002\u9c9c\u8273\u7684\u7ea2\u8272\u9ad8\u8ddf\u978b\uff0c\u5b83\u7684\u4e3b\u4eba\u5e94\u8be5\u662f\u4e2a\u7231\u7f8e\u70ed\u7231 \u751f\u6d3b\u7684\u4eba\uff0c\u8fc7\u5e74\u5e94\u8be5\u6709\u4eba\u7b49\u7740\u5979\u56de\u5bb6\u3002\u65e0\u8bba\u8fd9\u4e2a\u63a8\u6d4b\u662f\u5426\u5c5e\u5b9e\uff0c\u8fd9\u79cd\u53ef\u80fd\u6027\u8ba9\u6211 \u4e00\u76f4\u5f88\u96be\u53d7\u3002\u4ee5\u81f3\u4e8e\u4e4b\u540e\u6211\u4eb2\u81ea\u76ee\u7779\u7684\u8111\u6d46\u6d82\u5730\u7684\u8f66\u7978\uff0c\u51b2\u51fb\u529b\u90fd\u8d76\u4e0d\u4e0a\u8fd9\u4e2a\u6211 \u5e76\u672a\u770b\u89c1\u753b\u9762\u7684\u4e8b\u6545\u3002\u6211\u73b0\u5728\u5f00\u8f66\u5f88\u5c0f\u5fc3\uff0c\u5927\u6982\u5f97\u76ca\u4e8e\u5c0f\u65f6\u5019\u770b\u7684\u8f66\u7978\u3002</p>"},{"location":"writing/shenzhen/#_4","title":"\u7535\u89c6","text":"<p>\u56e0\u4e3a\u9760\u8fd1\u9999\u6e2f\uff0c\u6240\u4ee5\u6536\u5f97\u5230\u9999\u6e2f\u7684\u7535\u53f0\u3002\u6211\u521a\u5230\u7684\u90a3\u51e0\u5e74\u662f\u770b\u52a8\u753b\u7684\u9ad8\u5cf0\u65f6\u671f\uff0c \u653e\u5047\u4ece\u65e9\u770b\u5230\u4e0b\u5348\uff0c\u5c31\u7b97\u4e0a\u5b66\u4e86\u8fd8\u53ef\u4ee5\u770b\u5468\u516d\u7684\u3002\u7b2c\u4e00\u4e2a\u770b\u7684\u662f\u300a\u9762\u5305\u8d85\u4eba\u300b\uff0c \u5f88\u7528\u5fc3\u7684\u628a\u91cc\u9762\u7684\u4eba\u7269\u753b\u5728\u7eb8\u4e0a\uff0c\u7528\u526a\u5200\u526a\u4e0b\u6765\u73a9\u3002\u4e0d\u5e78\u5f53\u65f6\u7531\u4e8e\u548c\u53e6\u5916\u4e24\u5bb6\u5408 \u4f4f\u4e24\u623f\u4e00\u5385\uff0c\u5730\u65b9\u6324\uff0c\u5927\u4eba\u6536\u62fe\u4e1c\u897f\u81ea\u7136\u52e4\u5feb\uff0c\u6211\u79bb\u5f00\u4e00\u4f1a\u513f\u9762\u5305\u8d85\u4eba\u4eec\u5c31\u6ca1\u5f71 \u4e86\uff0c\u6297\u8bae\u4e4b\uff0c\u56de\u590d\u4e3a\u201c\u4e0d\u77e5\u9053\u4f60\u8981\u4fdd\u7559\u8fd9\u4e2a\u201d\u3002\u81f3\u4eca\u89c9\u5f97\u5927\u4eba\u4e0d\u628a\u5c0f\u5b69\u5b50\u7684\u51b3\u5b9a\u5f53 \u56de\u4e8b\u662f\u5f88\u66b4\u653f\u7684\u884c\u4e3a\uff0c\u6211\u73cd\u85cf\u7684\u6b63\u7248\u9f99\u73e0\u6f2b\u753b\u3001\u76d7\u7248\u6f2b\u753b\u548c\u76d7\u7248\u73a9\u5177\u90fd\u4ee5\u201c\u4f60\u957f \u5927\u5c31\u4e0d\u60f3\u8981\u8fd9\u4e9b\u201d\u4e3a\u7531\u8fde\u54c4\u5e26\u9a97\u7684\u9001\u7ed9\u8868\u5f1f\u4e86\u3002</p>"},{"location":"writing/shenzhen/#_5","title":"\u6e38\u620f","text":"<p>\u8bf4\u5230\u6e38\u620f\uff0c\u60f3\u8d77\u548c\u90bb\u5c45\u73a9\u8857\u9738\u7684\u4e00\u4ef6\u62bd\u8c61\u4e8b\u3002\u8857\u9738\u91cc\u7684\u6625\u4e3d\u8eab\u6750\u5f88\u597d\uff0c\u6240\u4ee5\u90bb\u5c45\u5f88 \u559c\u6b22\u5979\uff0c\u60f3\u4e86\u534a\u5929\u7ec8\u4e8e\u60f3\u5230\u4e86\u4e00\u4e2a\u5077\u7aa5\u529e\u6cd5\uff0c\u5c31\u662f\u7528\u4f1a\u653e\u7535\u7684\u7eff\u602a\u517d\u53bb\u7535\u5979\uff0c\u7136 \u540e\u753b\u9762\u5c31\u4f1a\u663e\u793a\u9aa8\u67b6\u3002\u4e3a\u4ec0\u4e48\u8981\u770b\u9aa8\u67b6\uff1f\u56e0\u4e3a\u53ea\u6709\u8fd9\u6837\u624d\u6ca1\u8863\u670d\u3002\u4e8e\u662f\u5927\u4eba\u4e0d\u5728 \u7684\u65f6\u5019\uff0c\u5c31\u6709\u4e24\u4e2a\u5c0f\u5b69\u5bf9\u7740\u6682\u505c\u7684\u6e38\u620f\u5c4f\u5e55\u4e0a\u7684\u4e00\u4e2a\u9aa8\u67b6\u82b1\u75f4\uff0c\u715e\u6709\u4ecb\u4e8b\u7684\u8bc4\u8bba\uff0c \u8fd8\u6ca1\u89c9\u5f97\u6709\u5565\u4e0d\u5bf9\u52b2\u3002</p>"},{"location":"writing/shenzhen/#_6","title":"\u4e66\u57ce","text":"<p>\u6df1\u5733\u6700\u5927\u7684\u5356\u4e66\u7684\u5730\u65b9\u3002\u60ed\u6127\u7684\u8bf4\uff0c\u5c0f\u65f6\u5019\u6211\u5bf9\u5356\u4e66\u85cf\u4e66\u7684\u5174\u8da3\u5927\u4e8e\u770b\u4e66 \u7684\u5174\u8da3\uff0c\u6240\u4ee5\u8001\u5f80\u4e66\u57ce\u8dd1\uff0c\u53ea\u8981\u542c\u8bf4\u67d0\u672c\u4e66\u503c\u5f97\u770b\uff0c\u5c31\u4e00\u5b9a\u8981\u4e70\u56de\u6765\u3002\u8d8a\u662f\u624b\u5934 \u6ca1\u94b1\u8d8a\u7231\u901b\u4e66\u57ce\uff0c\u6bcf\u672c\u90fd\u770b\u51e0\u773c\u518d\u51b3\u5b9a\u3002\u66f4\u60ed\u6127\u7684\u8bf4\uff0c\u4e66\u57ce\u6700\u5438\u5f15\u6211\u7684\u5374\u662f\u91cc\u9762 \u597d\u5403\u53c8\u5ec9\u4ef7\u7684\u7092\u7c89\uff0c\u8fd8\u6709\u5176\u5b83\u5730\u65b9\u4e70\u4e0d\u5230\u7684\u82f9\u679c\u6c7d\u6c34\u3002</p>"},{"location":"writing/shenzhen/#_7","title":"\u534e\u5f3a\u5317/\u8d5b\u683c\u5e7f\u573a","text":"<p>\u4ece\u521a\u5230\u6df1\u5733\u768492\u5e74\u5230\u79bb\u5f00\u768403\u5e74\u4e00\u76f4\u89c1\u8bc1\u7740\u8d5b\u683c\u7684\u53d8\u5316\uff0c\u4f5c\u4e3a\u7535\u5b50\u4ea7\u54c1\u4e2d\u5fc3\uff0c\u5341 \u5e74\u91cc\u6162\u6162\u4ece\u826f\u4e86\u3002</p> <p>92\u5e74\u7684\u8d5b\u683c\u51e0\u4e4e\u662f\u4e3a\u7ea2\u767d\u673a\u800c\u5f00\u7684\uff0c\u5145\u6ee1\u5c0f\u9738\u738b\u5b66\u4e60\u673a\u548c\u76d7\u7248\u5361\u5e26\u3002\u4ef7\u683c\u5bf9\u4e8e\u666e \u901a\u6536\u5165\u5bb6\u5ead\u504f\u8d35\uff0c\u53ea\u542b\u4e00\u4e2a\u6e38\u620f\u7684\u5361\u5e26\u6211\u4ece\u6ca1\u820d\u5f97\u4e70\u3002\u56db\u5408\u4e00\u3001\u516d\u5408\u4e00\u7b49\u8f83\u5e38\u89c1\uff0c \u91cc\u9762\u4e00\u822c\u9644\u5e26\u4e00\u4e24\u4e2a\u5f88\u597d\u73a9\u7684\u3002N\u5408\u4e00\u7684\u5361\u5e26\u5219\u6709\u9a97\u4eba\u5acc\u7591\uff0c\u5927\u90e8\u5206\u6e38\u620f\u65e0\u804a\uff0c \u5f88\u591a\u91cd\u590d\u3002\u552f\u4e00\u7684\u4f8b\u5916\u662f\u67d0\u516d\u5341\u56db\u5408\u4e00\u5361\u5e26\uff0c\u5927\u90e8\u5206\u6e38\u620f\u4e0d\u540c\uff0c\u5927\u90e8\u5206\u597d\u73a9\uff0c\u5305 \u62ec\u7ecf\u5178\u7684\u8d85\u7ea7\u739b\u4e3d\u3001\u5403\u8c46\u4eba\u3001\u70b8\u5f39\u4eba\u2026\u2026\u7b49\u7b49\uff0c\u53ef\u60dc\u5f88\u96be\u4e70\u5230\uff0c\u4ef7\u683c\u4e5f\u6bd4\u5176\u5b83N\u5408 \u4e00\u7684\u8d35\uff0c\u6211\u7238\u8fd8\u4e86\u534a\u5929\u4ef7\u624d\u52c9\u5f3a\u4e70\u4e0b\u3002</p> <p>97\u5e74\u7684\u65f6\u5019\u7ea2\u767d\u673a\u6e10\u6e10\u88ab\u7535\u8111\u9876\u66ff\uff0c\u8d5b\u683c\u4e5f\u66f4\u65b0\u6362\u4ee3\u4e3a\u7535\u8111\u914d\u4ef6\u548c\u76d7\u7248\u5149\u76d8\u4e4b\u5730\u3002 \u901b\u8d77\u6765\u662f\u76f8\u5f53\u60ec\u610f\uff0c\u968f\u4fbf\u627e\u4e2a\u67dc\u53f0\uff0c\u4e00\u5f20\u4e00\u5f20\u5149\u76d8\u7ffb\u8fc7\u53bb\uff0c\u4e70\u5b8c\u8fd8\u80fd\u8e6d\u4e00\u4f1a\u5468\u8fb9 \u4e66\u5e97\u91cc\u7684\u5404\u79cd\u8bbe\u8ba1\u8f6f\u4ef6\u6559\u7a0b\u3002\u540e\u6765\u5468\u8fb9\u7e41\u534e\u70b9\u4e86\uff0c\u8def\u8fb9\u6709\u4e86\u5c0f\u644a\uff0c\u5176\u4e2d\u70e4\u9c7f\u9c7c\u6700 \u4f73\uff0c\u867d\u7136\u90a3\u536b\u751f\u6761\u4ef6\u6709\u70b9\u2026</p> <p>02\u5e74\u5de6\u53f3\u9891\u7e41\u7684\u4e25\u6253\u4f7f\u5f97\u5149\u76d8\u5c0f\u8d29\u4eec\u65f6\u4e0d\u65f6\u7f29\u4e0b\u53bb\u4e00\u9635\u5b50\uff0c\u5927\u8857\u4e0a\u5f00\u59cb\u6709\u96f6\u96f6\u6563 \u6563\u7684\u4eba\u5411\u8def\u4eba\u53d1\u51fa\u5492\u8bed\uff1a\u201c\u6e38\u620f\u8f6f\u4ef6\u5149\u76d8\u201d\u3002\u67d0\u6b21\u548c\u5c0f\u670b\u53cb\u51fa\u53bb\u6dd8\u789f\uff0c\u53d1\u73b0 \u8d5b\u683c\u5185\u90e8\u53ea\u5269\u826f\u6c11\uff0c\u5c31\u8ddf\u7740\u4e00\u4e2a\u53d1\u5492\u8bed\u7684\u963f\u59e8\u8fdb\u5165\u4e00\u795e\u79d8\u623f\u95f4\u5bfb\u5149\u76d8\uff0c\u8c8c\u4f3c\u8d27\u4e0d \u591a\u300206\u5e74\u56de\u56fd\u518d\u53bb\u8d5b\u683c\uff0c\u76d7\u7248\u7684\u5f71\u5b50\u90fd\u6ca1\u4e86\u3002</p>"},{"location":"writing/shenzhen/#_8","title":"\u56fd\u8d38\u5927\u53a6","text":"<p>\u521a\u5230\u6df1\u5733\u65f6\uff0c\u6211\u89c9\u5f97\u5b83\u662f\u7e41\u534e\u7684\u4ee3\u8868\u3002\u5b83\u9876\u4e0a\u6709\u4e2a\u65cb\u8f6c\u9910\u5385\uff0c\u53ef\u4ee5\u770b\u5230\u9999\u6e2f\u3002\u9910 \u5385\u91cc\u7684\u65e9\u8336\u505a\u5f97\u4e0d\u9519\uff0c\u4f46\u65e0\u7591\u5f88\u8d35\uff0c\u8c01\u8ba9\u8fd9\u5c42\u697c\u4f1a\u52a8\u5462\u3002\u5916\u516c\u5916\u5a46\u6765\u73a9\u7684\u65f6\u5019\u624d \u53bb\u4e86\u4e00\u6b21\uff0c\u8bb0\u5f97\u5f88\u6e05\u695a\u90a3\u91cc\u5356\u7684\u679c\u51bb\u4e0a\u7acb\u4e86\u628a\u7eb8\u4f1e\uff0c\u5438\u5f15\u5c0f\u5b69\u5b50\uff0c\u6240\u4ee5\u6211\u6b7b\u6d3b\u90fd \u8981\u70b9\u3002\u56fd\u8d38\u5927\u53a6\u7684\u65e9\u8336\u5728\u5e95\u697c\u4e5f\u5356\uff0c\u4ef7\u683c\u5c31\u53ef\u4ee5\u627f\u53d7\u4e86\u3002\u6240\u8c13\u5355\u7eaf\uff0c\u83ab\u8fc7\u4e8e\u5403\u7684 \u65f6\u5019\u53ea\u60f3\u7740\u7f8e\u5473\u5427\uff0c\u751f\u6d3b\u7e41\u6742\u8d77\u6765\u540e\uff0c\u5c31\u65e0\u6b64\u95f2\u60c5\u9038\u81f4\u4e86\u3002</p>"},{"location":"writing/shenzhen/#_9","title":"\u6c99\u5934\u89d2","text":"<p>\u6c99\u5934\u89d2\u6709\u6761\u4e2d\u82f1\u8857\uff0c\u987e\u540d\u601d\u4e49\uff0c\u9999\u6e2f\u56de\u5f52\u4e4b\u524d\uff0c\u4e2d\u56fd\u548c\u82f1\u56fd\u7684\u79df\u754c\u5c31\u5728\u90a3\u91cc\u3002\u6709 \u4ea4\u754c\u81ea\u7136\u6709\u4ea4\u6d41\uff0c\u6709\u4ea4\u6d41\u81ea\u7136\u6709\u8d70\u79c1\uff0c\u6240\u4ee5\u4e2d\u82f1\u8857\u662f\u4eba\u4eec\u6dd8\u6d0b\u8d27\u7684\u5b9d\u5730\u3002\u6211\u5c0f\u65f6 \u5019\u6ca1\u6709\u53bb\u8fc7\uff0c\u53ea\u542c\u4e86\u4e2a\u5927\u6982\uff0c\u636e\u8bf4\u53ef\u4ee5\u901b\u4f46\u4e0d\u80fd\u4e70\uff0c\u4e70\u4e86\u8981\u5077\u5077\u5e26\u51fa\u6765\uff0c\u88ab\u53d1\u73b0 \u4f1a\u6ca1\u6536\u5e76\u7f5a\u6b3e\uff0c\u7b97\u4e0d\u4e0a\u4e25\u60e9\uff0c\u68c0\u67e5\u7684\u4eba\u4e5f\u7741\u4e00\u53ea\u773c\u95ed\u4e00\u53ea\u773c\uff0c\u6240\u4ee5\u8bb8\u591a\u4eba\u5192\u9669\u3002</p> <p>\u5f53\u521d\u641c\u522e\u6765\u7684\u7269\u54c1\u5e76\u65e0\u7279\u522b\u4e4b\u5904\uff0c\u5c31\u662f\u4e00\u4e9b\u6d0b\u6c34\u679c\u548c\u6d0b\u96f6\u98df\uff08\u542b\u5783\u573e\u98df\u54c1\uff09\u3002\u7531 \u4e8e\u4e0d\u61c2\u82f1\u6587\uff0c\u8fd8\u4e70\u4e86\u4e0d\u5c11\u5931\u8d25\u54c1\uff0c\u6bd4\u5982\u88ab\u6211\u8bc4\u4ef7\u4e3a\u6811\u6839\u5473\u7684\u5976\u7c89\u3001\u6211\u575a\u6301\u8bf4\u662f\u5582 \u7ed9\u5ba0\u7269\u9a6c\u5403\u7684\u602a\u5473\u997c\u5e72\u3002\u901a\u5fc3\u7c89\u4e70\u6765\uff0c\u6211\u4eec\u90fd\u4e0d\u77e5\u9053\u600e\u4e48\u5403\uff0c\u5c31\u8c61\u716e\u7c89\u4e00\u6837\u505a\uff0c \u5473\u9053\u53e3\u611f\u90fd\u4e00\u822c\u3002</p> <p>\u552f\u4e00\u8bb0\u5fc6\u6df1\u523b\u7684\u597d\u7269\u662f\u5bcc\u58eb\u82f9\u679c\uff0c\u4ece\u5934\u5230\u811a\u90fd\u6df1\u7ea2\u7684\u90a3\u79cd\uff0c\u4e5f\u8bb8\u662f\u4eba\u54c1\u7206\u53d1\u4e5f\u8bb8 \u662f\u975e\u6b63\u5e38\u6e20\u9053\u6765\u7684\u9999\uff0c\u6bcf\u4e2a\u90fd\u53c8\u751c\u53c8\u8106\uff0c\u4e00\u76f4\u662f\u6700\u7231\u3002\u56fd\u5185\u6b63\u5f0f\u5f15\u8fdb\u5bcc\u58eb\u82f9\u679c\u540e\uff0c \u5c31\u518d\u4e5f\u6ca1\u5403\u5230\u90a3\u4e48\u9ad8\u8d28\u91cf\u7684\u82f9\u679c\u4e86\u3002</p>"},{"location":"writing/showlog/","title":"\u6f14\u51fa\u6d41\u6c34\u8d26","text":""},{"location":"writing/showlog/#_2","title":"\u9996\u6f14","text":"<p>\u4e00\u5468\u524d\u6211\u611f\u5192\u54b3\u5f97\u4eba\u795e\u5171\u6124\uff0c\u4e24\u5929\u524d\u4e3b\u5531\u55d3\u5b50\u51fa\u95ee\u9898\uff0c\u6628\u5929\u6f14\u7ec3\u5386\u53f2\u6c34\u5e73\u6700\u5dee\u3002 \u4eca\u5929\u5230\u4e86\u573a\u5730\u6211\u624d\u53d1\u73b0\u6ca1\u5e26\u9694\u97f3\u8033\u673a\u3002\u5230\u4e86\u573a\u5730\u624d\u53d1\u73b0tom\u9f13\u53ea\u67092\u4e2a\uff0c\u800c\u4e0d\u662f\u5e73 \u65f6\u7ec3\u76843\u4e2a\uff1bcrash\u9572\u7247\u5728\u53f3\u8fb9\uff0c\u800c\u4e0d\u662f\u5e73\u65f6\u7684\u5de6\u8fb9\u3002\u6f14\u51fa</p> <p>\u4e3a\u4e86\u4e0d\u51fa\u72b6\u51b5\uff0c\u6211\u4eec\u6309\u7167\u7ed9\u5b9a\u7684\u5378\u8d27\u65f6\u95f4\u5230\u573a\uff0c\u88ab\u544a\u77e5\u7b2c\u4e00\u4e2a\u6f14\u51fa\u7684\u4e50\u961f\u6700\u540e\u4e00 \u4e2a\u8c03\u97f3\uff0c\u4e8e\u662f\u7b49\u5f85\u4e86\u6f2b\u957f\u76843\u5c0f\u65f6\u624d\u5f00\u59cb\u8c03\u97f3\u3002</p> <p>\u767b\u53f0\u5c45\u7136\u6ca1\u6709\u592a\u7d27\u5f20\uff0c\u5c31\u8fd9\u4e48\u8d70\u4e0a\u53bb\u4e86\uff0c\u624b\u6ca1\u591a\u5c11\u6c57\uff0c\u4e4b\u540e\u4e5f\u6ca1\u6709\u6389\u9f13\u69cc\u3002</p> <p>\u7b2c\u4e00\u9996\uff1a\u8fd8\u6ca1\u51e0\u5c0f\u8282\uff0c\u5c31\u53d1\u89c9\u9f13\u51f3\u4f4d\u7f6e\u4e0d\u5927\u597d\uff0c\u52a0\u4e0a\u5e95\u9f13\u8e0f\u677f\u7684\u4e3b\u4eba\u628a\u5b83\u8c03\u7684\u5f88 \u7d27\uff0c\u5bfc\u81f4\u53f3\u817f\u4f7f\u4e0d\u4e0a\u52b2\uff0c\u8e29\u4e0d\u52a8\uff0c\u7acb\u9a6c\u6709\u70b9\u5fc3\u865a\u5230\u540e\u9762\u6ca1\u529b\u5c31\u5b8c\u4e86\u3002\u4e0d\u8fc7\u53ea\u80fd\u786c \u7740\u5934\u76ae\u4e0a\u3002\u6ca1\u8fc7\u591a\u4e45\u89c2\u4f17\u5c45\u7136\u5f00\u59cb\u6b22\u547c\uff0c\u5c45\u7136\u5f00\u59cb\u8e66\u8df3\u8d77\u6765\uff0c\u5927\u559c\u3002\u53f3\u817f\u5dee\u4e0d\u591a \u70ed\u8eab\u8d77\u6765\uff0c\u4e5f\u4f7f\u5f97\u4e0a\u52b2\u4e86\uff0c\u5927\u677e\u4e00\u53e3\u6c14\u3002</p> <p>\u7b2c\u4e8c\u9996come together\u5f00\u59cb\u6253\u4e00\u9635\u5b50tom\u9f13\u7684\u90e8\u5206\uff0c\u56e0\u4e3a\u9f13\u6570\u91cf\u4ece3\u4e2a\u51cf\u5c11\u52302\u4e2a\uff0c \u8111\u5b50\u77ed\u8def\u4e86\u51e0\u6b21\uff0c\u51fa\u4e86\u51e0\u6ef4\u51b7\u6c57\uff0c\u5fc3\u60f3\u8be5\u4e0d\u4f1a\u5c31\u8fd9\u4e48\u6302\u4e86\u5427\uff0c\u4ee5\u540e\u6bcf\u6b21\u90fd\u8981\u5728\u8fd9 \u90e8\u5206\u6389\u94fe\u5b50\u5c31\u592a\u660e\u663e\u4e86\u3002\u597d\u5728\u867d\u7136\u4e25\u91cd\u6f0f\u6253\u591a\u6b21\u5173\u952e\u9f13\u70b9\uff0c\u4f46\u6ca1\u9519\u62cd\u6ca1\u6389\u62cd\uff0c\u6ca1 \u5f71\u54cd\u6b4c\u66f2\u8fdb\u5ea6\u3002\u6de1\u5b9a\u601d\u8003\u4e4b\u540e\u4fa5\u5e78\u6062\u590d\u6c34\u5e73\u3002\u540e\u6765\u770b\u5f55\u50cf\uff0c\u5c45\u7136\u57fa\u672c\u542c\u4e0d\u51fa\u6765\uff0c \u9664\u975e\u77e5\u9053\u8c31\u5b50\u539f\u672c\u5e94\u8be5\u662f\u600e\u6837\u7684\u3002</p> <p>\u4e4b\u540e\u5c31\u6ca1\u5927\u72b6\u51b5\u4e86\uff0c\u867d\u6709\u5c0f\u9519\u4f46\u7b97\u662f\u987a\u5229\u5b8c\u6210\u7b2c\u4e09\uff0c\u56db\u9996\u3002\u53f0\u4e0b\u89c2\u4f17\u7ee7\u7eed\u5f88\u7ed9\u9762 \u5b50\uff0c\u53eb\u518d\u6765\u4e00\u9996\uff08\u8c8c\u4f3c\u4e0d\u662f\u793c\u8282\u6027\u7684\uff0c\u56e0\u4e3a\u7b2c\u4e8c\u4e2a\u4e50\u961f\u4ed6\u4eec\u5c31\u6ca1\u558a\uff09\uff0c\u4e0d\u8fc7\u6211\u4eec \u771f\u53ea\u4f1a4\u9996\uff0c\u6240\u4ee5\u5c31\u9057\u61be\u4e0b\u573a\u4e86\u3002</p> <p>\u7ed3\u675f\u540e\u610f\u5916\u7684\u53d7\u5230\u8868\u626c\u3002\u56e0\u4e3a\u9009\u66f2\u8ba8\u5de7\uff0c\u53d1\u6325\u4e5f\u4e0d\u9519\u3002\u5c45\u7136\u8fd8\u6709\u4e0d\u8ba4\u8bc6\u7684\u4eba\u8fc7\u6765 \u5938\u6211\u7684\u9f13\uff0c\u53d7\u5ba0\u82e5\u60ca\u3002\u4f5c\u4e3a\u7b2c\u4e00\u573a\u516c\u5f00\u6f14\u51fa\uff0c\u9762\u5bf970-80\u4e2a\u89c2\u4f17\uff0c\u6548\u679c\u6bd4\u6211\u4eec\u9884 \u8ba1\u7684\u7b80\u76f4\u597d\u592a\u591a\uff0c\u89c2\u4f17\u660e\u663e\u591f\u75af\uff0c\u4e0b\u6765\u4eb2\u53cb\u56e2\u7eb7\u7eb7\u8fc7\u6765\u8868\u626c+\u62cd\u7167\u7559\u5ff5\u3002\u5927\u5bb6\u90fd \u6ca1\u6709\u592a\u7d27\u5f20\uff0c\u8fde\u6211\u8fd9\u4e2a\u8138\u76ae\u6700\u8584\u7684\u90fd\u6ca1\u6296\uff0c\u636e\u8bf4\u770b\u8d77\u6765\u8fd8\u8f7b\u677e\u81ea\u7136\u3002</p> <p>\u5728\u53f0\u4e0b\u5f85\u5f97\u6bd4\u53f0\u4e0a\u8fd8\u4e0d\u81ea\u5728\uff0c\u592a\u591a\u4eba\u4e86\uff0c\u89e6\u53d1\u793e\u6050\uff0c\u5c31\u4e70\u4e86\u4e00\u76d2\u8fa3\u9e21\u7fc5\u627e\u4e2a\u7a7a\u684c \u5b50\u5750\u4e0b\u8fb9\u5543\u8fb9\u770b\uff0c\u9664\u4e86\u7cbe\u5f69\u4e4b\u5904\u6325\u6325\u624b\uff0c\u5176\u5b83\u65f6\u95f4\u4f4e\u8c03\u8fdb\u98df\u3002\u5468\u56f4\u4e00\u4e2a\u8ba4\u8bc6\u7684\u4eba \u90fd\u6ca1\u6709\uff0c\u719f\u4eba\u90fd\u805a\u5230\u53f0\u524d\u53bb\u4e86\u3002\u9e21\u7fc5\u592a\u8fa3\uff0c\u6ca1\u5403\u51e0\u5757\uff0c\u5c31\u6574\u76d2\u9001\u7ed9\u7f8e\u5973\u8d1d\u65af\u624b\u3002 \u540e\u6765\u5979\u8bf4\u76d2\u5b50\u91cc\u6709\u4e00\u4e2a\u5543\u4e86\u4e00\u534a\u7684\u9e21\u7fc5\u548c\u51e0\u6839\u9aa8\u5934\uff0c\u6211\u8fd9\u624d\u60f3\u8d77\uff0c\u662f\u6211\u5e72\u7684\u3002</p>"},{"location":"writing/showlog/#_3","title":"\u4e07\u5723\u8282\u6f14\u51fa","text":"<p>\u5728\u4e00\u4e2a\u540c\u5b66\u5bb6\u7684\u4e07\u5723\u8282\u805a\u4f1a\uff0c8\u9996\u6b4c\uff0c\u65b0\u7684\u52a0\u65e7\u7684\u3002\u63d0\u524d\u4e00\u4e2a\u5c0f\u65f6\u8fc7\u4e86\u4e00\u904d\u66f2\u76ee\uff0c \u53d1\u6325\u90a3\u4e2a\u597d\uff0c\u7ed3\u679c\u6b63\u5f0f\u6f14\u51fa\u5404\u79cd\u6389\u94fe\u5b50\u3002\u597d\u5728\u89c2\u4f17\u5f88\u6fc0\u52a8\uff0c\u5f88\u652f\u6301\uff0c\u6bd5\u7adf\u90fd\u662f \u719f\u4eba\uff0c\u6240\u4ee5\u7b97\u662f\u6210\u529f\u7684\u6f14\u51fa\u3002\u9996\u6b21\u805a\u4f1a\u6f14\u51fa\uff0c\u6512\u4e86\u70b9\u7ecf\u9a8c\u503c\u3002</p> <p>\u6210\u529f\u90e8\u5206\uff1a</p> <ol> <li>\u9009\u66f2\u8ba8\u597d\uff0c\u5f88\u591a\u89c2\u4f17\u8ddf\u7740\u5531\uff0c\u5f88\u7ed9\u8138</li> <li>\u88ab\u8981\u6c42encore\uff0c\u53ef\u60dc\u6ca1\u6709encore\u66f2\uff0c\u5168\u6d88\u8017\u5149\u4e86</li> <li>\u961f\u5458\u670d\u88c5\u7a7f\u7684\u591f\u7ed9\u529b\uff0c\u89c2\u4f17\u770b\u5f97\u5f88\u5f00\u5fc3\u3002</li> <li>\u9a8c\u8bc1\u4e86\u6389\u9f13\u69cc\u4e0d\u6389\u62cd\u6280\u80fd\uff0c\u4e24\u5c0f\u8282\u5185\u6293\u4e86\u5907\u7528\u9f13\u69cc\u6de1\u5b9a\u7ee7\u7eed\u6253\u3002\u4e0d\u8fc7\u540e\u9762\u6709    \u70b9\u5c0f\u7d27\u5f20\uff0c\u6015\u6c57\u624b\u518d\u6389\u68d2\u3002</li> </ol> <p>\u60b2\u50ac\u4e8b\u4ef6\uff1a</p> <ol> <li>\u5927\u5bb6\u90fd\u6ca1\u6709\u8c03\u97f3\u7ecf\u9a8c\uff0c\u4e0d\u50cf\u4e0a\u6b21\u6709\u4e13\u4e1a\u8c03\u97f3\u5e08\u3002\u97f3\u91cf\u4e0d\u77e5\u9053\u8c03\u6210\u600e\u6837\u4e86\uff0c\u6211    \u89c9\u5f97\u5dee\u5f69\u6392\u4e00\u6761\u8857\u3002\u9996\u6b21\u7528\u7535\u9f13\uff0c\u6211\u542c\u5230\u7684\u53cd\u9988\u4e3b\u8981\u6765\u81ea\u6572\u6a61\u80f6\uff0camp\u79bb\u5f97\u6709    \u70b9\u8fdc\uff0c\u4e5f\u4e0d\u77e5\u9053\u6572\u6210\u5565\u6548\u679c\u4e86\uff0c\u5e78\u597d\u5176\u4ed6\u4e50\u624b\u548c\u89c2\u4f17\u90fd\u8bf4\u6ca1\u6709\u8fd9\u4e2a\u95ee\u9898\uff0c\u542c    \u5230\u7684\u662f\u65e0\u5e72\u6270\u7684\u529f\u653e\u9f13\u58f0\u3002\u597d\u5728\u9ea6\u514b\u98ce\u591f\u54cd\uff0c\u56e0\u4e3a\u5973\u8d1d\u65af\u624b\u517c\u4e3b\u5531\u6700\u51fa\u5f69\u3002</li> <li>\u7531\u4e8e\u5f55\u97f3\u8bbe\u5907\u5728\u6211\u80cc\u540e\uff0c\u4f4d\u7f6e\u4e0d\u597d\uff0c\u5f55\u4e0b\u6765\u7684\u9f13\u5c31\u662f\u6a61\u80f6\u58f0\u4e3a\u4e3b\u3002</li> <li>\u592a\u4e45\u6ca1\u7528\u7535\u9f13\uff0c\u5bf9\u96f6\u90e8\u4ef6\u4f4d\u7f6e\u7684\u808c\u8089\u8bb0\u5fc6\u57fa\u672c\u5168\u90e8\u6d88\u9000\uff0c\u4fe9\u9f13\u69cc\u82e5\u5e72\u6b21\u649e\u8f66\uff0c    \u51e0\u6b21\u5dee\u70b9\u6389\uff0c\u8fd8\u597d\u53ea\u6389\u4e00\u6b21\u3002</li> <li>\u6709\u7740\u4eba\u6765\u75af\u7684\u961f\u53cb\uff0c\u6f14\u51fa\u5b8c\u5c31\u75af\u53bb\u4e86\uff0c\u642c\u5668\u6750\u7f3a\u4e00\u4eba\u3002\u628a\u4e1c\u897f\u642c\u56de\u53bb\u771f\u662f\u4f53    \u529b\u6d3b\uff0c\u8fd8\u4e24\u6b21\u88ab\u9501\u5728\u95e8\u5916\uff0c\u56e0\u4e3a\u5ba4\u5185\u7684\u4eba\u5728\u805a\u4f1a\u542c\u4e0d\u5230\u3002</li> <li>\u6709\u4e9b\u8282\u594f\u7a0d\u7a0d\u8981\u7528\u5fc3\u6570\u8282\u62cd\u7684\u5730\u65b9\uff0c\u9000\u7f29\u4e86\uff0c\u7f29\u6c34\u6210\u7b80\u6613\u7248\u672c\u3002</li> </ol> <p>\u5f55\u50cf</p> <ul> <li>Float on</li> <li>Baby one more time</li> </ul>"},{"location":"writing/showlog/#_4","title":"\u6bd5\u4e1a\u6f14\u51fa","text":"<p>\u6bd5\u4e1a\u524d\u7684\u544a\u522b\u6f14\u51fa\u3002\u7531\u4e8e\u51e0\u4e2a\u6708\u7279\u5fd9\uff0c\u6f14\u51fa\u524d\u6700\u5f3a\u70c8\u7684\u60f3\u6cd5\u662f\u5feb\u70b9\u5b8c\u4e8b\u5c31\u544a\u4e00\u6bb5 \u843d\u4e0d\u7528\u6392\u7ec3\u4e86\u3002\u6f14\u51fa\u5b8c\u6bd5\uff0c\u56de\u5fc6\u8d77\u4e50\u961f\u7684\u4e00\u5207\uff0c\u624d\u540e\u4e4b\u540e\u89c9\u7684\u611f\u6168\u4e86\u4e00\u628a\u3002\u6211\u51c6 \u5907\u7684\u6709\u70b9\u5fc3\u4e0d\u5728\u7109\uff0c\u5230\u6f14\u51fa\u624d\u60f3\u8d77\u51e0\u4e2a\u670b\u53cb\u90fd\u6ca1\u6709\u901a\u77e5\u5230\u3002</p> <p>\u4e50\u961f\u6210\u7acb\u4e00\u5e74\uff0c\u8fd9\u662f\u7b2c\u4e09\u573a\u8868\u6f14\u3002\u7531\u4e8e\u5927\u5bb6\u90fd\u662f\u7b2c\u4e00\u6b21\u7ec4\u56e2\uff0c\u6240\u4ee5\u90fd\u5f88\u73cd\u60dc\uff0c\u51c6 \u5907\u7684\u5f88\u5c3d\u529b\u3002\u4e00\u5f00\u59cb\u867d\u7136\u5927\u5bb6\u90fd\u6709\u4e2a\u4e50\u961f\u68a6\uff0c\u5374\u6ca1\u6562\u60f3\u592a\u597d\uff0c\u53ea\u4ee5\u63d0\u9ad8\u6c34\u5e73\u4e3a \u76ee\u7684\u3002\u540e\u6765\u673a\u7f18\u5de7\u5408\u53c2\u52a0\u4e86\u4e00\u573a\u7cfb\u91cc\u7684\u5e74\u5ea6\u6f14\u51fa\uff0c\u5e74\u5e95\u53c8\u88ab\u9080\u8bf7\u53bb\u53e6\u4e00\u4e2aparty \u6f14\u51fa\uff0c\u6da8\u4e86\u7ecf\u9a8c\u503c\u3002\u4eca\u5e74\u7684\u5e74\u5ea6\u6f14\u51fa\uff0c\u6211\u4eec\u5f53\u7136\u6ca1\u6709\u653e\u8fc7\u673a\u4f1a\u3002\u8fd9\u6b21\u53d1\u6325\u6bd4\u53bb\u5e74 \u597d\u591a\u4e86\uff0c\u6280\u672f\u548c\u914d\u5408\u7684\u7d27\u5bc6\u5ea6\u5927\u6709\u63d0\u9ad8\u3002\u6f14\u51fa\u5b8c\u540e\u5df4\u7ed3\u4e86\u53e6\u4e00\u4e2a\u6709\u9152\u5427network \u7684\u4e50\u961f\uff0c\u4ee5\u540e\u5e94\u8be5\u80fd\u83b7\u5f97\u66f4\u591a\u673a\u4f1a\uff0c\u8bf4\u4e0d\u5b9a\u80fd\u8301\u58ee\u6210\u957f\u4e00\u628a\u3002</p> <p>\u56de\u5fc6\u5f53\u521d\u5404\u79cd\u673a\u7f18\u5de7\u5408\u3002\u4e0d\u77e5\u9053\u4ece\u54ea\u91cc\u5ffd\u60a0\u4eba\u53c2\u52a0\u4e00\u4e2a\u8d77\u6b65\u4e50\u961f\uff0c\u4e0d\u77e5\u9053\u80fd\u575a\u6301 \u591a\u4e45\u4eba\u5fc3\u4e0d\u6563\uff0c\u66f4\u4e0d\u77e5\u9053\u8c01\u4f1a\u540c\u610f\u6211\u4eec\u6f14\u51fa\u3002\u770b\u4e50\u961f\u6210\u957f\u8d77\u6765\uff0c\u6162\u6162\u5f97\u5230\u8d4f\u8bc6\uff0c \u65e0\u6bd4\u6709\u6210\u5c31\u611f\u3002</p> <p>\u8fd9\u6b21\u6ca1\u6709\u53bb\u5e74\u7b2c\u4e00\u6b21\u4e0a\u53f0\u7d27\u5f20\uff0c\u9152\u5427\u73af\u5883\u8212\u9002\u4e00\u70b9\uff0c\u66f4\u52a0\u201c\u4eb2\u6c11\u201d\u3002\u53ef\u662f\u624b\u5fc3\u4e00\u76f4 \u5728\u51fa\u6c57\uff0c\u4e0a\u53f0\u524d\u4e00\u76f4\u62c5\u5fc3\u6389\u68d2\u3002\u4e00\u4e0a\u53f0\uff0c\u6ca1\u60f3\u5230\u4e24\u76cf\u5927\u706f\u5982\u6d74\u9738\uff0c\u70e4\u7684\u76f4\u51fa\u6c57\uff0c \u7b2c\u4e00\u9996\u6b4c\u5c31\u534e\u4e3d\u4e3d\u7684\u6389\u68d2\u6389\u62cd\u5b50\u3002\u540e\u6765\u4e00\u76f4\u90fd\u6015\u6389\uff0c\u7279\u522b\u5c0f\u5fc3\u3002</p> <p>\u9152\u5427\u7684\u9f13\u5f88\u597d\uff0c\u53e6\u4e00\u4e2a\u4e50\u961f\u7684\u4e13\u4e1a\u9f13\u624btune\u8fc7\uff0csnare\u5c24\u5176\u6253\u8d77\u6765\u8212\u670d\uff0c\u73b0\u573a\u97f3 \u6548\u4e5f\u5f88\u597d\uff0c\u571f\u4eba\u89c9\u5f97\u597d\u4e45\u6ca1\u6709\u7528\u8fd9\u4e48\u597d\u7684\u9f13\u4e86\u3002\u9152\u5427\u63d0\u4f9b\u4e86\u4e24\u4e2acrash\uff0c\u9996\u6b21\u611f \u53d7\u53cccrash\u5f85\u9047\uff0c\u5c31\u4e00\u4e2a\u723d\u5b57\u3002\u9f13\u51f3\u6709\u70b9\u9ad8\uff0c\u8c03\u8d77\u6765\u6bd4\u8f83\u9ebb\u70e6\uff0c\u5e95\u9f13\u7684\u8e0f\u677f\u592a\u7d27\uff0c \u6240\u4ee5bass\u7684\u53d1\u6325\u5f88\u4e0d\u7406\u60f3\u3002\u6559\u8bad\u662f\u4ee5\u540e\u82e5\u6700\u6c42\u6700\u9ad8\u53d1\u6325\uff0c\u8fd8\u662f\u8be5\u5e26\u70b9\u81ea\u5df1\u7684\u5668\u6750\u3002 \u952e\u76d8\u6bd4\u8f83\u60b2\u50ac\uff0c\u5c40\u90e8\u6709\u574f\uff0c\u5f88\u591a\u97f3\u51fa\u4e0d\u6765\u3002</p> <p>White stripes: seven nation army / hardest button to button</p>"},{"location":"writing/train_music/","title":"\u706b\u8f66\u97f3\u4e50\u5199\u5230\u54ea\u7b97\u54ea","text":"<p>\u8ddf\u706b\u8f66\u6cbe\u70b9\u8fb9\u7684\u6b4c\uff0c\u60f3\u5230\u591a\u5c11\u5199\u591a\u5c11\uff0c\u770b\u4e2a\u4e50\u5b50\u5c31\u884c\uff0c\u4e0d\u5bf9\u51c6\u786e\u6027\u771f\u5b9e\u6027\u8d1f\u8d23</p>"},{"location":"writing/train_music/#take-the-a-train","title":"Take the A train","text":"<p>\u8fd9\u9996\u6b4c\u7684MV\u771f\u662f\u6109\u60a6\uff0c\u91cc\u9762\u6bcf\u4e2a\u4eba\u90fd\u5e26\u7740\u7b11\u3002</p>"},{"location":"writing/train_music/#crazy-train","title":"Crazy train","text":"<p>\u5409\u4ed6\u624bRandy\u7684\u6f14\u51fa\u529f\u4e0d\u53ef\u6ca1\u3002\u4ed6\u7684\u6027\u683c\u5728\u6447\u6eda\u5c4a\u7b97\u662f\u6bd4\u8f83\u5e72\u51c0\u7684\u4e86\uff0c\u8ddfOzzy\u6df7 \u4e00\u8d77\u6ca1\u6210\u763e\u541b\u5b50\u3002\u53ef\u60dc\u65e9\u901d\uff0c\u88abDUI\u5f00\u98de\u673a\u7684\u5751\u4e86\u3002</p>"}]}